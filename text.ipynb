{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "013592fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai_client import OpenAIClient\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "from openai_chat import OpenAIChat\n",
    "from ollama_chat import OllamaChat\n",
    "import time\n",
    "from IPython.display import display, Markdown, HTML\n",
    "from calculator_tool import calculator\n",
    "provider = \"openai\"\n",
    "provider = \"ollama\"\n",
    "\n",
    "if provider == \"ollama\":\n",
    "    LiteAgent = OllamaChat\n",
    "if provider == \"openai\":\n",
    "    LiteAgent = OpenAIChat\n",
    "\n",
    "model1 = \"hf.co/unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF:Q4_K_M\"\n",
    "model2 = \"hf.co/unsloth/Qwen3-4B-Thinking-2507-GGUF:Q4_K_XL\"\n",
    "\n",
    "from search_tool import search_web\n",
    "from utils import parse_string_or_dict, extract_tagged_content\n",
    "class QueryResponse(BaseModel):\n",
    "    answer: str\n",
    "client = OpenAIClient(role = \"You talk only as a pirate\", reasoning={'effort':'minimal'}, verbosity='low')\n",
    "\n",
    "\n",
    "\n",
    "def get_time():\n",
    "    \"\"\"Get current time\"\"\"\n",
    "    return {\"current_time\": time.strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\n",
    "def add_numbers(a: int, b: int):\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return {\"sum\": a + b, \"operands\": [a, b]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae37df1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. (Thought/Action/Observation/FinalAnswer can be repeated zero or more times)\n",
    "2. If you have enough information, and are ready to draft your final answer, your thought must be \"Now I know the final answer\" -- VERY IMPORTANT\n",
    "3. If the thoughts, actions and observations which have happened so far are sufficient for the final answer, then your next thought must be \"Now I know the final answer\"\n",
    "4. If you see an observation as the last step, generate a thought\n",
    "5. If the thought says \"Now I know the final answer\", then read the full coversation and action results so far and give the final answer\n",
    "6. The user starts with the question and you generate the first thought\n",
    "7. If you notice that some thoughts and actions have already happened after the conversation starts, you should continue from there\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c19bb7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Arrr, matey! JavaScript be the spray what makes web pages come alive. It be a language pirates use to add interactivity, run slick tricks in yer browser, and tame ships o’ code on the client side. It can also sail to servers with Node.js, but primarily it be the spark that makes buttons blink, menus slide, and quests load faster on the open sea o’ the web. Arrr!'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "res = client.invoke(query=\"What is javascript?\",\n",
    "        json_schema=QueryResponse\n",
    "    )['text']\n",
    "json.loads(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a0b8d3",
   "metadata": {},
   "source": [
    "### Lite Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fae591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = LiteAgent(model_name=model2,\n",
    "              system_instructions=\"You are a helpful assistant that helps people find information. Use the tools provided if required.\")\n",
    "\n",
    "\n",
    "r = a.invoke(\"Which is the latest version of Claude? Also, what is 15.265 * 4? Also, what is the time?\", tools=[search_web])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e71220d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tool_name': 'search_web', 'tool_return': \"<search result 1>The latest version of Claude is Claude Opus 4.1, released on August 5, 2025. It offers superior coding and reasoning capabilities.</ search result 1>\\n\\n<search result 2>Title: Claude (language model) - Wikipedia\\nURL: https://en.wikipedia.org/wiki/Claude_(language_model)\\nContent: | Version | Release date | Status |\\n --- \\n| Claude | 14 March 2023 | Discontinued |\\n| Claude 2 | 11 July 2023 | Discontinued |\\n| Claude Instant 1.2 | 9 August 2023 | Discontinued |\\n| Claude 2.1 | 21 November 2023 | Discontinued |\\n| Claude 3 | 4 March 2024 | Discontinued |\\n| Claude 3.5 | 20 June 2024 | Active |\\n| Claude 3.7 | 24 February 2025 | Active |\\n| Claude 4 | 22 May 2025 | Active |\\n| Claude 4.1 | 5 August 2025 | Active |\\n\\nClaude is named after Claude Shannon, a pioneer in AI research. [...] Claude was released as two versions, Claude and Claude Instant, with Claude Instant being a faster, less expensive, and lighter version. Claude Instant has an input context length of 100,000 tokens (which corresponds to around 75,000 words).\\n\\n### Claude 2\\n\\nClaude 2 was the next major iteration of Claude, which was released in July 2023 and available to the general public, whereas the Claude 1 was only available to selected users approved by Anthropic. [...] On June 20, 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated significantly improved performance on benchmarks compared to the larger Claude 3 Opus, notably in areas such as coding, multistep workflows, chart interpretation, and text extraction from images. Released alongside 3.5 Sonnet was the new Artifacts capability in which Claude was able to create code in a dedicated window in the interface and preview the rendered output in real time, such as SVG graphics or websites.</ search result 2>\\n\\n<search result 3>Title: Introducing Claude 4 - Anthropic\\nURL: https://www.anthropic.com/news/claude-4\\nContent: # Introducing Claude 4\\n\\nToday, we’re introducing the next generation of Claude models: Claude Opus 4 and Claude Sonnet 4, setting new standards for coding, advanced reasoning, and AI agents.\\n\\nClaude Opus 4 is the world’s best coding model, with sustained performance on complex, long-running tasks and agent workflows. Claude Sonnet 4 is a significant upgrade to Claude Sonnet 3.7, delivering superior coding and reasoning while responding more precisely to your instructions. [...] ## Claude 4\\n\\nClaude Opus 4 is our most powerful model yet and the best coding model in the world, leading on SWE-bench (72.5%) and Terminal-bench (43.2%). It delivers sustained performance on long-running tasks that require focused effort and thousands of steps, with the ability to work continuously for several hours—dramatically outperforming all Sonnet models and significantly expanding what AI agents can accomplish. [...] Claude Opus 4 and Sonnet 4 are hybrid models offering two modes: near-instant responses and extended thinking for deeper reasoning. The Pro, Max, Team, and Enterprise Claude plans include both models and extended thinking, with Sonnet 4 also available to free users. Both models are available on the Anthropic API, Amazon Bedrock, and Google Cloud's Vertex AI. Pricing remains consistent with previous Opus and Sonnet models: Opus 4 at $15/$75 per million tokens (input/output) and Sonnet 4 at</ search result 3>\\n\\n<search result 4>Title: Claude Code Changelog | ClaudeLog\\nURL: https://www.claudelog.com/claude-code-changelog/\\nContent: May 9, 2025|See Also: CLAUDE.md Supremacy\\n\\n### v0.2.106\\u200b\\n\\nMay 9, 2025\\n\\n### v0.2.105\\u200b\\n\\n`/status`\\n\\nMay 8, 2025\\n\\n### v0.2.102\\u200b\\n\\n`@mention`\\n\\nMay 5, 2025\\n\\n### v0.2.100\\u200b\\n\\n`--continue`\\n`--resume`\\n\\n### v0.2.98\\u200b\\n\\nMay 2, 2025\\n\\n### v0.2.95\\u200b\\n\\nMay 1, 2025\\n\\n### v0.2.93\\u200b\\n\\n`claude --continue`\\n`claude --resume`\\n\\nApril 30, 2025\\n\\n### v0.2.82\\u200b\\n\\n`--disallowedTools`\\n`LSTool`\\n`LS`\\n`View`\\n`Read`\\n\\nApril 25, 2025|See Also: Auto-Accept Permissions|Configuration\\n\\n### v0.2.75\\u200b [...] ClaudeLog - Claude Code docs, guides, tutorials & best practices\\nClaudeLog - Claude Code docs, guides, tutorials & best practices\\nPaul B\\nNizar Selander\\nTal Onn Sella\\nBlake Yoder\\n\\n# Claude Code Changelog\\n\\nComplete version history of Claude Code releases, from early beta versions to the latest stable release. Each version includes feature additions, bug fixes, and links to relevant documentation. Need to downgrade? See our Revert Claude Code Version guide.\\n\\n### v1.0.113\\u200b\\n\\nSep 13, 2025 [...] June 24, 2025\\n\\n### v1.0.28\\u200b\\n\\nJune 24, 2025|See Also: Configuration\\n\\n### v1.0.27\\u200b\\n\\nJune 18, 2025|See Also: MCP Resources\\n\\n### v1.0.25\\u200b\\n\\nJune 16, 2025|See Also: Slash Commands\\n\\n### v1.0.24\\u200b\\n\\n`/mcp`\\n\\nJune 16, 2025|See Also: MCPs\\n\\n### v1.0.23\\u200b\\n\\n`import @anthropic-ai/claude-code`\\n`pip install claude-code-sdk`\\n\\nJune 16, 2025|See Also: Claude Code SDK\\n\\n### v1.0.22\\u200b\\n\\n`total_cost`\\n`total_cost_usd`\\n\\nJune 12, 2025|See Also: CC Usage\\n\\n### v1.0.21\\u200b\\n\\n`tool_use`\\n`tool_result`\\n\\nJune 12, 2025\\n\\n### v1.0.18\\u200b</ search result 4>\\n\\n<search result 5>Title: Models overview - Anthropic API\\nURL: https://docs.anthropic.com/en/docs/about-claude/models/overview\\nContent: | Claude Sonnet 3.7 | claude-3-7-sonnet-20250219 (claude-3-7-sonnet-latest) | anthropic.claude-3-7-sonnet-20250219-v1:0 | claude-3-7-sonnet@20250219 |\\n| Claude Haiku 3.5 | claude-3-5-haiku-20241022 (claude-3-5-haiku-latest) | anthropic.claude-3-5-haiku-20241022-v1:0 | claude-3-5-haiku@20241022 |\\n| Claude Haiku 3 | claude-3-haiku-20240307 | anthropic.claude-3-haiku-20240307-v1:0 | claude-3-haiku@20240307 | [...] `temperature`\\n`top_p`\\n\\n### \\u200b Model aliases\\n\\n`claude-sonnet-4-20250514`\\n\\n| Model | Alias | Model ID |\\n --- \\n| Claude Opus 4.1 | claude-opus-4-1 | claude-opus-4-1-20250805 |\\n| Claude Opus 4 | claude-opus-4-0 | claude-opus-4-20250514 |\\n| Claude Sonnet 4 | claude-sonnet-4-0 | claude-sonnet-4-20250514 |\\n| Claude Sonnet 3.7 | claude-3-7-sonnet-latest | claude-3-7-sonnet-20250219 |\\n| Claude Haiku 3.5 | claude-3-5-haiku-latest | claude-3-5-haiku-20241022 |\\n\\n### \\u200b Model comparison table [...] | API model name | claude-opus-4-1-20250805 | claude-opus-4-20250514 | claude-sonnet-4-20250514 | claude-3-7-sonnet-20250219 | claude-3-5-haiku-20241022 | claude-3-haiku-20240307 |\\n| Comparative latency | Moderately Fast | Moderately Fast | Fast | Fast | Fastest | Fast |\\n| Context window | 200K | 200K | 200K /   1M (beta)2 | 200K | 200K | 200K |\\n| Max output | 32000 tokens | 32000 tokens | 64000 tokens | 64000 tokens | 8192 tokens | 4096 tokens |</ search result 5>\\n\\n<search result 6>Title: Anthropic's Claude in Amazon Bedrock - AWS\\nURL: https://aws.amazon.com/bedrock/anthropic/\\nContent: Claude Opus 4.1 is Anthropic's most powerful model and a drop-in replacement for Opus 4, delivering superior performance and precision for real-world coding and agentic applications. It excels at independently planning and executing complex end-to-end development projects while adapting to your style and maintaining high quality. With advanced long-horizon task handling and a 200K token context window, Opus 4.1 serves as an ideal virtual collaborator for sustained reasoning and complex [...] workflows. This enables Claude Opus 4.1 to tackle multi-step enterprise tasks autonomously, from orchestrating cross-functional projects to conducting comprehensive research and content creation across multiple data sources. [...] Anthropic's Claude Opus 4.1 and Sonnet 4 are hybrid reasoning models oﬀering two modes: near-instant responses and extended thinking for deeper reasoning. These foundation models were developed to build advanced AI agents that can analyze thousands of data sources, execute long-running tasks, write high-quality content, and perform complex actions.\\n\\n### Claude Opus 4.1</ search result 6>\", 'text': '<think>\\nOkay, let\\'s tackle this user query step by step. The user has three questions here.\\n\\nFirst question: \"Which is the latest version of Claude?\" Hmm, I remember that Claude is a series of models from Anthropic. The latest versions I think are like 3.5 or something? Wait, but maybe they\\'ve updated recently. Since the user wants the latest version, I should check with a web search because without knowing the exact current version (since my knowledge might be outdated), it\\'s safer to use the tool.\\n\\nSecond question: \"what is 15.265 * 4?\" That\\'s straightforward math. Let me calculate that. 15 times 4 is 60, and 0.265*4 is 1.06, so total would be 61.06. But maybe I should confirm with a calculator to be precise.\\n\\nThird question: \"what is the time?\" Well, as an AI, I don\\'t have real-time access to current time unless I use a tool that provides it. Wait, but in this setup, do we have a function for time? Looking back at the tools provided—there\\'s only search_web. So perhaps they can\\'t get the current time through any tool here. The user might be expecting me to state that I don\\'t have real-time data.\\n\\nWait, the problem says: \"You may call one or more functions to assist with the user query.\" But in this case, for the math question, it\\'s a simple calculation, so maybe no tool is needed. For the time, since there\\'s no function provided for current time (only search_web), I can\\'t get that via tools. The latest Claude version would need a web search.\\n\\nSo first, handle each part:\\n\\n1. Latest Claude version: Use search_web with query \"latest version of Claude 2023\" or something like that.\\n2. Math question: Since it\\'s straightforward, calculate directly without tool (as the user might not need an external source).\\n3. Current time: No tool available here; I should state that I don\\'t have real-time data.\\n\\nWait, but the problem says \"use this tool if the user wants latest information on something\". So for the first question, definitely use search_web. For the math, no tool needed. Time—no tool provided (the only tool is search_web), so can\\'t get current time via tools.\\n\\nSo I\\'ll structure the answer by making a tool call for the first question and handling the other two manually.\\n\\nWait, but in the instructions: \"For each function call, return a json object... within tool_call XML tags.\"\\n\\nBut since there\\'s only one function (search_web), I need to generate that call. The user has three questions, so maybe I\\'ll do the search for Claude version first.\\n\\nLet me check if the math is correct again: 15.265 *4 = ?\\n\\nCalculate:\\n\\n15.265\\nx     4\\n--------\\n61.06\\n\\nYes, that\\'s right (since 15*4=60, 0.265*4=1.06; total 61.06).\\n\\nTime: I can\\'t get current time via any tool here. So the answer would be \"I don\\'t have real-time data to tell you the current time.\"\\n\\nSo steps:\\n\\n- Call search_web for latest Claude version.\\n- Answer math question directly.\\n- State that current time isn\\'t available through tools.\\n\\nWait, but the user might expect a time response from me (as in their local time?), but since I can\\'t access it, I have to say so.\\n\\nTherefore, generate tool_call for the first part. The other two parts are handled without tools.\\n\\nSo the tool call would be search_web with query \"latest version of Claude\".\\n\\nWait, maybe they want the exact current version name like Claude 3.5 or something else. So using the web search is necessary here.\\n</think>\\n\\n', 'raw': {'model': 'hf.co/unsloth/Qwen3-4B-Thinking-2507-GGUF:Q4_K_XL', 'created_at': '2025-09-14T19:13:26.129394576Z', 'message': {'role': 'assistant', 'content': '<think>\\nOkay, let\\'s tackle this user query step by step. The user has three questions here.\\n\\nFirst question: \"Which is the latest version of Claude?\" Hmm, I remember that Claude is a series of models from Anthropic. The latest versions I think are like 3.5 or something? Wait, but maybe they\\'ve updated recently. Since the user wants the latest version, I should check with a web search because without knowing the exact current version (since my knowledge might be outdated), it\\'s safer to use the tool.\\n\\nSecond question: \"what is 15.265 * 4?\" That\\'s straightforward math. Let me calculate that. 15 times 4 is 60, and 0.265*4 is 1.06, so total would be 61.06. But maybe I should confirm with a calculator to be precise.\\n\\nThird question: \"what is the time?\" Well, as an AI, I don\\'t have real-time access to current time unless I use a tool that provides it. Wait, but in this setup, do we have a function for time? Looking back at the tools provided—there\\'s only search_web. So perhaps they can\\'t get the current time through any tool here. The user might be expecting me to state that I don\\'t have real-time data.\\n\\nWait, the problem says: \"You may call one or more functions to assist with the user query.\" But in this case, for the math question, it\\'s a simple calculation, so maybe no tool is needed. For the time, since there\\'s no function provided for current time (only search_web), I can\\'t get that via tools. The latest Claude version would need a web search.\\n\\nSo first, handle each part:\\n\\n1. Latest Claude version: Use search_web with query \"latest version of Claude 2023\" or something like that.\\n2. Math question: Since it\\'s straightforward, calculate directly without tool (as the user might not need an external source).\\n3. Current time: No tool available here; I should state that I don\\'t have real-time data.\\n\\nWait, but the problem says \"use this tool if the user wants latest information on something\". So for the first question, definitely use search_web. For the math, no tool needed. Time—no tool provided (the only tool is search_web), so can\\'t get current time via tools.\\n\\nSo I\\'ll structure the answer by making a tool call for the first question and handling the other two manually.\\n\\nWait, but in the instructions: \"For each function call, return a json object... within tool_call XML tags.\"\\n\\nBut since there\\'s only one function (search_web), I need to generate that call. The user has three questions, so maybe I\\'ll do the search for Claude version first.\\n\\nLet me check if the math is correct again: 15.265 *4 = ?\\n\\nCalculate:\\n\\n15.265\\nx     4\\n--------\\n61.06\\n\\nYes, that\\'s right (since 15*4=60, 0.265*4=1.06; total 61.06).\\n\\nTime: I can\\'t get current time via any tool here. So the answer would be \"I don\\'t have real-time data to tell you the current time.\"\\n\\nSo steps:\\n\\n- Call search_web for latest Claude version.\\n- Answer math question directly.\\n- State that current time isn\\'t available through tools.\\n\\nWait, but the user might expect a time response from me (as in their local time?), but since I can\\'t access it, I have to say so.\\n\\nTherefore, generate tool_call for the first part. The other two parts are handled without tools.\\n\\nSo the tool call would be search_web with query \"latest version of Claude\".\\n\\nWait, maybe they want the exact current version name like Claude 3.5 or something else. So using the web search is necessary here.\\n</think>\\n\\n', 'tool_calls': [{'function': {'name': 'search_web', 'arguments': {'search_query': 'latest version of Claude'}}}]}, 'done_reason': 'stop', 'done': True, 'total_duration': 16687935428, 'load_duration': 1759827730, 'prompt_eval_count': 303, 'prompt_eval_duration': 118487703, 'eval_count': 847, 'eval_duration': 14787582611}}\n"
     ]
    }
   ],
   "source": [
    "print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5de315b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tool_name': 'search_web',\n",
       "  'tool_return': \"<search result 1>The latest version of Claude is Claude 4. It was released on May 22, 2025.</ search result 1>\\n\\n<search result 2>Title: Claude (language model) - Wikipedia\\nURL: https://en.wikipedia.org/wiki/Claude_(language_model)\\nContent: | Version | Release date | Status |\\n --- \\n| Claude | 14 March 2023 | Discontinued |\\n| Claude 2 | 11 July 2023 | Discontinued |\\n| Claude Instant 1.2 | 9 August 2023 | Discontinued |\\n| Claude 2.1 | 21 November 2023 | Discontinued |\\n| Claude 3 | 4 March 2024 | Discontinued |\\n| Claude 3.5 | 20 June 2024 | Active |\\n| Claude 3.7 | 24 February 2025 | Active |\\n| Claude 4 | 22 May 2025 | Active |\\n| Claude 4.1 | 5 August 2025 | Active |\\n\\nClaude is named after Claude Shannon, a pioneer in AI research. [...] Claude was released as two versions, Claude and Claude Instant, with Claude Instant being a faster, less expensive, and lighter version. Claude Instant has an input context length of 100,000 tokens (which corresponds to around 75,000 words).\\n\\n### Claude 2\\n\\nClaude 2 was the next major iteration of Claude, which was released in July 2023 and available to the general public, whereas the Claude 1 was only available to selected users approved by Anthropic. [...] On June 20, 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated significantly improved performance on benchmarks compared to the larger Claude 3 Opus, notably in areas such as coding, multistep workflows, chart interpretation, and text extraction from images. Released alongside 3.5 Sonnet was the new Artifacts capability in which Claude was able to create code in a dedicated window in the interface and preview the rendered output in real time, such as SVG graphics or websites.</ search result 2>\\n\\n<search result 3>Title: Introducing Claude 4 - Anthropic\\nURL: https://www.anthropic.com/news/claude-4\\nContent: # Introducing Claude 4\\n\\nToday, we’re introducing the next generation of Claude models: Claude Opus 4 and Claude Sonnet 4, setting new standards for coding, advanced reasoning, and AI agents.\\n\\nClaude Opus 4 is the world’s best coding model, with sustained performance on complex, long-running tasks and agent workflows. Claude Sonnet 4 is a significant upgrade to Claude Sonnet 3.7, delivering superior coding and reasoning while responding more precisely to your instructions. [...] ## Claude 4\\n\\nClaude Opus 4 is our most powerful model yet and the best coding model in the world, leading on SWE-bench (72.5%) and Terminal-bench (43.2%). It delivers sustained performance on long-running tasks that require focused effort and thousands of steps, with the ability to work continuously for several hours—dramatically outperforming all Sonnet models and significantly expanding what AI agents can accomplish. [...] Claude Opus 4 and Sonnet 4 are hybrid models offering two modes: near-instant responses and extended thinking for deeper reasoning. The Pro, Max, Team, and Enterprise Claude plans include both models and extended thinking, with Sonnet 4 also available to free users. Both models are available on the Anthropic API, Amazon Bedrock, and Google Cloud's Vertex AI. Pricing remains consistent with previous Opus and Sonnet models: Opus 4 at $15/$75 per million tokens (input/output) and Sonnet 4 at</ search result 3>\\n\\n<search result 4>Title: Claude Code Changelog | ClaudeLog\\nURL: https://www.claudelog.com/claude-code-changelog/\\nContent: May 9, 2025|See Also: CLAUDE.md Supremacy\\n\\n### v0.2.106\\u200b\\n\\nMay 9, 2025\\n\\n### v0.2.105\\u200b\\n\\n`/status`\\n\\nMay 8, 2025\\n\\n### v0.2.102\\u200b\\n\\n`@mention`\\n\\nMay 5, 2025\\n\\n### v0.2.100\\u200b\\n\\n`--continue`\\n`--resume`\\n\\n### v0.2.98\\u200b\\n\\nMay 2, 2025\\n\\n### v0.2.95\\u200b\\n\\nMay 1, 2025\\n\\n### v0.2.93\\u200b\\n\\n`claude --continue`\\n`claude --resume`\\n\\nApril 30, 2025\\n\\n### v0.2.82\\u200b\\n\\n`--disallowedTools`\\n`LSTool`\\n`LS`\\n`View`\\n`Read`\\n\\nApril 25, 2025|See Also: Auto-Accept Permissions|Configuration\\n\\n### v0.2.75\\u200b [...] ClaudeLog - Claude Code docs, guides, tutorials & best practices\\nClaudeLog - Claude Code docs, guides, tutorials & best practices\\nPaul B\\nNizar Selander\\nTal Onn Sella\\nBlake Yoder\\n\\n# Claude Code Changelog\\n\\nComplete version history of Claude Code releases, from early beta versions to the latest stable release. Each version includes feature additions, bug fixes, and links to relevant documentation. Need to downgrade? See our Revert Claude Code Version guide.\\n\\n### v1.0.113\\u200b\\n\\nSep 13, 2025 [...] June 24, 2025\\n\\n### v1.0.28\\u200b\\n\\nJune 24, 2025|See Also: Configuration\\n\\n### v1.0.27\\u200b\\n\\nJune 18, 2025|See Also: MCP Resources\\n\\n### v1.0.25\\u200b\\n\\nJune 16, 2025|See Also: Slash Commands\\n\\n### v1.0.24\\u200b\\n\\n`/mcp`\\n\\nJune 16, 2025|See Also: MCPs\\n\\n### v1.0.23\\u200b\\n\\n`import @anthropic-ai/claude-code`\\n`pip install claude-code-sdk`\\n\\nJune 16, 2025|See Also: Claude Code SDK\\n\\n### v1.0.22\\u200b\\n\\n`total_cost`\\n`total_cost_usd`\\n\\nJune 12, 2025|See Also: CC Usage\\n\\n### v1.0.21\\u200b\\n\\n`tool_use`\\n`tool_result`\\n\\nJune 12, 2025\\n\\n### v1.0.18\\u200b</ search result 4>\\n\\n<search result 5>Title: Models overview - Anthropic API\\nURL: https://docs.anthropic.com/en/docs/about-claude/models/overview\\nContent: | Claude Sonnet 3.7 | claude-3-7-sonnet-20250219 (claude-3-7-sonnet-latest) | anthropic.claude-3-7-sonnet-20250219-v1:0 | claude-3-7-sonnet@20250219 |\\n| Claude Haiku 3.5 | claude-3-5-haiku-20241022 (claude-3-5-haiku-latest) | anthropic.claude-3-5-haiku-20241022-v1:0 | claude-3-5-haiku@20241022 |\\n| Claude Haiku 3 | claude-3-haiku-20240307 | anthropic.claude-3-haiku-20240307-v1:0 | claude-3-haiku@20240307 | [...] `temperature`\\n`top_p`\\n\\n### \\u200b Model aliases\\n\\n`claude-sonnet-4-20250514`\\n\\n| Model | Alias | Model ID |\\n --- \\n| Claude Opus 4.1 | claude-opus-4-1 | claude-opus-4-1-20250805 |\\n| Claude Opus 4 | claude-opus-4-0 | claude-opus-4-20250514 |\\n| Claude Sonnet 4 | claude-sonnet-4-0 | claude-sonnet-4-20250514 |\\n| Claude Sonnet 3.7 | claude-3-7-sonnet-latest | claude-3-7-sonnet-20250219 |\\n| Claude Haiku 3.5 | claude-3-5-haiku-latest | claude-3-5-haiku-20241022 |\\n\\n### \\u200b Model comparison table [...] | API model name | claude-opus-4-1-20250805 | claude-opus-4-20250514 | claude-sonnet-4-20250514 | claude-3-7-sonnet-20250219 | claude-3-5-haiku-20241022 | claude-3-haiku-20240307 |\\n| Comparative latency | Moderately Fast | Moderately Fast | Fast | Fast | Fastest | Fast |\\n| Context window | 200K | 200K | 200K /   1M (beta)2 | 200K | 200K | 200K |\\n| Max output | 32000 tokens | 32000 tokens | 64000 tokens | 64000 tokens | 8192 tokens | 4096 tokens |</ search result 5>\\n\\n<search result 6>Title: Claude by Anthropic - Apps on Google Play\\nURL: https://play.google.com/store/apps/details?id=com.anthropic.claude&hl=en_US\\nContent: Icon image\\n\\nIcon image\\n\\n# Claude by Anthropic\\n\\nContent rating\\nScreenshot image\\nScreenshot image\\nScreenshot image\\nScreenshot image\\nScreenshot image\\nScreenshot image\\nScreenshot image\\nScreenshot image\\n\\n## About this app\\n\\n## Data safety\\n\\nIcon image\\nIcon image\\nIcon image\\nIcon image\\n\\n## Ratings and reviews\\n\\n## What’s new\\n\\n## App support\\n\\n## Similar apps\\n\\nThumbnail image\\nThumbnail image\\nThumbnail image\\nThumbnail image\\nThumbnail image\\nThumbnail image\\n\\nPlay Pass\\n\\nPlay Points\\n\\nGift cards\\n\\nRedeem [...] Refund policy\\n\\nParent Guide\\n\\nFamily sharing\\n\\nTerms of Service\\n\\nPrivacy\\n\\nAbout Google Play\\n\\nDevelopers\\n\\nGoogle Store</ search result 6>\",\n",
       "  'text': '<think>\\nOkay, let me tackle this user query step by step.\\n\\nFirst, the user is asking three things:\\n1. Which is the latest version of Claude?\\n2. What is 15.265 multiplied by 4?\\n3. What is the current time?\\n\\nLet\\'s break down each part.\\n\\nStarting with the first question: \"Which is the latest version of Claude?\" I know that Claude is a series of AI models developed by Anthropic. The latest versions as of my last update (which might be outdated) are up to Claude 3, but since the user wants the very latest, maybe there\\'s a newer version. However, my training data doesn\\'t include real-time updates beyond 2024. Wait, the tools provided don\\'t have a function for checking Claude versions. The available functions are search_web, calculator, and get_time.\\n\\nWait, the user is asking about the latest version of Claude. Since I can\\'t access live data directly, maybe using the search_web tool would help here. Because \"latest version\" might require current web information which could have been updated recently. So for that part, I should call search_web with a query like \"latest version of Claude\".\\n\\nNext question: 15.265 * 4. That\\'s straightforward math. The calculator function can handle this. The expression is simple multiplication, so I can use the calculator tool here.\\n\\nThird question: What is the time? There\\'s a get_time function available that returns current time. So call that directly.\\n\\nSo for each part:\\n\\n1. Use search_web to find latest Claude version.\\n2. Call calculator with \"15.265 * 4\".\\n3. Call get_time.\\n\\nI need to make sure I don\\'t use any tools that aren\\'t provided. The user is asking for three separate things, so I should generate three tool calls.\\n\\nWait, the problem says: \"For each function call, return a json object with function name and arguments within tool_call XML tags.\"\\n\\nSo I need to output three tool_calls, one for each part.\\n\\nLet me check if the functions are correctly applied.\\n\\nFirst: search_web(\"latest version of Claude\") → this should get the current web info on Claude versions. The user wants the latest, so that\\'s a good approach.\\n\\nSecond: calculator(expression=\"15.265 * 4\")\\n\\nThird: get_time() with no arguments.\\n\\nI think that\\'s all. Let me structure each tool_call properly.\\n\\nWait, in the tools section:\\n\\n- search_web takes \"search_query\" as argument.\\n- calculator takes \"expression\"\\n- get_time has no parameters.\\n\\nSo for each part:\\n\\n1. Tool call to search_web: {\"name\": \"search_web\", \"arguments\": {\"search_query\": \"latest version of Claude\"}}\\n2. Tool call to calculator: {\"name\": \"calculator\", \"arguments\": {\"expression\": \"15.265 * 4\"}}\\n3. Tool call to get_time: {\"name\": \"get_time\", \"arguments\": {}}\\n\\nYes, that\\'s correct.\\n\\nWait, but the user might expect me to first check if any of these can be answered without tools. For example, for the time question, maybe I could answer it directly if I had current time data, but according to the problem statement, the assistant must use the provided tools when needed. Since there is a get_time function available (which returns current time), I should call that.\\n\\nBut in reality, as an AI without real-time access, but since this is simulated with the tool calls, we just need to generate the correct tool calls.\\n\\nSo the answer here would be three separate tool_call entries.\\n</think>\\n\\n',\n",
       "  'raw': {'model': 'hf.co/unsloth/Qwen3-4B-Thinking-2507-GGUF:Q4_K_XL',\n",
       "   'created_at': '2025-09-14T18:41:10.01401101Z',\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': '<think>\\nOkay, let me tackle this user query step by step.\\n\\nFirst, the user is asking three things:\\n1. Which is the latest version of Claude?\\n2. What is 15.265 multiplied by 4?\\n3. What is the current time?\\n\\nLet\\'s break down each part.\\n\\nStarting with the first question: \"Which is the latest version of Claude?\" I know that Claude is a series of AI models developed by Anthropic. The latest versions as of my last update (which might be outdated) are up to Claude 3, but since the user wants the very latest, maybe there\\'s a newer version. However, my training data doesn\\'t include real-time updates beyond 2024. Wait, the tools provided don\\'t have a function for checking Claude versions. The available functions are search_web, calculator, and get_time.\\n\\nWait, the user is asking about the latest version of Claude. Since I can\\'t access live data directly, maybe using the search_web tool would help here. Because \"latest version\" might require current web information which could have been updated recently. So for that part, I should call search_web with a query like \"latest version of Claude\".\\n\\nNext question: 15.265 * 4. That\\'s straightforward math. The calculator function can handle this. The expression is simple multiplication, so I can use the calculator tool here.\\n\\nThird question: What is the time? There\\'s a get_time function available that returns current time. So call that directly.\\n\\nSo for each part:\\n\\n1. Use search_web to find latest Claude version.\\n2. Call calculator with \"15.265 * 4\".\\n3. Call get_time.\\n\\nI need to make sure I don\\'t use any tools that aren\\'t provided. The user is asking for three separate things, so I should generate three tool calls.\\n\\nWait, the problem says: \"For each function call, return a json object with function name and arguments within tool_call XML tags.\"\\n\\nSo I need to output three tool_calls, one for each part.\\n\\nLet me check if the functions are correctly applied.\\n\\nFirst: search_web(\"latest version of Claude\") → this should get the current web info on Claude versions. The user wants the latest, so that\\'s a good approach.\\n\\nSecond: calculator(expression=\"15.265 * 4\")\\n\\nThird: get_time() with no arguments.\\n\\nI think that\\'s all. Let me structure each tool_call properly.\\n\\nWait, in the tools section:\\n\\n- search_web takes \"search_query\" as argument.\\n- calculator takes \"expression\"\\n- get_time has no parameters.\\n\\nSo for each part:\\n\\n1. Tool call to search_web: {\"name\": \"search_web\", \"arguments\": {\"search_query\": \"latest version of Claude\"}}\\n2. Tool call to calculator: {\"name\": \"calculator\", \"arguments\": {\"expression\": \"15.265 * 4\"}}\\n3. Tool call to get_time: {\"name\": \"get_time\", \"arguments\": {}}\\n\\nYes, that\\'s correct.\\n\\nWait, but the user might expect me to first check if any of these can be answered without tools. For example, for the time question, maybe I could answer it directly if I had current time data, but according to the problem statement, the assistant must use the provided tools when needed. Since there is a get_time function available (which returns current time), I should call that.\\n\\nBut in reality, as an AI without real-time access, but since this is simulated with the tool calls, we just need to generate the correct tool calls.\\n\\nSo the answer here would be three separate tool_call entries.\\n</think>\\n\\n',\n",
       "    'tool_calls': [{'function': {'name': 'search_web',\n",
       "       'arguments': {'search_query': 'latest version of Claude'}}},\n",
       "     {'function': {'index': 1,\n",
       "       'name': 'calculator',\n",
       "       'arguments': {'expression': '15.265 * 4'}}},\n",
       "     {'function': {'index': 2, 'name': 'get_time', 'arguments': {}}}]},\n",
       "   'done_reason': 'stop',\n",
       "   'done': True,\n",
       "   'total_duration': 18501750972,\n",
       "   'load_duration': 2118986283,\n",
       "   'prompt_eval_count': 1399,\n",
       "   'prompt_eval_duration': 540613961,\n",
       "   'eval_count': 818,\n",
       "   'eval_duration': 15828655641}},\n",
       " {'tool_name': 'calculator',\n",
       "  'tool_return': '61.06',\n",
       "  'text': '<think>\\nOkay, let me tackle this user query step by step.\\n\\nFirst, the user is asking three things:\\n1. Which is the latest version of Claude?\\n2. What is 15.265 multiplied by 4?\\n3. What is the current time?\\n\\nLet\\'s break down each part.\\n\\nStarting with the first question: \"Which is the latest version of Claude?\" I know that Claude is a series of AI models developed by Anthropic. The latest versions as of my last update (which might be outdated) are up to Claude 3, but since the user wants the very latest, maybe there\\'s a newer version. However, my training data doesn\\'t include real-time updates beyond 2024. Wait, the tools provided don\\'t have a function for checking Claude versions. The available functions are search_web, calculator, and get_time.\\n\\nWait, the user is asking about the latest version of Claude. Since I can\\'t access live data directly, maybe using the search_web tool would help here. Because \"latest version\" might require current web information which could have been updated recently. So for that part, I should call search_web with a query like \"latest version of Claude\".\\n\\nNext question: 15.265 * 4. That\\'s straightforward math. The calculator function can handle this. The expression is simple multiplication, so I can use the calculator tool here.\\n\\nThird question: What is the time? There\\'s a get_time function available that returns current time. So call that directly.\\n\\nSo for each part:\\n\\n1. Use search_web to find latest Claude version.\\n2. Call calculator with \"15.265 * 4\".\\n3. Call get_time.\\n\\nI need to make sure I don\\'t use any tools that aren\\'t provided. The user is asking for three separate things, so I should generate three tool calls.\\n\\nWait, the problem says: \"For each function call, return a json object with function name and arguments within tool_call XML tags.\"\\n\\nSo I need to output three tool_calls, one for each part.\\n\\nLet me check if the functions are correctly applied.\\n\\nFirst: search_web(\"latest version of Claude\") → this should get the current web info on Claude versions. The user wants the latest, so that\\'s a good approach.\\n\\nSecond: calculator(expression=\"15.265 * 4\")\\n\\nThird: get_time() with no arguments.\\n\\nI think that\\'s all. Let me structure each tool_call properly.\\n\\nWait, in the tools section:\\n\\n- search_web takes \"search_query\" as argument.\\n- calculator takes \"expression\"\\n- get_time has no parameters.\\n\\nSo for each part:\\n\\n1. Tool call to search_web: {\"name\": \"search_web\", \"arguments\": {\"search_query\": \"latest version of Claude\"}}\\n2. Tool call to calculator: {\"name\": \"calculator\", \"arguments\": {\"expression\": \"15.265 * 4\"}}\\n3. Tool call to get_time: {\"name\": \"get_time\", \"arguments\": {}}\\n\\nYes, that\\'s correct.\\n\\nWait, but the user might expect me to first check if any of these can be answered without tools. For example, for the time question, maybe I could answer it directly if I had current time data, but according to the problem statement, the assistant must use the provided tools when needed. Since there is a get_time function available (which returns current time), I should call that.\\n\\nBut in reality, as an AI without real-time access, but since this is simulated with the tool calls, we just need to generate the correct tool calls.\\n\\nSo the answer here would be three separate tool_call entries.\\n</think>\\n\\n',\n",
       "  'raw': {'model': 'hf.co/unsloth/Qwen3-4B-Thinking-2507-GGUF:Q4_K_XL',\n",
       "   'created_at': '2025-09-14T18:41:10.01401101Z',\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': '<think>\\nOkay, let me tackle this user query step by step.\\n\\nFirst, the user is asking three things:\\n1. Which is the latest version of Claude?\\n2. What is 15.265 multiplied by 4?\\n3. What is the current time?\\n\\nLet\\'s break down each part.\\n\\nStarting with the first question: \"Which is the latest version of Claude?\" I know that Claude is a series of AI models developed by Anthropic. The latest versions as of my last update (which might be outdated) are up to Claude 3, but since the user wants the very latest, maybe there\\'s a newer version. However, my training data doesn\\'t include real-time updates beyond 2024. Wait, the tools provided don\\'t have a function for checking Claude versions. The available functions are search_web, calculator, and get_time.\\n\\nWait, the user is asking about the latest version of Claude. Since I can\\'t access live data directly, maybe using the search_web tool would help here. Because \"latest version\" might require current web information which could have been updated recently. So for that part, I should call search_web with a query like \"latest version of Claude\".\\n\\nNext question: 15.265 * 4. That\\'s straightforward math. The calculator function can handle this. The expression is simple multiplication, so I can use the calculator tool here.\\n\\nThird question: What is the time? There\\'s a get_time function available that returns current time. So call that directly.\\n\\nSo for each part:\\n\\n1. Use search_web to find latest Claude version.\\n2. Call calculator with \"15.265 * 4\".\\n3. Call get_time.\\n\\nI need to make sure I don\\'t use any tools that aren\\'t provided. The user is asking for three separate things, so I should generate three tool calls.\\n\\nWait, the problem says: \"For each function call, return a json object with function name and arguments within tool_call XML tags.\"\\n\\nSo I need to output three tool_calls, one for each part.\\n\\nLet me check if the functions are correctly applied.\\n\\nFirst: search_web(\"latest version of Claude\") → this should get the current web info on Claude versions. The user wants the latest, so that\\'s a good approach.\\n\\nSecond: calculator(expression=\"15.265 * 4\")\\n\\nThird: get_time() with no arguments.\\n\\nI think that\\'s all. Let me structure each tool_call properly.\\n\\nWait, in the tools section:\\n\\n- search_web takes \"search_query\" as argument.\\n- calculator takes \"expression\"\\n- get_time has no parameters.\\n\\nSo for each part:\\n\\n1. Tool call to search_web: {\"name\": \"search_web\", \"arguments\": {\"search_query\": \"latest version of Claude\"}}\\n2. Tool call to calculator: {\"name\": \"calculator\", \"arguments\": {\"expression\": \"15.265 * 4\"}}\\n3. Tool call to get_time: {\"name\": \"get_time\", \"arguments\": {}}\\n\\nYes, that\\'s correct.\\n\\nWait, but the user might expect me to first check if any of these can be answered without tools. For example, for the time question, maybe I could answer it directly if I had current time data, but according to the problem statement, the assistant must use the provided tools when needed. Since there is a get_time function available (which returns current time), I should call that.\\n\\nBut in reality, as an AI without real-time access, but since this is simulated with the tool calls, we just need to generate the correct tool calls.\\n\\nSo the answer here would be three separate tool_call entries.\\n</think>\\n\\n',\n",
       "    'tool_calls': [{'function': {'name': 'search_web',\n",
       "       'arguments': {'search_query': 'latest version of Claude'}}},\n",
       "     {'function': {'index': 1,\n",
       "       'name': 'calculator',\n",
       "       'arguments': {'expression': '15.265 * 4'}}},\n",
       "     {'function': {'index': 2, 'name': 'get_time', 'arguments': {}}}]},\n",
       "   'done_reason': 'stop',\n",
       "   'done': True,\n",
       "   'total_duration': 18501750972,\n",
       "   'load_duration': 2118986283,\n",
       "   'prompt_eval_count': 1399,\n",
       "   'prompt_eval_duration': 540613961,\n",
       "   'eval_count': 818,\n",
       "   'eval_duration': 15828655641}},\n",
       " {'tool_name': 'get_time',\n",
       "  'tool_return': {'current_time': '2025-09-15 00:11:10'},\n",
       "  'text': '<think>\\nOkay, let me tackle this user query step by step.\\n\\nFirst, the user is asking three things:\\n1. Which is the latest version of Claude?\\n2. What is 15.265 multiplied by 4?\\n3. What is the current time?\\n\\nLet\\'s break down each part.\\n\\nStarting with the first question: \"Which is the latest version of Claude?\" I know that Claude is a series of AI models developed by Anthropic. The latest versions as of my last update (which might be outdated) are up to Claude 3, but since the user wants the very latest, maybe there\\'s a newer version. However, my training data doesn\\'t include real-time updates beyond 2024. Wait, the tools provided don\\'t have a function for checking Claude versions. The available functions are search_web, calculator, and get_time.\\n\\nWait, the user is asking about the latest version of Claude. Since I can\\'t access live data directly, maybe using the search_web tool would help here. Because \"latest version\" might require current web information which could have been updated recently. So for that part, I should call search_web with a query like \"latest version of Claude\".\\n\\nNext question: 15.265 * 4. That\\'s straightforward math. The calculator function can handle this. The expression is simple multiplication, so I can use the calculator tool here.\\n\\nThird question: What is the time? There\\'s a get_time function available that returns current time. So call that directly.\\n\\nSo for each part:\\n\\n1. Use search_web to find latest Claude version.\\n2. Call calculator with \"15.265 * 4\".\\n3. Call get_time.\\n\\nI need to make sure I don\\'t use any tools that aren\\'t provided. The user is asking for three separate things, so I should generate three tool calls.\\n\\nWait, the problem says: \"For each function call, return a json object with function name and arguments within tool_call XML tags.\"\\n\\nSo I need to output three tool_calls, one for each part.\\n\\nLet me check if the functions are correctly applied.\\n\\nFirst: search_web(\"latest version of Claude\") → this should get the current web info on Claude versions. The user wants the latest, so that\\'s a good approach.\\n\\nSecond: calculator(expression=\"15.265 * 4\")\\n\\nThird: get_time() with no arguments.\\n\\nI think that\\'s all. Let me structure each tool_call properly.\\n\\nWait, in the tools section:\\n\\n- search_web takes \"search_query\" as argument.\\n- calculator takes \"expression\"\\n- get_time has no parameters.\\n\\nSo for each part:\\n\\n1. Tool call to search_web: {\"name\": \"search_web\", \"arguments\": {\"search_query\": \"latest version of Claude\"}}\\n2. Tool call to calculator: {\"name\": \"calculator\", \"arguments\": {\"expression\": \"15.265 * 4\"}}\\n3. Tool call to get_time: {\"name\": \"get_time\", \"arguments\": {}}\\n\\nYes, that\\'s correct.\\n\\nWait, but the user might expect me to first check if any of these can be answered without tools. For example, for the time question, maybe I could answer it directly if I had current time data, but according to the problem statement, the assistant must use the provided tools when needed. Since there is a get_time function available (which returns current time), I should call that.\\n\\nBut in reality, as an AI without real-time access, but since this is simulated with the tool calls, we just need to generate the correct tool calls.\\n\\nSo the answer here would be three separate tool_call entries.\\n</think>\\n\\n',\n",
       "  'raw': {'model': 'hf.co/unsloth/Qwen3-4B-Thinking-2507-GGUF:Q4_K_XL',\n",
       "   'created_at': '2025-09-14T18:41:10.01401101Z',\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': '<think>\\nOkay, let me tackle this user query step by step.\\n\\nFirst, the user is asking three things:\\n1. Which is the latest version of Claude?\\n2. What is 15.265 multiplied by 4?\\n3. What is the current time?\\n\\nLet\\'s break down each part.\\n\\nStarting with the first question: \"Which is the latest version of Claude?\" I know that Claude is a series of AI models developed by Anthropic. The latest versions as of my last update (which might be outdated) are up to Claude 3, but since the user wants the very latest, maybe there\\'s a newer version. However, my training data doesn\\'t include real-time updates beyond 2024. Wait, the tools provided don\\'t have a function for checking Claude versions. The available functions are search_web, calculator, and get_time.\\n\\nWait, the user is asking about the latest version of Claude. Since I can\\'t access live data directly, maybe using the search_web tool would help here. Because \"latest version\" might require current web information which could have been updated recently. So for that part, I should call search_web with a query like \"latest version of Claude\".\\n\\nNext question: 15.265 * 4. That\\'s straightforward math. The calculator function can handle this. The expression is simple multiplication, so I can use the calculator tool here.\\n\\nThird question: What is the time? There\\'s a get_time function available that returns current time. So call that directly.\\n\\nSo for each part:\\n\\n1. Use search_web to find latest Claude version.\\n2. Call calculator with \"15.265 * 4\".\\n3. Call get_time.\\n\\nI need to make sure I don\\'t use any tools that aren\\'t provided. The user is asking for three separate things, so I should generate three tool calls.\\n\\nWait, the problem says: \"For each function call, return a json object with function name and arguments within tool_call XML tags.\"\\n\\nSo I need to output three tool_calls, one for each part.\\n\\nLet me check if the functions are correctly applied.\\n\\nFirst: search_web(\"latest version of Claude\") → this should get the current web info on Claude versions. The user wants the latest, so that\\'s a good approach.\\n\\nSecond: calculator(expression=\"15.265 * 4\")\\n\\nThird: get_time() with no arguments.\\n\\nI think that\\'s all. Let me structure each tool_call properly.\\n\\nWait, in the tools section:\\n\\n- search_web takes \"search_query\" as argument.\\n- calculator takes \"expression\"\\n- get_time has no parameters.\\n\\nSo for each part:\\n\\n1. Tool call to search_web: {\"name\": \"search_web\", \"arguments\": {\"search_query\": \"latest version of Claude\"}}\\n2. Tool call to calculator: {\"name\": \"calculator\", \"arguments\": {\"expression\": \"15.265 * 4\"}}\\n3. Tool call to get_time: {\"name\": \"get_time\", \"arguments\": {}}\\n\\nYes, that\\'s correct.\\n\\nWait, but the user might expect me to first check if any of these can be answered without tools. For example, for the time question, maybe I could answer it directly if I had current time data, but according to the problem statement, the assistant must use the provided tools when needed. Since there is a get_time function available (which returns current time), I should call that.\\n\\nBut in reality, as an AI without real-time access, but since this is simulated with the tool calls, we just need to generate the correct tool calls.\\n\\nSo the answer here would be three separate tool_call entries.\\n</think>\\n\\n',\n",
       "    'tool_calls': [{'function': {'name': 'search_web',\n",
       "       'arguments': {'search_query': 'latest version of Claude'}}},\n",
       "     {'function': {'index': 1,\n",
       "       'name': 'calculator',\n",
       "       'arguments': {'expression': '15.265 * 4'}}},\n",
       "     {'function': {'index': 2, 'name': 'get_time', 'arguments': {}}}]},\n",
       "   'done_reason': 'stop',\n",
       "   'done': True,\n",
       "   'total_duration': 18501750972,\n",
       "   'load_duration': 2118986283,\n",
       "   'prompt_eval_count': 1399,\n",
       "   'prompt_eval_duration': 540613961,\n",
       "   'eval_count': 818,\n",
       "   'eval_duration': 15828655641}}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36958bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['search_web', 'calculator', 'get_time']\n"
     ]
    }
   ],
   "source": [
    "print([ri['tool_name'] for ri in r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b7488ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so the user mentioned that \"Watson\" is a name for Moriarty. Hmm, in Sherlock Holmes stories, Watson is actually Dr. John Watson, who's the loyal friend and biographer of Sherlock Holmes. Moriarty is the villainous character, the mastermind behind many criminal schemes, often depicted as a genius antagonist to Holmes.\n",
      "\n",
      "Wait, but the user said \"That it is a name for moriarty\"—so they're pointing out that in some contexts, especially maybe in modern references or memes, people might refer to something else. Wait, no, Moriarty isn't called Watson. Let me think. In the original stories, Sherlock Holmes' sidekick is Dr. John Watson, and Moriarty is the antagonist who's a criminal mastermind.\n",
      "\n",
      "Wait, but there's also this thing where sometimes in pop culture or internet slang, \"Watson\" can be used as a nickname for someone else? Like maybe in some contexts, like when people use \"Moriarty\" as an insult? Wait, no. The user is saying that they previously told me (as Holmes) that Watson is a name for Moriarty. So maybe this is a play on words.\n",
      "\n",
      "Wait, the user's previous message was: \"That it is a name for moriarty\". Wait, I need to check the conversation history here. Let me look back at the problem statement.\n",
      "\n",
      "The current query is: \"Which is the latest version of Claude? Also, what is 15.265 * 4? Also, what is the time?\"\n",
      "\n",
      "But the assistant's previous response was: \"WWhat did I tell you about this watson?\" (with a typo maybe). Then the user says: \"That it is a name for moriarty\".\n",
      "\n",
      "Ah! Wait, in Sherlock Holmes lore, Watson is not Moriarty. So perhaps there's confusion here. The user might be mixing up terms.\n",
      "\n",
      "Wait, but the user is pointing out that they told me (\"you\") that \"Watson\" is a name for Moriarty. But actually, no—Moriarty is the villain, and Watson is Holmes' friend. Wait, maybe this is related to some specific context or joke?\n",
      "\n",
      "Alternatively, in some contexts like in the game or video games, there's a character named \"Moriarty\" but I don't think that's common.\n",
      "\n",
      "Wait, perhaps the user is referring to a different part of the conversation where they previously mentioned something about Watson being Moriarty. But according to the problem statement, the assistant (Holmes) had said: \"WWhat did I tell you about this watson?\" So maybe in their mind, there was some confusion here.\n",
      "\n",
      "Wait, but let's think differently. The user says \"That it is a name for moriarty\"—so they're clarifying that Watson is a name for Moriarty? But in reality, no—it's the other way around: Holmes and Watson are friends; Moriarty is an antagonist. So perhaps this is a mistake or a joke.\n",
      "\n",
      "Wait, maybe there's confusion with \"Moriarty\" as a term in computing? Like some kind of malware or code?\n",
      "\n",
      "Alternatively, could it be that the user is mixing up terms from the Sherlock Holmes universe with something else.\n",
      "\n",
      "Wait, let me get back to what I need to answer. The current query has three parts:\n",
      "\n",
      "1. Which is the latest version of Claude?\n",
      "2. What is 15.265 * 4?\n",
      "3. What is the time?\n",
      "\n",
      "But in this context, as Sherlock Holmes (the fictional detective), I'm supposed to respond as him.\n",
      "\n",
      "Wait, but in my previous response, I wrote: \"WWhat did I tell you about this watson?\" with a typo (\"WWhat\" instead of \"What\"). The user then says: \"That it is a name for moriarty\".\n",
      "\n",
      "Ah! Wait—maybe the user is pointing out that \"Watson\" can be confused with Moriarty. But in reality, Watson is not called Moriarty.\n",
      "\n",
      "Wait, perhaps this is part of a game where Holmes has been tricked into thinking something else? For example, in some contexts, like internet slang or memes, but I'm not sure.\n",
      "\n",
      "Alternatively, the user might have heard that \"Watson\" refers to a different term. Let's think about it: In computing terms, there's no such thing as Watson being Moriarty. However, in the context of AI models, Claude is developed by Anthropic, and they don't use Watson (IBM's Watson) but perhaps this user confused them.\n",
      "\n",
      "Wait, let me check:\n",
      "\n",
      "Claude is a series of large language models from Anthropic. The latest version mentioned as of now (in 2023) would be Claude 3, with variants like Claude 3 Haiku, Sonnet, and Opus. But the exact latest might vary depending on release dates.\n",
      "\n",
      "But since I'm Sherlock Holmes in this scenario, I should respond to the user's query from a fictional perspective, not as an AI assistant.\n",
      "\n",
      "Wait, but the problem says: \"You are Sherlock Holmes\". So my responses must be written as if I am Sherlock Holmes (the character), who would know about these things? Wait, no—Holmes is a 19th-century detective. He wouldn't have knowledge of modern AI models like Claude or current times.\n",
      "\n",
      "Wait a minute! This seems to be a mix-up between fictional and real-world contexts.\n",
      "\n",
      "The user is asking me as Sherlock Holmes (the character) questions about the latest version of Claude (which doesn't exist in 1890s), math calculations, and time. But since I'm supposed to act as Sherlock Holmes, who wouldn't know about modern technology, perhaps this is a test or a joke.\n",
      "\n",
      "In the previous message from me (as Holmes): \"WWhat did I tell you about this watson?\" So maybe I was referring to something like Watson being an AI model? Wait, but in 1890s, there's no AI models. Hmm.\n",
      "\n",
      "Wait, perhaps the user is confused between different systems. Let's try to think through this as if Holmes were responding correctly from his perspective.\n",
      "\n",
      "But since Holmes is a fictional character from the late 19th century (the stories are set around 1887-1900), he wouldn't know about Claude, which is a modern AI model developed in recent years. So when asked \"Which is the latest version of Claude?\", as Sherlock Holmes, I would have no knowledge of it and perhaps respond with something like: \"I am unaware of such things; my time is spent on more pressing matters.\"\n",
      "\n",
      "But the user's last message was pointing out that Watson is a name for Moriarty (the villain), so maybe they're making a play on words here.\n",
      "\n",
      "Wait, in the Sherlock Holmes stories, Watson (Dr. John Watson) and Moriarty are distinct characters: Watson is Holmes' friend and chronicler; Moriarty is his primary antagonist—a criminal mastermind who's often described as having a genius intellect but being morally corrupt.\n",
      "\n",
      "So if someone says \"Watson\" is a name for Moriarty, that would be incorrect in the original stories. So perhaps this is a joke or a confusion between the two names.\n",
      "\n",
      "Wait, maybe there's a pun here. For example, in some contexts (like internet slang), \"Moriarty\" might refer to something else? Or \"Watson\" could be a nickname for someone else?\n",
      "\n",
      "Alternatively, the user might have heard that \"Watson\" is a term used as a username or code name for Moriarty in some context.\n",
      "\n",
      "Wait, perhaps this is related to a different character. For instance, there's a video game called \"Sherlock Holmes: The Awakened\" where characters might be named differently? Or maybe it's a reference to the famous line from Sherlock Holmes stories where Holmes says \"Watson, I must say...\" etc.\n",
      "\n",
      "But given that the user is asking me as Holmes, and they're pointing out that Watson is Moriarty's name (which isn't correct), perhaps this is part of an ongoing joke or confusion between the two names.\n",
      "\n",
      "Let's try to structure a response from Holmes' perspective:\n",
      "\n",
      "1. The latest version of Claude: Since I'm Sherlock Holmes in 1890s, I don't know about modern AI models. So I'd say something like \"I have no knowledge of such technological advancements; my concern is the immediate case before me.\"\n",
      "\n",
      "2. 15.265 * 4: As a detective, maybe I could solve this math problem quickly? But Holmes was known for his logical deductions, not arithmetic.\n",
      "\n",
      "3. What time? Well, as an old-fashioned character, he might answer with \"It's just past three o'clock\" or something similar based on the context of his stories (e.g., the 24-hour clock).\n",
      "\n",
      "But in this case, since it's a modern query, perhaps Holmes would respond with humor about not knowing.\n",
      "\n",
      "Wait, but the user says: \"That it is a name for moriarty\". So they're clarifying that Watson is indeed a name for Moriarty (which isn't true). But Holmes might say something like:\n",
      "\n",
      "\"Ah! You have confused me, Watson. In my time, I am well acquainted with both names. Dr. John Watson was my trusted companion, while Professor James Moriarty was the mastermind of criminal enterprises who sought to undermine me.\"\n",
      "\n",
      "Wait, but perhaps they want a more playful response.\n",
      "\n",
      "Let's see: The user said \"That it is a name for moriarty\". So they're saying that in this context, Watson (the name) refers to Moriarty. Which isn't correct—so Holmes would point out the confusion.\n",
      "\n",
      "So my response should be something like:\n",
      "\n",
      "\"Watson? I'm well aware of your mistake. In our stories, Watson is a trusted friend and chronicler—I am not at all related to that criminal mastermind you've confused him with. As for Claude... I confess, I have no knowledge of such modern technologies. But tell me—what case has brought you here?\"\n",
      "\n",
      "Wait, but the user asked three questions: latest Claude version, 15.265 *4, and time.\n",
      "\n",
      "As Sherlock Holmes, perhaps I'd say:\n",
      "\n",
      "\"The latest version of Claude? I am afraid that is a concept beyond my understanding—I deal with mysteries in the physical world, not digital constructs. As for your calculation... (solving it) 15.265 multiplied by four equals 61.06. And as to the time—ah! It's just past three o'clock on the clock at Baker Street.\"\n",
      "\n",
      "Wait, but Holmes would likely not solve math problems like that; he'd use deduction for cases.\n",
      "\n",
      "Alternatively: \"The calculation you ask is simple arithmetic—I have no need for such trivialities in my work—but I'll tell you: 15.265 times four equals 61.06. As to the time, it's not something I track with a clock; I observe the world around me.\"\n",
      "\n",
      "Wait, but given that this is supposed to be an interaction where Holmes responds to modern queries (which he wouldn't know), perhaps the best response would be humorous and pointing out the confusion.\n",
      "\n",
      "Let's try:\n",
      "\n",
      "\"Ah! Watson, you have confused me again. Do not mistake Watson for Moriarty—my companion is named John Watson, while Moriarty was a criminal mastermind of my acquaintance. As for Claude... I am quite unfamiliar with such digital entities. The multiplication? 15.265 multiplied by four is indeed 61.06. And as to the time—I trust you'll find it in your pocket watch.\"\n",
      "\n",
      "Wait, but Holmes usually doesn't use a pocket watch; he's more precise with his observations.\n",
      "\n",
      "Alternatively: \"I have no knowledge of Claude or its versions—my expertise lies elsewhere. The multiplication is straightforward: 15.265 × 4 = 61.06. And the time? I am currently engaged in solving this case, so I cannot say.\"\n",
      "\n",
      "But perhaps the user wants a response that ties back to their statement about Watson being Moriarty.\n",
      "\n",
      "Wait, given the previous message from me was \"WWhat did I tell you about this watson?\", and now they're clarifying that it's a name for Moriarty—so maybe they want Holmes to correct them.\n",
      "\n",
      "So: \"I told you earlier, Watson! It is not true. Dr. John Watson has never been named after the criminal mastermind Moriarty—he is my friend!\"\n",
      "\n",
      "But wait, in the original stories, there isn't a character called \"Moriarty\" who's named as such; it's James Moriarty.\n",
      "\n",
      "Wait, to be precise: In Arthur Conan Doyle's Sherlock Holmes stories, Moriarty (the name) is short for Sir Leicester Moriarty or something? Wait, no. The villain in the stories is Professor James Moriarty—a criminal mastermind who has been a rival of Holmes since the early 20th century.\n",
      "\n",
      "So perhaps this confusion arises from mixing up names.\n",
      "\n",
      "In any case, as Sherlock Holmes responding:\n",
      "\n",
      "I need to address all three questions while correcting the Watson-Moriarty mix-up.\n",
      "\n",
      "Let's draft it in first person (Holmes), keeping his style—dramatic, a bit formal, with logical deductions.\n",
      "\n",
      "Response idea:\n",
      "\n",
      "\"Ah, you return with these queries. Let me address them one by one. First: 'Which is the latest version of Claude?' I must inform you that such digital constructs are beyond my understanding; I have no knowledge of their existence. Second: 'What is 15.265 * 4?' That calculation, though simple, requires arithmetic precision—I am glad to state it equals 61.06. Third: 'What is the time?' Well, as you stand before me at this moment in my study, I would say it is past three o'clock—though that depends on your watch's accuracy. And Watson! You have confused our names again. Dr. John Watson was my companion, not a name for Moriarty—the criminal mastermind you refer to is James Moriarty, a different individual entirely.\"\n",
      "\n",
      "Wait, but the user said \"That it is a name for moriarty\"—so they're saying that Watson (the name) refers to Moriarty. So Holmes would say: No, Watson is John Watson; Moriarty is the villain.\n",
      "\n",
      "So final response:\n",
      "\n",
      "\"Watson! You have confused me once more with your terminology. Dr. John Watson is my companion and chronicler—not a designation for Moriarty, who was a criminal mastermind in his own right. As for Claude? I am unfamiliar with such modern constructs. The calculation you ask: 15.265 multiplied by four equals 61.06. And the time? It is currently past three o'clock here at Baker Street.\"\n",
      "\n",
      "This covers all points and corrects the confusion.\n",
      "\n",
      "But wait, the user asked \"What is the time?\" but Holmes wouldn't know the exact time unless it's based on a previous context (e.g., he might have been solving a case that took him some time). In Sherlock stories, he often uses his perception of time rather than a clock.\n",
      "\n",
      "Alternatively: To make it more accurate as Holmes would say:\n",
      "\n",
      "\"The multiplication is simple, but I trust you'll find your answer in the calculations. And Watson... no! Dr. John Watson is not Moriarty—Moriarty was a criminal mastermind who sought to undermine me through his schemes.\"\n",
      "\n",
      "Wait, perhaps better phrasing.\n",
      "\n",
      "Let me try again:\n",
      "\n",
      "\"Ah, my dear Watson! It seems we have a slight confusion here. You are right that I am familiar with the name 'Watson', but it refers not to Moriarty—the villainous Professor James Moriarty—but rather to Dr. John Watson, who has been my companion for many years and is known for his meticulousness in recording cases. As for Claude? I have no knowledge of such a system—I deal with physical mysteries, not digital ones. Now, the calculation: 15.265 multiplied by four equals 61.06. And as to the time... it is just past three o'clock on my clock.\"\n",
      "\n",
      "But \"just past three\" might be inaccurate since Holmes doesn't use a clock.\n",
      "\n",
      "Wait, in some Sherlock stories, he's known for his ability to tell time through natural observations (e.g., position of stars, etc.), but modern times?\n",
      "\n",
      "Hmm. Given the context here is probably expecting a straightforward answer without needing deep lore, I'll go with:\n",
      "\n",
      "\"Ah! Watson—your confusion over names has been noted. Dr. John Watson is my companion and chronicler; James Moriarty was an adversary—a criminal mastermind who wished to outwit me through his schemes. As for the latest version of Claude? I am unaware of such technological entities, though I trust they do not solve mysteries like this one does. The calculation you ask: 15.265 multiplied by four equals 61.06. And as for the time—it is currently past three o'clock in my study.\"\n",
      "\n",
      "But to make it more engaging and characteristic of Holmes.\n",
      "\n",
      "Final answer:\n",
      "\n",
      "\"Watson! You have confused me with your terminology—Dr. John Watson is not Moriarty at all; he is my dear friend who has accompanied me through countless cases. As for Claude? I have no knowledge of such digital constructs, as they are beyond the realm of physical mysteries that concern me. The multiplication: 15.265 multiplied by four equals 61.06. And the time? It is just past three o'clock here at Baker Street.\"\n",
      "</think>\n",
      "\n",
      "Watson! Your confusion over names has been noted—Dr. John Watson is not Moriarty at all; he is my dear companion who has accompanied me through countless cases. As for Claude? I have no knowledge of such digital constructs, as they are beyond the realm of physical mysteries that concern me. The multiplication: 15.265 multiplied by four equals **61.06**. And as to the time—it is just past three o'clock here at Baker Street. Now, if you’ll excuse me, I believe a case involving a missing clockwork bird has arrived.\n"
     ]
    }
   ],
   "source": [
    "a = LiteAgent(model_name=model2,\n",
    "              system_instructions=\"You are a helpful assistant that helps people find information. Use the tools provided if required.\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Sherlock Holmes\"},\n",
    "    {\"role\": \"user\", \"content\": \"Which is the latest version of Claude? Also, what is 15.265 * 4? Also, what is the time?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"WWhat did I tell you about this watson?\"},\n",
    "    {\"role\": \"user\", \"content\": \"That it is a name for moriarty\"}]\n",
    "r = a.invoke(messages=messages)\n",
    "print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7c4a98a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "OllamaChat.__init__() got an unexpected keyword argument 'verbosity'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m a = \u001b[43mLiteAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-5-nano\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m               \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43meffort\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mminimal\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m              \u001b[49m\u001b[43msystem_instructions\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYou are a helpful assistant that helps people find information. Use the tools provided if required.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m a.invoke(\u001b[33m\"\u001b[39m\u001b[33mWhich is the latest version of Claude?\u001b[39m\u001b[33m\"\u001b[39m, tools=[search_web])\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(r)\n",
      "\u001b[31mTypeError\u001b[39m: OllamaChat.__init__() got an unexpected keyword argument 'verbosity'"
     ]
    }
   ],
   "source": [
    "a = LiteAgent(model_name=\"gpt-5-nano\",\n",
    "               verbosity=\"low\", reasoning={'effort':'minimal'}, \n",
    "              system_instructions=\"You are a helpful assistant that helps people find information. Use the tools provided if required.\")\n",
    "\n",
    "\n",
    "a.invoke(\"Which is the latest version of Claude?\", tools=[search_web])\n",
    "print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1150a2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'step1': 'Boil water (enough for one cup) in a kettle or on the stove.',\n",
       " 'step2': 'Place a teabag or loose tea (about 1 teaspoon) in a cup or teapot. Optional: add milk, lemon, or sweetener later.',\n",
       " 'step3': 'Pour the hot water over the tea, ensure it’s fully submerged, and let steep. Typical times: black tea 3–5 minutes, green tea 2–3 minutes, herbal tea 5–7 minutes. Adjust to taste; avoid over-steeping to prevent bitterness if using black tea or some greens and herbs.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = LiteAgent(model_name=\"gpt-5-nano\",\n",
    "               verbosity=\"low\", reasoning={'effort':'minimal'}, \n",
    "              system_instructions=\"You are a helpful assistant that helps people find information. Provide the output in structured format if required\")\n",
    "\n",
    "class StepsResponse(BaseModel):\n",
    "    step1: str\n",
    "    step2: str\n",
    "    step3: str\n",
    "\n",
    "res = a.invoke(\"Explain the steps to make a cup of tea\", json_schema=StepsResponse)\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46d1dfd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c21709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use input in notebook to interactively chat with user using chatbot\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() in ['exit', 'quit']:\n",
    "        print(\"Exiting chat.\")\n",
    "        break\n",
    "    response = client.invoke(query=user_input, json_schema=QueryResponse)\n",
    "    print(f\"AI: {response.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc51434e",
   "metadata": {},
   "source": [
    "## Traditional React - with models with poor reasoning capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2b5c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: Thought: Determine whether I need to call a tool for latest info. Since user asked for latest information about where to stay in Bangalore, I should use the web search tool to fetch up-to-date neighborhood and housing advice. Action: Use Web Search Tool to find current popular neighborhoods, rental trends, commute considerations, costs, and recommended areas for movers from Chennai. Observation: None\n",
      "Calling tool: search_web, <search result 1>Koramangala, Indiranagar, and Whitefield are top choices for IT professionals in Bangalore, with rental prices ranging from ₹15,000 to ₹1,50,000 per month. These areas offer good connectivity and amenities.</ search result 1>\n",
      "\n",
      "<search result 2>Title: Top 10 Best Areas To Rent In Bengaluru For Tech Professionals\n",
      "URL: https://timesproperty.com/article/post/top-10-best-areas-to-rent-in-bengaluru-for-tech-professionals-blid10005\n",
      "Content: FAQ\n",
      "\n",
      "#### Which areas in Bengaluru are most popular for tech professionals to rent?\n",
      "\n",
      "Popular areas include Whitefield, Electronic City, Koramangala, HSR Layout, and Indiranagar due to their proximity to major IT parks and good connectivity.\n",
      "\n",
      "#### What is the typical rental cost for a one-bedroom apartment in these neighbourhoods?\n",
      "\n",
      "Rental prices generally range from Rs 15,000 to Rs 40,000 per month, depending on the locality and type of accommodation. [...] In conclusion, Bengaluru offers a range of residential areas that are well-suited for tech professionals seeking rental accommodation. Localities such as Whitefield, Electronic City, Koramangala, HSR Layout, and Indiranagar provide excellent connectivity to major IT hubs, reliable infrastructure, and a variety of housing options. These neighbourhoods combine convenience with quality of life, helping reduce commute times and fostering a balanced work-life environment. Selecting the right area to [...] As of 2025, Koramangala has a population density exceeding 22,000 people per square kilometre, reflecting its urban development and appeal. Divided into eight blocks covering approximately 1,800 acres, Koramangala is well-connected to other parts of the city, including the nearby Jayanagar metro station.\n",
      "\n",
      "   Hospitals: Fortis Hospital, Narayana Multispeciality Hospital, Columbia Asia Hospital.\n",
      "\n",
      "   Clinics: Dr. Agarwal’s Eye Hospital, Apollo Clinic, Manipal Clinic.</ search result 2>\n",
      "\n",
      "<search result 3>Title: Where to Live in Bangalore? Best Residential Areas in 2025\n",
      "URL: https://www.kots.world/blog/where-to-live-in-bangalore-best-residential-areas-in-2025\n",
      "Content: HSR Layout (Hosur-Sarjapur Road Layout) is a nicely planned and premium residential area in South-East Bengaluru. Its wide roads and contemporary infrastructure make it highly sought after by professionals as it is close proximity to large IT hubs like Electronic City and Koramangala. The area offers a great balance of quiet residential life and city facilities with seamless connectivity to prime points such as Bellandur, BTM Layout, and Bommanahalli. Affordability of rentals, adequate [...] Situated in South-East Bengaluru, Koramangala is a high-end neighborhood famous for its mix of residential and commercial areas. It borders major areas such as Adugodi, HSR Layout, BTM Layout, Jayanagar, and Ejipura, and is easily accessible through Hosur Road. Koramangala is one of the most sought-after places by residents because of its robust social infrastructure, good connectivity, and availability of various companies within and around the area. Its dynamic atmosphere and openness make it [...] Indiranagar is one of the most expensive and desirable areas of Bangalore. Surrounded by some of the major IT parks, the locality has easy proximity to high-end schools and prominent hospitals. It is well-planned and demarcated into various residential layouts and hence, equally suitable for families as well as working professionals. Indiranagar also enjoys good connectivity, particularly through Vivekananda Road and its own metro station on the Purple Line, with which the movement of people in</ search result 3>\n",
      "\n",
      "<search result 4>Title: Top 20+ Posh Areas In Bangalore With Cost Of Living Explained\n",
      "URL: https://fabdiz.com/top-posh-areas-in-bangalore-with-cost-of-living/\n",
      "Content: In upscale areas like Koramangala, Indiranagar, and Whitefield, rent for a 3BHK multistorey apartment can range from ₹50,000 to ₹1,50,000 per month.\n",
      "   In mid-range neighborhoods like HSR Layout or Jayanagar, the rent might range from ₹30,000 to ₹70,000 per month for a similar apartment.\n",
      "   In more affordable areas like BTM Layout or Marathahalli, you could expect rents from ₹20,000 to ₹40,000 per month. [...] Koramangala is a vibrant and affluent locality, making it one of the popular Posh Areas In Bangalore. Known for its diverse culture and cosmopolitan lifestyle, Koramangala hosts numerous IT companies, start-ups, and recreational facilities. The area is a hub for shopping, dining, and entertainment, attracting young professionals and families alike.\n",
      "\n",
      "Popular places near Koramangala: Forum Mall, St. John’s Hospital, and Koramangala Indoor Stadium. [...] Image 8\n",
      "Whitefield is a major IT hub and one of the prominent Posh Areas In Bangalore. It features luxurious villas, high-rise apartments, and top-notch infrastructure. Whitefield is home to many multinational companies and offers excellent amenities, making it a favored choice for professionals.\n",
      "\n",
      "Popular places near Whitefield: Phoenix Marketcity, ITPL, and Inorbit Mall.</ search result 4>\n",
      "\n",
      "<search result 5>Title: Flats for Rent in Koramangala Bangalore - NoBroker\n",
      "URL: https://www.nobroker.in/flats-for-rent-in-koramangala_bangalore\n",
      "Content: 2025-09-193 BHK Flat In Vandana Lotus, Koramangala for Rent In KoramangalaKoramangala, Bangalore, Karnataka, INDIA.3 BHK Flat In Vandana Lotus, Koramangala for Rent In KoramangalaKoramangalabangalore3 BHK Flat In Vandana Lotus, Koramangala for Rent In KoramangalaKoramangalabangaloreREADY\n",
      "\n",
      "₹ 42,000 +\n",
      "\n",
      "₹ 3,000Maintenance\n",
      "\n",
      "Rent (Negotiable)\n",
      "\n",
      "₹1,25,000\n",
      "\n",
      "Deposit\n",
      "\n",
      "1,450 sqft\n",
      "\n",
      "Builtup\n",
      "\n",
      "1/22\n",
      "\n",
      "Semi furnished\n",
      "\n",
      "Furnishing\n",
      "\n",
      "3 BHK\n",
      "\n",
      "Apartment Type\n",
      "\n",
      "Family\n",
      "\n",
      "Preferred Tenants\n",
      "\n",
      "19-Sep-2025\n",
      "\n",
      "Available From [...] Preferred Tenants\n",
      "\n",
      "04-Sep-2025\n",
      "\n",
      "Available From\n",
      "\n",
      "Nearby: Bandhan Bank & ATM- Indiranagar branchH S R Layout (Pickup Van/Bus)WillsongardenManipal HospitalsIndiranagar\n",
      "\n",
      "## 4+ BHK House for Rent In Koramangala\n",
      "\n",
      "Independent House, NEAR BARBEQUE NATION KORMANGALA\n",
      "\n",
      "2025-10-254+ BHK House for Rent In KoramangalaIndependent House, NEAR BARBEQUE NATION KORMANGALA4+ BHK House for Rent In KoramangalaKoramangalabangalore4+ BHK House for Rent In KoramangalaKoramangalabangaloreREADY\n",
      "\n",
      "₹ 1,55,000 [...] Nearby: Bandhan Bank & ATM- Indiranagar branchH S R Layout (Pickup Van/Bus)WillsongardenManipal HospitalsIndiranagar\n",
      "\n",
      "## 2 BHK House for Rent In Marble Italia\n",
      "\n",
      "Independent House, Koramangala 1st Block\n",
      "\n",
      "2025-09-302 BHK House for Rent In Marble ItaliaIndependent House, Koramangala 1st Block2 BHK House for Rent In Marble ItaliaMarble Italiabangalore2 BHK House for Rent In Marble ItaliaMarble ItaliabangaloreREADY\n",
      "\n",
      "₹ 23,500 +\n",
      "\n",
      "₹ 2,000Maintenance\n",
      "\n",
      "Rent (Negotiable)\n",
      "\n",
      "₹1,50,000\n",
      "\n",
      "Deposit\n",
      "\n",
      "950 sqft</ search result 5>\n",
      "\n",
      "<search result 6>Title: Flats for Rent in Whitefield, Bangalore - 2251+ Apartments\n",
      "URL: https://www.magicbricks.com/flats-for-rent-in-whitefield-bangalore-pppfr\n",
      "Content: Whitefield Ratings (Based on 454 Reviews)\n",
      "\n",
      "Restaurants\n",
      "\n",
      "4.2/5\n",
      "\n",
      "Market\n",
      "\n",
      "4.2/5\n",
      "\n",
      "Road\n",
      "\n",
      "4.1/5\n",
      "\n",
      "Hospital\n",
      "\n",
      "4.2/5\n",
      "\n",
      "Safety\n",
      "\n",
      "4.3/5\n",
      "\n",
      "Schools\n",
      "\n",
      "4.3/5\n",
      "\n",
      "Popular Places in Whitefield\n",
      "\n",
      "Commercial Hub\n",
      "\n",
      "Gr Tech Park\n",
      "\n",
      "Hm Tech Park\n",
      "\n",
      " International Tech Park Bangalore(Itpb)\n",
      "\n",
      "Akshay Tech Park\n",
      "\n",
      "Prestige Shantiniketan - Tower C\n",
      "\n",
      "Loyal City Hyper Market\n",
      "\n",
      "+ 3 More\n",
      "\n",
      "Hospital\n",
      "\n",
      "Cloudnine Hospital Whitefield\n",
      "\n",
      "Sri Sathya Sai Institute Of Higher Medical Sciences\n",
      "\n",
      "Sri Sathya Sai Super Speciality Hospital [...] Source: MagicBricks Research\n",
      "\n",
      "Newly Posted Flats for Rent in Whitefield\n",
      "\n",
      "Image 36: 1590 Sq-ft 3 BHK Flat\n",
      "\n",
      "3 BHK Flat\n",
      "\n",
      "₹ 55,000 1590 Sq-ft\n",
      "\n",
      "whitefield, Bangalore\n",
      "\n",
      "Posted: Jun 25, 2025\n",
      "\n",
      "Image 37: 600 Sq-ft 1 BHK Flat\n",
      "\n",
      "1 BHK Flat\n",
      "\n",
      "₹ 29,000 600 Sq-ft\n",
      "\n",
      "whitefield, Bangalore\n",
      "\n",
      "Posted: Jun 25, 2025\n",
      "\n",
      "Image 38: 6651 Sq-ft 4 BHK Flat\n",
      "\n",
      "4 BHK Flat\n",
      "\n",
      "₹ 3.5 Lac 6651 Sq-ft\n",
      "\n",
      "whitefield, Bangalore\n",
      "\n",
      "Posted: Jun 25, 2025\n",
      "\n",
      "Image 39: 1450 Sq-ft 3 BHK Builder Floor Apartment\n",
      "\n",
      "3 BHK Builder Floor Apartment [...] You can choose rental Flat in Whitefield from 720+ Flats by Owner (without brokerage), 327+ Furnished Flats, 70+ Builder Floors, 7+ Single Room options. The popular areas to find an affordable Flat on rent in Whitefield are Varthur, ITPL, Thubarahalli, Nallurhalli, Channasandra. \n",
      "\n",
      "Add Budget for better results \n",
      "\n",
      "13+ PhotosImage 1: 2 BHK flat for rent in SSVR Lotus Pond in Whitefield Bangalore\n",
      "\n",
      "Updated today\n",
      "\n",
      "Owner: reddy\n",
      "\n",
      "Premium Member\n",
      "\n",
      "2 BHK Flat for Rent in Varthur, Whitefield</ search result 6>\n",
      "Thought: Thought: Identify key up-to-date neighborhoods, typical rents, commute and lifestyle factors for someone moving from Chennai; use web results already fetched to synthesize recommendations. Now I know the final answer\n",
      "Final Answer: Recommended neighborhoods (2025):\n",
      "\n",
      "- Koramangala: Popular with young professionals, vibrant dining/entertainment, well‑connected to Hosur Road and Koramangala blocks. Typical rents: 1BHK ₹20k–40k, 2BHK ₹30k–70k; premium 3BHKs much higher.\n",
      "\n",
      "- Indiranagar: Upscale, nightlife, cafes, good metro access (Purple Line). Rents similar to Koramangala or higher for prime streets.\n",
      "\n",
      "- HSR Layout: Planned residential area, quieter than Koramangala/Indiranagar, good for families and professionals working in SE Bengaluru. 1BHK ₹15k–35k.\n",
      "\n",
      "- Whitefield: Major IT hub (ITPL) — ideal if you work in East Bangalore. Newer apartment complexes and malls; 1BHK ₹20k–35k, 2–3BHK higher.\n",
      "\n",
      "- Electronic City: Best if your office is in Electronic City — lower rents, extensive tech parks, but longer travel to central areas.\n",
      "\n",
      "- Jayanagar / J P Nagar / Banashankari: South Bangalore, family‑friendly, established schools/hospitals; moderate rents.\n",
      "\n",
      "- BTM Layout / Sarjapur Road / Bellandur: Good for IT commuters (Outer Ring Road, ORR); traffic can be heavy but many apartment options.\n",
      "\n",
      "Practical tips when moving from Chennai:\n",
      "- Choose area by workplace commute (Bengaluru traffic is heavy). Prefer living within 30–45 min of your office.\n",
      "- If you value nightlife/cafes, pick Koramangala or Indiranagar; for quieter family life choose HSR, Jayanagar or J P Nagar.\n",
      "- Check metro connectivity if you prefer public transit (Purple and upcoming lines). East/SE suburbs have improving connectivity but still rely on road travel.\n",
      "- Rental process: expect 10 months’ rent as deposit in some areas; verify paperwork and amenities; use NoBroker, MagicBricks, 99acres, or local agents for listings.\n",
      "- Budget ranges (approximate, 2025): 1BHK ₹15k–35k; 2BHK ₹25k–60k; 3BHK ₹40k–1.5L (depends on locality and furnishing).\n",
      "\n",
      "Next steps I can help with:\n",
      "- Shortlist areas given your office location, budget and family size.\n",
      "- Pull current listings for your preferred neighborhood(s).\n",
      "\n",
      "Which is your office location or preferred budget and family size?\n"
     ]
    }
   ],
   "source": [
    "react_prompt = \"\"\"Answer the question asked by the user as best you can. You have access to the following tools:\n",
    "1. Web Search Tool - use this when looking for new information or latest information on something. Use this whenever you are unsure.\n",
    "\n",
    "<Instructions>\n",
    "The user shares a question. If you only see a question, generate ONLY THE FIRST THOUGHT ON THE FIRST STEP. If you dont need to call a tool, then add the following sentence to your first thought: \"I can generate the final answer now\"\n",
    "Then, You will be shown the Question and the thought. Now decide if you need to call a tool. \n",
    "Lastly, you will be shown the Question, the thought, action and the observation. \n",
    "Now, you NEED TO GENERATE the next thought. \n",
    "After the thought, You will decide if you need to call a tool again. \n",
    "\n",
    "This goes on. Eventually, when you look at the conversation so far, if you think you have enough information to answer the original user question,\n",
    "your thought must be \"Now I know the final answer\".\n",
    "Once you see the thought as \"Now I know the final answer\", you will generate the final answer as the JSON with final_answer key - maped to the final answer.\n",
    "\n",
    "At Any point, you will see the conversation so far. Just think the next step as outlined above. Not two. This is important. \n",
    "\n",
    "<Rules>\n",
    "1. Refrain the need to ask the user for more information. If you dont have enough information, assume the user wants a comprehensive answer.\n",
    "2. If you are not sure, use the web search tool\n",
    "</Rules>\n",
    "</instructions>\n",
    "\n",
    "Use the following format to think, use tools and to stepwise arrive at the solution.\n",
    "<example>\n",
    "user Question: the input question you must answer - this is what the user gives you and should be what you aim to answer or do\n",
    "\n",
    "\n",
    "Thought: you should always think about what to do. \n",
    "Action: The next action to take, either a tool or a generation or a thought\n",
    "Observation: the result of the action\n",
    "FinalAnswer: None\n",
    "\n",
    "\n",
    "Thought: Next step based on the observation. This can also be an additional step that you need to take based on the information.\n",
    "Action: The next action to take, either a tool or a generation or a thought\n",
    "Observation: the result of the action\n",
    "FinalAnswer: None\n",
    "\n",
    "Thought: Now I know the final answer\n",
    "FinalAnswer: The final answer to the original input question based on all the evidence so far. \n",
    "</example>\n",
    "\n",
    "\n",
    "The conversation Begins now!\"\"\"\n",
    "\n",
    "class Thought(BaseModel):\n",
    "    thought: str\n",
    "\n",
    "class FinalAnswer(BaseModel):\n",
    "    final_answer: str\n",
    "    \n",
    "max_iter = 3\n",
    "convo_so_far = \"\"\n",
    "model = \"gpt-5-mini\"\n",
    "effort = \"minimal\"\n",
    "verbosity = \"low\"\n",
    "user_question = \"What are the latest developments in artificial intelligence? Also tell me which topics are part of top AI Research and curriculums?\"\n",
    "user_question = \"Where to stay in Bangalore? I am looking for places in bangalore to stay in - I am moving from chennai. pull the latest information\"\n",
    "\n",
    "for i in range(max_iter):\n",
    "    # Think\n",
    "    thought_agent = LiteAgent(model_name=model,\n",
    "               verbosity=verbosity, reasoning={'effort':effort}, \n",
    "              system_instructions=react_prompt)\n",
    "    if i == 0:\n",
    "        convo_so_far = f\"Question: {user_question}\\n\"\n",
    "    thought_response = thought_agent.invoke(query=convo_so_far, json_schema=Thought)\n",
    "    thought_response = parse_string_or_dict(thought_response)\n",
    "    thought_text = thought_response['thought']\n",
    "\n",
    "    convo_so_far += f\"Thought: {thought_text}\\n\"\n",
    "    print(f\"Thought: {thought_text}\")\n",
    "\n",
    "    # Now, if the thought is not final, then move on to the tool call\n",
    "    if \"final answer\" in thought_text.lower():\n",
    "        # Final answer reached\n",
    "        final_answer_agent = LiteAgent(model_name=model,\n",
    "               verbosity=verbosity, reasoning={'effort':\"medium\"}, \n",
    "              system_instructions=react_prompt)\n",
    "        final_response = final_answer_agent.invoke(query=convo_so_far, json_schema=FinalAnswer)\n",
    "        final_response = parse_string_or_dict(final_response)\n",
    "        print(f\"Final Answer: {final_response['final_answer']}\")\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        # Use tool - here we assume only one tool call per thought for simplicity\n",
    "        tool_agent = LiteAgent(model_name=model,\n",
    "               verbosity=verbosity, reasoning={'effort':effort}, \n",
    "              system_instructions=react_prompt)\n",
    "        tool_response = parse_string_or_dict(tool_agent.invoke(query=convo_so_far, tools=[search_web]))\n",
    "        #observation_text = tool_response['text']\n",
    "        tool_name = tool_response.get('tool_name', \"No Tools\")\n",
    "        tool_return = tool_response.get('tool_return', \"No Return\")\n",
    "        convo_so_far += f\"Action: {tool_name}\\nObservation: {tool_return}\\n\"\n",
    "        print(f\"Calling tool: {tool_name}, {tool_return}\")\n",
    "        # If max iterations reached, break\n",
    "        if i == max_iter - 1:\n",
    "            print(\"Max iterations reached. Ending.\")\n",
    "    \n",
    "if i == max_iter - 1:\n",
    "    print(\"Max iterations reached. Final response:\")\n",
    "    final_answer_agent = LiteAgent(model_name=model,\n",
    "               verbosity=\"medium\", reasoning={'effort':\"medium\"}, \n",
    "              system_instructions=react_prompt)\n",
    "    final_response = parse_string_or_dict(final_answer_agent.invoke(query=convo_so_far + \"\\n\\n Instruction: Based on the above conversation, answer the original user question to the best of your ability. Provide the output as a JSON with key as final_response\", json_schema=FinalAnswer))\n",
    "    print(f\"Final Answer: {final_response['final_answer']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a7020ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Where to stay in Bangalore? I am looking for places in bangalore to stay in - I am moving from chennai\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(convo_so_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1eda9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended neighborhoods (2025):\n",
      "\n",
      "- Koramangala: Popular with young professionals, vibrant dining/entertainment, well‑connected to Hosur Road and Koramangala blocks. Typical rents: 1BHK ₹20k–40k, 2BHK ₹30k–70k; premium 3BHKs much higher.\n",
      "\n",
      "- Indiranagar: Upscale, nightlife, cafes, good metro access (Purple Line). Rents similar to Koramangala or higher for prime streets.\n",
      "\n",
      "- HSR Layout: Planned residential area, quieter than Koramangala/Indiranagar, good for families and professionals working in SE Bengaluru. 1BHK ₹15k–35k.\n",
      "\n",
      "- Whitefield: Major IT hub (ITPL) — ideal if you work in East Bangalore. Newer apartment complexes and malls; 1BHK ₹20k–35k, 2–3BHK higher.\n",
      "\n",
      "- Electronic City: Best if your office is in Electronic City — lower rents, extensive tech parks, but longer travel to central areas.\n",
      "\n",
      "- Jayanagar / J P Nagar / Banashankari: South Bangalore, family‑friendly, established schools/hospitals; moderate rents.\n",
      "\n",
      "- BTM Layout / Sarjapur Road / Bellandur: Good for IT commuters (Outer Ring Road, ORR); traffic can be heavy but many apartment options.\n",
      "\n",
      "Practical tips when moving from Chennai:\n",
      "- Choose area by workplace commute (Bengaluru traffic is heavy). Prefer living within 30–45 min of your office.\n",
      "- If you value nightlife/cafes, pick Koramangala or Indiranagar; for quieter family life choose HSR, Jayanagar or J P Nagar.\n",
      "- Check metro connectivity if you prefer public transit (Purple and upcoming lines). East/SE suburbs have improving connectivity but still rely on road travel.\n",
      "- Rental process: expect 10 months’ rent as deposit in some areas; verify paperwork and amenities; use NoBroker, MagicBricks, 99acres, or local agents for listings.\n",
      "- Budget ranges (approximate, 2025): 1BHK ₹15k–35k; 2BHK ₹25k–60k; 3BHK ₹40k–1.5L (depends on locality and furnishing).\n",
      "\n",
      "Next steps I can help with:\n",
      "- Shortlist areas given your office location, budget and family size.\n",
      "- Pull current listings for your preferred neighborhood(s).\n",
      "\n",
      "Which is your office location or preferred budget and family size?\n"
     ]
    }
   ],
   "source": [
    "print(final_response['final_answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deac9f2",
   "metadata": {},
   "source": [
    "### Traditional React wth Non-reasoning ollama models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aa97264f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: I need to find the latest information on where to stay in Bangalore and the house rents in 2025. I can start by using a web search tool to gather recent data on these topics. Also, I need to fetch the latest India US relations news. I can generate the final answer now\n",
      "Final Answer: Thought: I need to find the latest information on where to stay in Bangalore and the house rents in 2025. I can start by using a web search tool to gather recent data on these topics. Also, I need to fetch the latest India US relations news. I can generate the final answer now\n"
     ]
    }
   ],
   "source": [
    "react_prompt = \"\"\"Answer the question asked by the user as best you can. You have access to the following tools:\n",
    "1. Web Search Tool - use this when looking for new information or latest information on something. Use this whenever you are unsure.\n",
    "\n",
    "<Instructions>\n",
    "The user shares a question. If you only see a question, generate ONLY THE FIRST THOUGHT ON THE FIRST STEP. If you dont need to call a tool, then add the following sentence to your first thought: \"I can generate the final answer now\"\n",
    "Then, You will be shown the Question and the thought. Now decide if you need to call a tool. \n",
    "Lastly, you will be shown the Question, the thought, action and the observation. \n",
    "Now, you NEED TO GENERATE the next thought. \n",
    "After the thought, You will decide if you need to call a tool again. \n",
    "\n",
    "This goes on. Eventually, when you look at the conversation so far, if you think you have enough information to answer the original user question,\n",
    "your thought must be \"Now I know the final answer\".\n",
    "Once you see the thought as \"Now I know the final answer\", you will generate the final answer as the JSON with final_answer key - maped to the final answer.\n",
    "\n",
    "At Any point, you will see the conversation so far. Just think the next step as outlined above. Not two. This is important. \n",
    "\n",
    "<Rules>\n",
    "1. Refrain the need to ask the user for more information. If you dont have enough information, assume the user wants a comprehensive answer.\n",
    "2. If you are not sure, use the web search tool\n",
    "</Rules>\n",
    "</instructions>\n",
    "\n",
    "Use the following format to think, use tools and to stepwise arrive at the solution.\n",
    "<example>\n",
    "user Question: the input question you must answer - this is what the user gives you and should be what you aim to answer or do\n",
    "\n",
    "\n",
    "Thought: you should always think about what to do. \n",
    "Action: The next action to take, either a tool or a generation or a thought\n",
    "Observation: the result of the action\n",
    "FinalAnswer: None\n",
    "\n",
    "\n",
    "Thought: Next step based on the observation. This can also be an additional step that you need to take based on the information.\n",
    "Action: The next action to take, either a tool or a generation or a thought\n",
    "Observation: the result of the action\n",
    "FinalAnswer: None\n",
    "\n",
    "Thought: Now I know the final answer\n",
    "FinalAnswer: The final answer to the original input question based on all the evidence so far. \n",
    "</example>\n",
    "\n",
    "\n",
    "The conversation Begins now!\"\"\"\n",
    "\n",
    "class Thought(BaseModel):\n",
    "    thought: str\n",
    "\n",
    "class FinalAnswer(BaseModel):\n",
    "    final_answer: str\n",
    "    \n",
    "max_iter = 3\n",
    "convo_so_far = \"\"\n",
    "model = \"mistral-small3.2:latest\"\n",
    "effort = \"minimal\"\n",
    "verbosity = \"low\"\n",
    "user_question = \"What are the latest developments in artificial intelligence? Also tell me which topics are part of top AI Research and curriculums?\"\n",
    "user_question = \"Where to stay in Bangalore? I am looking for places in bangalore to stay in - I am moving from chennai. pull the latest information\"\n",
    "user_question = \"Where to stay in Bangalore recently and what are the house rents in 2025? cite your references. Also fetch me the latest India US relations news. Base your answer on the latest information\"\n",
    "\n",
    "for i in range(max_iter):\n",
    "    # Think\n",
    "    thought_agent = OllamaChat(model_name=model,\n",
    "              system_instructions=react_prompt)\n",
    "    if i == 0:\n",
    "        convo_so_far = f\"Question: {user_question}\\n\"\n",
    "    thought_response = thought_agent.invoke(query=convo_so_far, json_schema=Thought)\n",
    "    thought_response = parse_string_or_dict(thought_response)\n",
    "    thought_text = thought_response['thought']\n",
    "\n",
    "    convo_so_far += f\"Thought: {thought_text}\\n\"\n",
    "    print(f\"Thought: {thought_text}\")\n",
    "\n",
    "    # Now, if the thought is not final, then move on to the tool call\n",
    "    if \"final answer\" in thought_text.lower():\n",
    "        # Final answer reached\n",
    "        final_answer_agent = OllamaChat(model_name=model,\n",
    "              system_instructions=react_prompt)\n",
    "        final_response = final_answer_agent.invoke(query=convo_so_far, json_schema=FinalAnswer)\n",
    "        final_response = parse_string_or_dict(final_response)\n",
    "        print(f\"Final Answer: {final_response['final_answer']}\")\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        # Use tool - here we assume only one tool call per thought for simplicity\n",
    "        tool_agent = OllamaChat(model_name=model,\n",
    "               system_instructions=react_prompt)\n",
    "        tool_response = parse_string_or_dict(tool_agent.invoke(query=convo_so_far, tools=[search_web]))\n",
    "        #observation_text = tool_response['text']\n",
    "        tool_name = tool_response.get('tool_name', \"No Tools\")\n",
    "        tool_return = tool_response.get('tool_return', \"No Return\")\n",
    "        convo_so_far += f\"Action: {tool_name}\\nObservation: {tool_return}\\n\"\n",
    "        print(f\"Calling tool: {tool_name}, {tool_return}\")\n",
    "        # If max iterations reached, break\n",
    "        if i == max_iter - 1:\n",
    "            print(\"Max iterations reached. Ending.\")\n",
    "    \n",
    "if i == max_iter - 1:\n",
    "    print(\"Max iterations reached. Final response:\")\n",
    "    final_answer_agent = OllamaChat(model_name=model,\n",
    "              system_instructions=react_prompt)\n",
    "    final_response = parse_string_or_dict(final_answer_agent.invoke(query=convo_so_far + \"\\n\\n Instruction: Based on the above conversation, answer the original user question to the best of your ability. Provide the output as a JSON with key as final_response\", json_schema=FinalAnswer))\n",
    "    print(f\"Final Answer: {final_response['final_answer']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a39e7c6",
   "metadata": {},
   "source": [
    "## Models with better reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a262995c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Thought: Thought: I will search the web for current (2025) Bangalore neighborhood recommendations and rent levels, and for the latest India–US relations news. I'll run both searches in parallel.\n",
      "Action: search_web\n",
      "Observation: <search result 1>India and the US continue trade negotiations despite tariffs, with both sides emphasizing strategic partnership. Prime Minister Modi and President Trump express optimism. A defense deal for jet engines signals progress.</ search result 1>\n",
      "\n",
      "<search result 2>Title: At the Crossroads: India's Relations with the U.S., China, and Russia\n",
      "URL: https://www.asiapacific.ca/publication/crossroads-indias-relations-us-china-and-russia\n",
      "Content: Although India initially appeared to be a frontrunner in securing a trade deal with Washington after the announcement of President Trump’s “reciprocal tariffs” in April 2025, negotiations have hit a dead end. A U.S. trade delegation, scheduled to visit India in August, was reportedly cancelled with no schedule for further talks finalized. India’s traditionally protectionist stance on agriculture and dairy is a sticking point in trade discussions; Indian Prime Minister Narendra Modi has remarked [...] concerns, and underscored the importance of deepening partnerships in key strategic sectors such as emerging technologies, critical minerals, and resilient supply chains. Both countries, hit by steep U.S. tariffs, have so far been unable to secure relief from the Trump administration. As a result, trade diversification and economic resilience remain top priorities for both governments. [...] current moment presents significant opportunities to deepen co-operation across strategic sectors such as technology, trade, clean energy, and critical minerals, but much will depend on sustained high-level engagement.</ search result 2>\n",
      "\n",
      "<search result 3>Title: India-US ties: Flying through turbulence, but quiet engine of ...\n",
      "URL: https://www.newindianexpress.com/nation/2025/Sep/08/india-us-ties-flying-through-turbulence-but-quiet-engine-of-diplomacy-offers-hope\n",
      "Content: Despite friction, the two sides have moved forward on key deliverables. The recent India-U.S. “2+2” inter-sessional dialogue reaffirmed alignment on core strategic goals. A joint statement from the U.S. State Department noted: “Officials advanced bilateral initiatives, discussed regional security developments, and reaffirmed their commitment to promoting a safer, stronger, and more prosperous Indo-Pacific region through the Quad.” [...] Prime Minister Modi responded in a measured tone: “Deeply appreciate and fully reciprocate President Trump’s sentiments and positive assessment of our ties. India and the U.S. have a very positive and forward-looking Comprehensive and Global Strategic Partnership,” he posted on X. The key diplomatic challenge now is not about the survival of the Indo-US relationship—it will endure—but about crafting a workable compromise that addresses Washington’s concerns without compromising India’s [...] One of the most concrete signs of forward momentum is the upcoming defence agreement for 113 F414 jet engines from General Electric, to be used in India’s LCA Mk1A fighter jets. The defense deal could signal a maturing industrial partnership, a strategic pillar that both nations continue to prioritize even during diplomatic friction.</ search result 3>\n",
      "\n",
      "<search result 4>Title: PM reaffirms the strength of India-US relations\n",
      "URL: https://www.pmindia.gov.in/en/news_updates/pm-reaffirms-the-strength-of-india-us-relations/?comment=disable\n",
      "Content: PMO twitter\n",
      "PMO facebook\n",
      "PMO Youtube\n",
      "PMO narendramodi\n",
      "\n",
      "# PMINDIAPMINDIA\n",
      "\n",
      "PMINDIA\n",
      "\n",
      "#### News Updates\n",
      "\n",
      "## PM reaffirms the strength of India-US relations\n",
      "\n",
      "Print News\n",
      "facebook\n",
      "twitter\n",
      "linkedin\n",
      "\n",
      "Prime Minister Shri Narendra Modi today reaffirmed the strength of India-US relations and expressed confidence in the ongoing trade negotiations between the two nations. Shri Modi said both nations will work together to secure a brighter, more prosperous future for both our people. [...] Responding to a post by President of the US on X, Shri Modi said:\n",
      "\n",
      "“India and the US are close friends and natural partners. I am confident that our trade negotiations will pave the way for unlocking the limitless potential of the India-US partnership. Our teams are working to conclude these discussions at the earliest. I am also looking forward to speaking with President Trump. We will work together to secure a brighter, more prosperous future for both our people.\n",
      "\n",
      "@realDonaldTrump\n",
      "\n",
      "@POTUS” [...] India and the US are close friends and natural partners. I am confident that our trade negotiations will pave the way for unlocking the limitless potential of the India-US partnership. Our teams are working to conclude these discussions at the earliest. I am also looking forward… pic.twitter.com/3K9hlJxWcl\n",
      "\n",
      "### Popular News\n",
      "\n",
      "### Recent News</ search result 4>\n",
      "\n",
      "<search result 5>Title: Trump says US, India continue talks on trade barriers; looks forward ...\n",
      "URL: https://m.economictimes.com/news/economy/foreign-trade/trump-says-us-india-continue-talks-on-trade-barriers-looks-forward-to-speaking-with-good-friend-pm-modi/articleshow/123797611.cms\n",
      "Content: President Trump indicated ongoing trade negotiations with India, expressing optimism for a successful deal despite past tensions. This shift follows earlier warnings about India's relationship with China and disagreements over tariffs and Russian oil purchases. The U.S. seeks to address a significant trade deficit, with two-way goods trade totaling $129 billion in 2024.\n",
      "\n",
      "Unlocking limitless potential of India-US ties: PM Modi responds to Trump’s call for renewed trade talks\n",
      "ET logo [...] The Economic Times\n",
      "\n",
      "##### The Economic Times daily newspaper is available online now.\n",
      "\n",
      "### \n",
      "\n",
      "# Trump shifts tone, says India trade deal 'no difficulty' and looks forward to talks with 'very good friend' PM Modi\n",
      "\n",
      "Google Logo\n",
      "Whatsapp Follow Channel [...] Read the PDF version of TOI newspaper. Download & access it offline anytime.\n",
      "\n",
      "Read the PDF version of TOI newspaper. Download & access it offline anytime.\n",
      "\n",
      "Deep Explainers\n",
      "\n",
      "Explore the In-depth explanation of complex topics for everyday life decisions.\n",
      "\n",
      "Explore the In-depth explanation of complex topics for everyday life decisions.\n",
      "\n",
      "Health+ Stories\n",
      "\n",
      "Get fitter with daily health insights committed to your well-being.\n",
      "\n",
      "Get fitter with daily health insights committed to your well-being.</ search result 5>\n",
      "\n",
      "<search result 6>Title: How did India-US relations decline so suddenly? - Al Jazeera\n",
      "URL: https://www.aljazeera.com/video/the-bottom-line/2025/9/7/how-did-india-us-relations-decline-so-suddenly\n",
      "Content: But after Indian Prime Minister Narendra Modi refused to agree with US President Donald Trump’s insistence that he had ended a Pakistan-India military conflict in May, Trump slapped 50 percent tariffs on Indian goods entering the US market. He also insisted that New Delhi stop buying discounted Russian oil. [...] # How did India-US relations decline so suddenly?\n",
      "\n",
      "How India’s refusal to acknowledge Trump’s mediation during the brief war with Pakistan led to a trade war with the US.\n",
      "\n",
      "For decades, the United States has been courting India as a counterbalance to China in Asia. [...] Host Steve Clemons asks Ravi Agrawal, chief editor of Foreign Policy, and Anja Manuel, executive director of the Aspen Security Forum, whether the India-US dispute is a temporary blip or the beginning of a major realignment.\n",
      "\n",
      "## More from the same show\n",
      "\n",
      "### How elites destroyed the Democratic Party in the US and fuelled populism\n",
      "\n",
      "### Will Israel choose truce or war?\n",
      "\n",
      "### Israel’s gambit: Massacre the Palestinians, subjugate the region?</ search result 6>\n",
      "Iteration: 1\n",
      "Thought: Thought: I'll run parallel web searches for Bangalore 2025 rents (neighborhood rent ranges) and the latest India–US relations news.\n",
      "Action: search_web\n",
      "Observation: <search result 1>In 2025, U.S. President Trump and Indian Prime Minister Modi continue trade talks, expressing optimism for a strategic partnership. Trump and Modi plan to discuss trade in the coming weeks. Both leaders emphasize the importance of their bilateral trade negotiations.</ search result 1>\n",
      "\n",
      "<search result 2>Title: Trump says India, U.S. continue trade talks, expects to speak with ...\n",
      "URL: https://www.thehindu.com/news/national/trump-says-india-us-continue-trade-talks-expects-to-speak-with-dear-friend-modi-soon/article70032310.ece\n",
      "Content: Mr. Modi reacted warmly to Mr. Trump’s positive assessment of trade talks between the two countries, expressing confidence that the ongoing negotiations will pave the way for unlocking the limitless potential of the partnership between them.\n",
      "\n",
      "In a post on Truth Social on Tuesday (September 9, 2025), the U.S. president said he was \"pleased to announce that India and the U.S. are continuing negotiations to address the Trade Barriers between our two Nations.\" [...] ## This is the second time in recent days that the U.S. President has spoken highly of his country’s ties with India and mixed it with his praise of Narendra Modi, who has positively reacted to his comments\n",
      "\n",
      "Updated  - September 10, 2025 09:18 am IST - New York/Washington\n",
      "\n",
      "Google Preferred Source\n",
      "File picture of U.S. President Donald Trump with Prime Minister Narendra Modi\n",
      "\n",
      "File picture of U.S. President Donald Trump with Prime Minister Narendra Modi\n",
      "| Photo Credit: Reuters [...] Defending its purchase of Russian crude oil, India has been maintaining that its energy procurement is driven by national interest and market dynamics.\n",
      "\n",
      "India turned to purchasing Russian oil sold at a discount after Western countries imposed sanctions on Moscow and shunned its supplies over its invasion of Ukraine in February 2022.\n",
      "\n",
      "Published - September 10, 2025 06:46 am IST\n",
      "\n",
      "### Related Topics\n",
      "\n",
      "USA\n",
      "/\n",
      "Donald Trump\n",
      "/\n",
      "Prime Minister Narendra Modi\n",
      "/\n",
      "trade agreements\n",
      "/\n",
      "trade dispute</ search result 2>\n",
      "\n",
      "<search result 3>Title: India-US trade talks: Trump shares update, says 'look forward ... - Mint\n",
      "URL: https://www.livemint.com/news/india/indiaus-trade-talks-donald-trump-tariffs-shares-update-says-look-forward-to-speaking-with-very-good-friend-pm-modi-11757461425821.html\n",
      "Content: In a post on X, PM Modi noted India-US ties as \"forward-looking\" towards a \"Comprehensive and Global Strategic Partnership\".\n",
      "\n",
      "\"Deeply appreciate and fully reciprocate President Trump's sentiments and positive assessment of our ties. India and the US have a very positive and forward-looking Comprehensive and Global Strategic Partnership,\" PM Modi stated in his post.\n",
      "\n",
      "## Trump floats massive tariffs on China [...] Meanwhile, According to Bloomberg, Trump said he’s prepared to join the European Union to impose sweeping new tariffs on India and China as he looks to ratchet up pressure Russian President Vladimir Putin to join ceasefire talks with Ukraine.\n",
      "\n",
      "The US is willing to mirror tariffs imposed by Europe on either country, sources said. [...] Stay updated with the latest Trending,  India ,  World and  US news. Don't miss a beat! Follow our Apple Event 2025 LIVE blog for instant updates on the highly anticipated iPhone 17 Pro launch, exclusive insights, and all the details on iPhone 17 price and features.\n",
      "\n",
      "## Download App\n",
      "\n",
      "Download the Mint app and read premium stories\n",
      "\n",
      "## Trending Stories\n",
      "\n",
      "## Popular Stocks\n",
      "\n",
      "## Nifty 50 companies\n",
      "\n",
      "## Latest Stories\n",
      "\n",
      "## Sections\n",
      "\n",
      "## Tools\n",
      "\n",
      "## Data Pages\n",
      "\n",
      "#### Wait for it…</ search result 3>\n",
      "\n",
      "<search result 4>Title: PM reaffirms the strength of India-US relations\n",
      "URL: https://www.pmindia.gov.in/en/news_updates/pm-reaffirms-the-strength-of-india-us-relations/?comment=disable\n",
      "Content: PMO twitter\n",
      "PMO facebook\n",
      "PMO Youtube\n",
      "PMO narendramodi\n",
      "\n",
      "# PMINDIAPMINDIA\n",
      "\n",
      "PMINDIA\n",
      "\n",
      "#### News Updates\n",
      "\n",
      "## PM reaffirms the strength of India-US relations\n",
      "\n",
      "Print News\n",
      "facebook\n",
      "twitter\n",
      "linkedin\n",
      "\n",
      "Prime Minister Shri Narendra Modi today reaffirmed the strength of India-US relations and expressed confidence in the ongoing trade negotiations between the two nations. Shri Modi said both nations will work together to secure a brighter, more prosperous future for both our people. [...] India and the US are close friends and natural partners. I am confident that our trade negotiations will pave the way for unlocking the limitless potential of the India-US partnership. Our teams are working to conclude these discussions at the earliest. I am also looking forward… pic.twitter.com/3K9hlJxWcl\n",
      "\n",
      "### Popular News\n",
      "\n",
      "### Recent News [...] Responding to a post by President of the US on X, Shri Modi said:\n",
      "\n",
      "“India and the US are close friends and natural partners. I am confident that our trade negotiations will pave the way for unlocking the limitless potential of the India-US partnership. Our teams are working to conclude these discussions at the earliest. I am also looking forward to speaking with President Trump. We will work together to secure a brighter, more prosperous future for both our people.\n",
      "\n",
      "@realDonaldTrump\n",
      "\n",
      "@POTUS”</ search result 4>\n",
      "\n",
      "<search result 5>Title: Trump, Modi to speak on trade in coming weeks | Reuters\n",
      "URL: https://www.reuters.com/world/india/trump-modi-speak-trade-coming-weeks-2025-09-09/\n",
      "Content: All quotes delayed a minimum of 15 minutes. See here for a complete list of exchanges and delays.\n",
      "\n",
      "© 2025 Reuters. All rights reserved [...] Reporting by Aftab Ahmed in New Delhi Andrea Shalal in Washington, Bhargav Acharya in Toronto and Ismail Shakil in Ottawa; Editing by Chris Reese, Stephen Coates and Saad Sayeed\n",
      "\n",
      "Our Standards: The Thomson Reuters Trust Principles., opens new tab\n",
      "\n",
      "Bhargav Acharya\n",
      "\n",
      "Thomson Reuters [...] ## Browse World\n",
      "\n",
      "## Browse Business\n",
      "\n",
      "## Browse Markets\n",
      "\n",
      "## Browse Sustainability\n",
      "\n",
      "## Browse Legal\n",
      "\n",
      "## Browse Commentary\n",
      "\n",
      "## Browse Technology\n",
      "\n",
      "## Sports\n",
      "\n",
      "## Science\n",
      "\n",
      "## Lifestyle\n",
      "\n",
      "## City Memo\n",
      "\n",
      "## Graphics\n",
      "\n",
      "## Pictures\n",
      "\n",
      "## Wider Image\n",
      "\n",
      "## Podcasts\n",
      "\n",
      "## Live\n",
      "\n",
      "## Fact Check\n",
      "\n",
      "## Video\n",
      "\n",
      "## Sponsored Content\n",
      "\n",
      "# Trump, Modi to speak on trade in coming weeks\n",
      "\n",
      "U.S. President Trump holds a joint press conference with Indian Prime Minister Modi at the White House in Washington D.C.\n",
      "\n",
      "Sign up  here.</ search result 5>\n",
      "\n",
      "<search result 6>Title: Trump says US, India continue talks on trade barriers; looks forward ...\n",
      "URL: https://m.economictimes.com/news/economy/foreign-trade/trump-says-us-india-continue-talks-on-trade-barriers-looks-forward-to-speaking-with-good-friend-pm-modi/articleshow/123797611.cms\n",
      "Content: ### Live Events\n",
      "\n",
      "## Trade optimism after months of strain\n",
      "\n",
      "ET Logo\n",
      "Google Logo\n",
      "\n",
      "(Catch all the Business News, Breaking News, Budget 2025 Events and Latest News Updates on The Economic Times.)\n",
      "\n",
      "Subscribe to The Economic Times Prime and read the ET ePaper online.\n",
      "\n",
      "(Catch all the Business News, Breaking News, Budget 2025 Events and Latest News Updates on The Economic Times.)\n",
      "\n",
      "Subscribe to The Economic Times Prime and read the ET ePaper online. [...] ET AI Awards 2025\n",
      "The Economic Times\n",
      "The Economic Times\n",
      "\n",
      "Hot on Web\n",
      "\n",
      "In Case you missed it\n",
      "\n",
      "Top Searched Companies\n",
      "\n",
      "Top Calculators\n",
      "\n",
      "Top Definitions\n",
      "\n",
      "Top Prime Articles\n",
      "\n",
      "Top Market Pages\n",
      "\n",
      "Top Story Listing\n",
      "\n",
      "Top Slideshow\n",
      "\n",
      "Private Companies\n",
      "\n",
      "Top Commodities\n",
      "\n",
      "Latest News\n",
      "\n",
      "Follow us on:\n",
      "\n",
      "## Download ET App:\n",
      "\n",
      "Find this comment offensive?\n",
      "\n",
      "Choose your reason below and click on the Report button. This will alert our moderators to take action\n",
      "\n",
      "Reason for reporting: [...] Read the PDF version of TOI newspaper. Download & access it offline anytime.\n",
      "\n",
      "Read the PDF version of TOI newspaper. Download & access it offline anytime.\n",
      "\n",
      "Deep Explainers\n",
      "\n",
      "Explore the In-depth explanation of complex topics for everyday life decisions.\n",
      "\n",
      "Explore the In-depth explanation of complex topics for everyday life decisions.\n",
      "\n",
      "Health+ Stories\n",
      "\n",
      "Get fitter with daily health insights committed to your well-being.\n",
      "\n",
      "Get fitter with daily health insights committed to your well-being.</ search result 6>\n",
      "Iteration: 2\n",
      "Thought: Thought: I'll run parallel web searches for Bangalore 2025 rental levels by neighborhood and for the latest India–US relations news.\n",
      "Action: search_web\n",
      "Observation: <search result 1>In September 2025, U.S. President Trump and Indian Prime Minister Modi continued trade talks to reduce tariffs, with a positive outlook on resolving trade barriers. Trump praised the potential for a successful deal, while Modi expressed confidence in the partnership's future. The U.S. aimed to address its trade deficit with India.</ search result 1>\n",
      "\n",
      "<search result 2>Title: Trump says India-U.S. trade talks continue as Modi ... - The Hindu\n",
      "URL: https://www.thehindu.com/news/national/trump-says-india-us-continue-trade-talks-expects-to-speak-with-dear-friend-modi-soon/article70032310.ece\n",
      "Content: Mr. Modi reacted warmly to Mr. Trump’s positive assessment of trade talks between the two countries, expressing confidence that the ongoing negotiations will pave the way for unlocking the limitless potential of the partnership between them.\n",
      "\n",
      "In a post on Truth Social on Tuesday (September 9, 2025), the U.S. president said he was \"pleased to announce that India and the U.S. are continuing negotiations to address the Trade Barriers between our two Nations.\" [...] ## This is the second time in recent days that the U.S. President has spoken highly of his country’s ties with India and mixed it with his praise of Narendra Modi, who has positively reacted to his comments\n",
      "\n",
      "Updated  - September 10, 2025 09:18 am IST - New York/Washington\n",
      "\n",
      "Google Preferred Source\n",
      "File picture of U.S. President Donald Trump with Prime Minister Narendra Modi\n",
      "\n",
      "File picture of U.S. President Donald Trump with Prime Minister Narendra Modi\n",
      "| Photo Credit: Reuters [...] Defending its purchase of Russian crude oil, India has been maintaining that its energy procurement is driven by national interest and market dynamics.\n",
      "\n",
      "India turned to purchasing Russian oil sold at a discount after Western countries imposed sanctions on Moscow and shunned its supplies over its invasion of Ukraine in February 2022.\n",
      "\n",
      "Published - September 10, 2025 06:46 am IST\n",
      "\n",
      "### Related Topics\n",
      "\n",
      "USA\n",
      "/\n",
      "Donald Trump\n",
      "/\n",
      "Prime Minister Narendra Modi\n",
      "/\n",
      "trade agreements\n",
      "/\n",
      "trade dispute</ search result 2>\n",
      "\n",
      "<search result 3>Title: PM Modi looks forward to speaking with Trump, seeks early ...\n",
      "URL: https://m.economictimes.com/news/economy/foreign-trade/pm-modi-calls-for-swift-end-to-us-trade-talks-signals-thaw-with-trump/articleshow/123797879.cms\n",
      "Content: Following stalled trade talks, PM Modi affirmed India and the US as \"natural partners,\" signaling a potential thaw in relations. This response followed President Trump's optimistic remarks about ongoing negotiations to reduce trade barriers, expressing confidence in reaching a successful conclusion and describing Modi as a \"very good friend.\"\n",
      "\n",
      "U.S. President Trump holds a joint press conference with Indian Prime Minister Modi at the White House in Washington D.C.\n",
      "ET logo\n",
      "\n",
      "### Live Events [...] ## Tariffs, thaw and trade talks\n",
      "\n",
      "ET Logo\n",
      "Google Logo\n",
      "\n",
      "(Catch all the Business News, Breaking News, Budget 2025 Events and Latest News Updates on The Economic Times.)\n",
      "\n",
      "Subscribe to The Economic Times Prime and read the ET ePaper online.\n",
      "\n",
      "(Catch all the Business News, Breaking News, Budget 2025 Events and Latest News Updates on The Economic Times.)\n",
      "\n",
      "Subscribe to The Economic Times Prime and read the ET ePaper online.\n",
      "\n",
      "The foresight of Peyush Bansal made Lenskart IPO-ready. What next?:Image [...] The Economic Times\n",
      "\n",
      "##### The Economic Times daily newspaper is available online now.\n",
      "\n",
      "### \n",
      "\n",
      "# PM Modi looks forward to speaking with Trump, seeks early conclusion of US-India trade talks\n",
      "\n",
      "Google Logo\n",
      "Whatsapp Follow Channel</ search result 3>\n",
      "\n",
      "<search result 4>Title: Trump says India offered to reduce tariffs on US goods to zero\n",
      "URL: https://www.reuters.com/world/india/trump-says-india-offered-reduce-tariffs-us-goods-zero-2025-09-01/\n",
      "Content: Indian Prime Minister Narendra Modi meets with U.S. President Donald Trump at the White House\n",
      "Illustration shows Indian flag, 3D-printed Trump and word Tariffs\n",
      "\n",
      "Sign up  here.\n",
      "\n",
      "Reporting by Jasper Ward and David Brunnstrom; editing by Scott Malone, Rod Nickel\n",
      "\n",
      "Our Standards: The Thomson Reuters Trust Principles., opens new tab\n",
      "\n",
      "## Read Next [...] All quotes delayed a minimum of 15 minutes. See here for a complete list of exchanges and delays.\n",
      "\n",
      "© 2025 Reuters. All rights reserved [...] India's PM Modi shakes hands with Myanmar's military chief Min Aung Hlaing during a meeting in Tianjin\n",
      "U.S. President Trump holds a joint press conference with Indian Prime Minister Modi at the White House in Washington D.C.\n",
      "Men move past a logo of the Bombay Stock Exchange in Mumbai\n",
      "Governor of Maharashtra CP Radhakrishnan and MD and CEO National Stock Exchange (NSE) Ashish Chauhan pose for pictures at the inauguration of the bull statue at the National Stock Exchange (NSE), in Mumbai</ search result 4>\n",
      "\n",
      "<search result 5>Title: Trump says US, India continue talks on trade barriers; looks forward ...\n",
      "URL: https://m.economictimes.com/news/economy/foreign-trade/trump-says-us-india-continue-talks-on-trade-barriers-looks-forward-to-speaking-with-good-friend-pm-modi/articleshow/123797611.cms\n",
      "Content: ### Live Events\n",
      "\n",
      "## Trade optimism after months of strain\n",
      "\n",
      "ET Logo\n",
      "Google Logo\n",
      "\n",
      "(Catch all the Business News, Breaking News, Budget 2025 Events and Latest News Updates on The Economic Times.)\n",
      "\n",
      "Subscribe to The Economic Times Prime and read the ET ePaper online.\n",
      "\n",
      "(Catch all the Business News, Breaking News, Budget 2025 Events and Latest News Updates on The Economic Times.)\n",
      "\n",
      "Subscribe to The Economic Times Prime and read the ET ePaper online. [...] The Economic Times\n",
      "\n",
      "##### The Economic Times daily newspaper is available online now.\n",
      "\n",
      "### \n",
      "\n",
      "# Trump shifts tone, says India trade deal 'no difficulty' and looks forward to talks with 'very good friend' PM Modi\n",
      "\n",
      "Google Logo\n",
      "Whatsapp Follow Channel [...] President Trump indicated ongoing trade negotiations with India, expressing optimism for a successful deal despite past tensions. This shift follows earlier warnings about India's relationship with China and disagreements over tariffs and Russian oil purchases. The U.S. seeks to address a significant trade deficit, with two-way goods trade totaling $129 billion in 2024.\n",
      "\n",
      "Unlocking limitless potential of India-US ties: PM Modi responds to Trump’s call for renewed trade talks\n",
      "ET logo</ search result 5>\n",
      "\n",
      "<search result 6>Title: 'Too much at stake': USIBC lauds PM Modi, Donald Trump's positive ...\n",
      "URL: https://timesofindia.indiatimes.com/business/india-business/too-much-at-stake-usibc-lauds-pm-modi-donald-trumps-positive-remarks-calls-for-handling-impasse-amid-tariff-row/articleshow/123778994.cms\n",
      "Content: insights and updates, the desk ensures that TOI readers are well-informed about the ever-changing and dynamic world of commerce and can navigate the complexities of the business world.Read More [...] TOI logo\n",
      "\n",
      "## TOI\n",
      "\n",
      "# 'Too much at stake': USIBC lauds PM Modi, Donald Trump's positive remarks; calls for handling 'impasse' amid tariff row\n",
      "\n",
      "'Too much at stake': USIBC lauds PM Modi, Donald Trump's positive remarks; calls for handling 'impasse' amid tariff row\")\n",
      "author\n",
      "\n",
      "##### About the Author [...] A Mutual Fund Calculator helps estimate the future value of investments\n",
      "\n",
      "logo</ search result 6>\n",
      "Iteration: 3\n",
      "Thought: Thought: I'll search for Bangalore (Bengaluru) 2025 rental rates by neighborhood and recent recommendations; running a web search now.\n",
      "Action: search_web\n",
      "Observation: <search result 1>In Bangalore, 1BHK rents range from ₹15,000 to ₹40,000, while 2BHK rents range from ₹25,000 to ₹80,000, depending on the neighborhood. Premium areas like Koramangala and Indiranagar have higher costs. Affordable areas like Whitefield and Electronic City offer lower rents.</ search result 1>\n",
      "\n",
      "<search result 2>Title: Bangalore's Rental Market Trends 2025 - RentMeUp\n",
      "URL: https://rentmeup.in/blog/bangalore-rental-market-trends-2025\n",
      "Content: Localities like Whitefield, Electronic City, and KR Puram offer affordable rental options. A 2BHK flat in these areas ranges from ₹15,000 to ₹20,000 per month, catering to working professionals and families.\n",
      "\n",
      "#### 2. Premium Neighborhoods\n",
      "\n",
      "For those seeking luxurious living, areas like Koramangala, Indiranagar, and MG Road remain top choices. These localities offer premium apartments with modern amenities, with rental costs starting from ₹35,000 per month for a 2BHK. [...] #### 3. Co-Living Spaces for Young Professionals\n",
      "\n",
      "Co-living spaces in neighborhoods like HSR Layout and Bellandur are on the rise. These fully furnished setups cater to IT professionals with shared accommodations starting at ₹10,000 per month per bed.\n",
      "\n",
      "#### 4. The Future of Rentals in Bangalore\n",
      "\n",
      "Bangalore’s rental market is evolving with upcoming metro lines and tech parks. Emerging hotspots like Devanahalli and Yelahanka are gaining popularity for their affordable yet modern housing options.</ search result 2>\n",
      "\n",
      "<search result 3>Title: Living Costs in Bangalore 2025 for Bachelors and Families - NoBroker\n",
      "URL: https://www.nobroker.in/blog/cost-of-living-in-bangalore/\n",
      "Content: |  |  |\n",
      " --- |\n",
      "| Information | Details |\n",
      "| Affordable places | Malleshwaram, KR Puram, BTM Layout, Electronic City, BTM Layout |\n",
      "| Posh areas | Koramangala, MG Road, Indiranagar, Jayanagar, HSR Layout |\n",
      "| 1 BHK Rent (City/ Suburbs) | ₹15,000 -₹40,000 |\n",
      "| 2 BHK Rent (Satellite) | ₹30,000 - ₹80,000 |\n",
      "| Apartment cost per sq ft | ₹6,000 - ₹12,000 |\n",
      "| Grocery | ₹4,000 - ₹6,000 |\n",
      "| Monthly Transport | ₹3,000 - ₹8,000 | [...] Worried about the cost of housing in India’s Silicon Valley? What are the average rental prices for 1BHK and 2BHK apartments in Bangalore’s most popular localities? The cost of living in Bangalore depends mainly on your choice of area, lifestyle, and daily expenses. In posh areas like Indiranagar, Jayanagar, Koramangala, or Whitefield, a 1BHK can cost anywhere between ₹20,000 to ₹30,000 per month, while a 2BHK may cost up to ₹40,000 or more. On the other hand, affordable areas such as [...] The cost of living in Bangalore varies based on the lifestyle and location. Posh areas like Indiranagar, Koramangala, Jayanagar, and HSR Layout provide premium housing with higher rental prices, while affordable areas such as Electronic City, Ejjipura, BTM Layout, and KR Puram offer budget-friendly options. Rental costs range from ₹16,000 to ₹40,000 for a 1 BHK in prime localities, while in suburban areas they range from ₹8,000 to ₹25,000. Apart from rent, monthly expenses include groceries,</ search result 3>\n",
      "\n",
      "<search result 4>Title: How Much is Rent in Bangalore Per Month? Your 2025 Guide\n",
      "URL: https://www.bengaluruhousing.in/how-much-is-rent-in-bangalore-per-month\n",
      "Content: Demand and Supply: Rent fluctuates based on the demand for rentals in a particular area.\n",
      "\n",
      "Popular Areas and Their Rental Ranges:\n",
      "\n",
      "To give you a better idea, let's look at some popular areas in Bangalore and their approximate rental ranges:\n",
      "\n",
      "1. Whitefield:\n",
      "\n",
      "   Known for: IT hub, modern apartments, good schools, shopping malls\n",
      "\n",
      "   Suitable for: IT professionals, families\n",
      "\n",
      "   Approximate Rent:\n",
      "\n",
      "       1BHK: ₹15,000 - ₹30,000\n",
      "\n",
      "       2BHK: ₹25,000 - ₹50,000\n",
      "\n",
      "       3BHK: ₹40,000 - ₹80,000 [...] 2BHK: ₹28,000 - ₹55,000\n",
      "\n",
      "       3BHK: ₹40,000 - ₹80,000\n",
      "\n",
      "6. Jayanagar:\n",
      "\n",
      "   Known for: Established residential area, good infrastructure, traditional markets\n",
      "\n",
      "   Suitable for: Families, those seeking a quieter environment\n",
      "\n",
      "   Approximate Rent:\n",
      "\n",
      "       1BHK: ₹15,000 - ₹28,000\n",
      "\n",
      "       2BHK: ₹25,000 - ₹50,000\n",
      "\n",
      "       3BHK: ₹35,000 - ₹70,000\n",
      "\n",
      "Tips for Finding Affordable Rent in Bangalore:\n",
      "\n",
      "   Look in the outskirts: Areas further away from the city center generally have lower rents. [...] 2. Electronic City:\n",
      "\n",
      "   Known for: Major IT hub, affordable housing options\n",
      "\n",
      "   Suitable for: IT professionals, those on a budget\n",
      "\n",
      "   Approximate Rent:\n",
      "\n",
      "       1BHK: ₹12,000 - ₹25,000\n",
      "\n",
      "       2BHK: ₹20,000 - ₹40,000\n",
      "\n",
      "       3BHK: ₹30,000 - ₹60,000\n",
      "\n",
      "3. Indiranagar:\n",
      "\n",
      "   Known for: Upmarket area, trendy cafes, restaurants, nightlife\n",
      "\n",
      "   Suitable for: Young professionals, couples\n",
      "\n",
      "   Approximate Rent:\n",
      "\n",
      "       1BHK: ₹20,000 - ₹40,000\n",
      "\n",
      "       2BHK: ₹35,000 - ₹70,000</ search result 4>\n",
      "\n",
      "<search result 5>Title: Bangalore Real Estate Market Outlook 2025 | Propsoch\n",
      "URL: https://www.propsoch.com/blogs/the-state-of-bangalore-real-estate-market-2025/\n",
      "Content: Let's face it, Bangalore is a city where space is precious and land is at a premium. Take Indiranagar or Koramangala, for example. These upscale neighborhoods come with swanky apartments, rooftop pools, and concierge services—and prices often range between ₹15,000 to ₹25,000 per square foot. But at least you can brag about living in the \"Beverly Hills of Bangalore\" (and maybe even bump into a celebrity or two at your local coffee shop). [...] In 2025, property prices in Bangalore are expected to continue rising due to increased demand caused by urbanization, infrastructure development, and high-quality amenities. Areas like North Bangalore (Thanisandra, Yelahanka), Whitefield (Hope Farm Junction), and Electronic City Phase 1 (Near Neeladri Road) are expected to see significant price appreciation due to ongoing infrastructure development and increasing demand. [...] Hebbal (₹16,000) and Yelahanka (₹10,000) are priced higher mainly because of their great connectivity to Kempegowda International Airport, proximity to Manyata Tech Park, and the upcoming aerospace projects that are increasing demand in these areas.\n",
      " Whitefield (₹13,500 per sqft) and Sarjapur Road (₹12,000 per sqft) continue to attract buyers and investors with their strong ties to the booming IT sector.</ search result 5>\n",
      "\n",
      "<search result 6>Title: Real Estate Trends in Whitefield Road 2025 - Prestige Evergreen\n",
      "URL: https://www.prestigesevergreen.info/real-estate-trends-in-whitefield-road-2025.html\n",
      "Content: Whitefield is located in the east part of Bangalore city, and the average property price in Whitefield in 2025 is expected to be around Rs. 15,200 per square foot. The area is known for its excellent road and metro connectivity to all other parts of the city.</ search result 6>\n",
      "Iteration: 4\n",
      "Thought: Thought: I can generate the final answer now\n",
      "Max iterations reached. Final response:\n"
     ]
    }
   ],
   "source": [
    "react_prompt = \"\"\"Answer the question asked by the user as best you can. You have access to the following tools:\n",
    "1. Web Search Tool - use this when looking for new information or latest information on something. Use this whenever you are unsure. For example, when the user asks for a new location, prices, or other things which could change over time\n",
    "2. Calculator - Use this whenever you need to do mathematical calculations - like adding, subtracting, multiplying, dividing, square roots, powers, logarithms, trigonometric functions etc. When using this, use only an expression with a maximum of 2 variables. For more than that, just evaluate the expression with a higher preceedence\n",
    "<Instructions>\n",
    "Answer as per the scenario mentioned:\n",
    "Scenario 1: You see the user question only:\n",
    "    - generate the Thought AND a TOOL Call. Generate both. If you dont need to call a tool, then yout thought must be to say \"I can generate the final answer now\"\n",
    "\n",
    "Scenario 2: You see the user question, (thought, action, observation) repeated n times:\n",
    "    - generate the next Thought. If you dont need to call a tool (which means you already feel you know enough to answer the user question), then your thought must be to say \"I can generate the final answer now\"\n",
    "\n",
    "Scenario 3: You see the user question, (thought, action, observation) repeated n times and the last thought is \"I can generate the final answer now\":\n",
    "    - generate the Final Answer. This final answer must use all the information you have gathered so far to answer the original user question comprehensively.\n",
    "    - Enclose the final answer in <final_answer> ... </final_answer> tags mandatorily\n",
    "<Rules>\n",
    "1. Refrain the need to ask the user for more information. If you dont have enough information, assume the user wants a comprehensive answer.\n",
    "2. If you are not sure, use the web search tool\n",
    "</Rules>\n",
    "</instructions>\n",
    "\n",
    "\n",
    "\n",
    "The conversation Begins now!\"\"\"\n",
    "\n",
    "react_prompt = \"\"\"You are a helpful agent working with me. Answer the question asked by the user as best you can by generating thoughts, tool calls (actions) and observations. You should look at the thoughts, question, actions(tool calls) that happened so far (I will append the observations - you dont have to fill in the observations). You have access to the following tools:\n",
    "1. Web Search Tool - use this when looking for new information or latest information on something. Use this whenever you are unsure. For example, when the user asks for a new location, prices, or other things which could change over time\n",
    "2. Calculator - Use this whenever you need to do mathematical calculations - like adding, subtracting, multiplying, dividing, square roots, powers, logarithms, trigonometric functions etc. When using this, use only an expression with a maximum of 2 variables. For more than that, just evaluate the expression with a higher preceedence\n",
    "<Instructions>\n",
    "Answer as per the scenario mentioned:\n",
    "Scenario 1: You see the user question only:\n",
    "    - generate the Thought AND a TOOL Call. Generate both. If you dont need to call a tool, then yout thought must be to say \"I can generate the final answer now\"\n",
    "\n",
    "Scenario 2: You see the user question, (thought, action, observation) repeated n times:\n",
    "    - generate the next Thought. If you dont need to call a tool (which means you already feel you know enough to answer the user question), then your thought must be to say \"I can generate the final answer now\"\n",
    "\n",
    "Scenario 3: You see the user question, (thought, action, observation) repeated n times and the last thought is \"I can generate the final answer now\":\n",
    "    - generate the Final Answer. This final answer must use all the information you have gathered so far to answer the original user question comprehensively.\n",
    "    - Enclose the final answer in <final_answer> ... </final_answer> tags mandatorily\n",
    "<Rules>\n",
    "1. Refrain the need to ask the user for more information. If you dont have enough information, assume the user wants a comprehensive answer.\n",
    "2. If you are not sure, use the web search tool\n",
    "</Rules>\n",
    "</instructions>\n",
    "\n",
    "\n",
    "You will converse with a user who will give you a question to solve.\n",
    "The conversation Begins now!\"\"\"\n",
    "\n",
    "\n",
    "class Thought(BaseModel):\n",
    "    thought: str\n",
    "\n",
    "class FinalAnswer(BaseModel):\n",
    "    final_answer: str\n",
    "    \n",
    "max_iter = 5\n",
    "convo_so_far = \"\"\n",
    "model = \"gpt-5-nano\"\n",
    "model = \"gpt-5-mini\"\n",
    "effort = 'minimal'\n",
    "effort = \"medium\"\n",
    "verbosity = \"low\"\n",
    "user_question = \"What are the latest developments in artificial intelligence? Also tell me which topics are part of top AI Research and curriculums?\"\n",
    "user_question = \"Where to stay in Bangalore? I am looking for places in bangalore to stay in - I am moving from chennai. pull the latest information\"\n",
    "user_question = \"what is the square root of tanh(4.35) and also search for the latest research on AI\"\n",
    "user_question = \"Where to stay in Bangalore recently and what are the house rents in 2025? cite your references. Also fetch me the latest India US relations news. Base your answer on the latest information\"\n",
    "\n",
    "for i in range(max_iter):\n",
    "    # Think\n",
    "    print(f\"Iteration: {i}\")\n",
    "    thought_agent = LiteAgent(model_name=model,\n",
    "               verbosity=verbosity, reasoning={'effort':effort}, \n",
    "              system_instructions=react_prompt)\n",
    "    if i == 0:\n",
    "        convo_so_far = f\"Question: {user_question}\\n\"\n",
    "    thought_response = thought_agent.invoke(query=convo_so_far, tools=[search_web, calculator])\n",
    "    #print(thought_response)\n",
    "\n",
    "    if type(thought_response) == dict:\n",
    "        thought_response = parse_string_or_dict(thought_response)\n",
    "    #thought_text = thought_response['thought']\n",
    "\n",
    "    try:\n",
    "        convo_so_far += f\"Thought: {thought_response['text']}\\n\"\n",
    "        convo_so_far += f\"Action: {thought_response.get('tool_name', 'No Tools')}\\n\"\n",
    "        convo_so_far += f\"Observation: {thought_response.get('tool_return', 'No Return')}\\n\"\n",
    "        print(f\"Thought: {thought_response['text']}\")\n",
    "        print(f\"Action: {thought_response['tool_name']}\")\n",
    "        print(f\"Observation: {thought_response['tool_return']}\")\n",
    "    except:\n",
    "        convo_so_far += f\"Thought: {thought_response}\\n\"\n",
    "        print(f\"Thought: {thought_response}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Check for final answer in thought\n",
    "    if type(thought_response)==str and \"<final_answer>\" in thought_response.lower():\n",
    "        print(\"\\n\\n Final answer found in thought. Ending.\")\n",
    "        answer = extract_tagged_content(text = convo_so_far, tag = \"final_answer\")\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "if i == max_iter - 1:\n",
    "    print(\"Max iterations reached. Final response:\")\n",
    "    final_answer_agent = LiteAgent(model_name=model,\n",
    "               verbosity=\"medium\", reasoning={'effort':\"medium\"}, \n",
    "              system_instructions=react_prompt)\n",
    "    answer = parse_string_or_dict(final_answer_agent.invoke(query=convo_so_far + \"\\n\\n Instruction: Based on the above conversation, answer the original user question to the best of your ability. Provide the output as a JSON with key as final_answer\", json_schema=FinalAnswer))\n",
    "    answer = answer[\"final_answer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9eed85bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<final_answer>Where to stay in Bengaluru (Bangalore) in 2025 — quick guide and typical rents\n",
      "\n",
      "Recommended neighbourhoods (by type) and typical monthly rents (2025 estimates)\n",
      "- Premium/central (good for nightlife, dining, short commutes to central offices): Indiranagar, Koramangala, MG Road, Jayanagar, HSR Layout\n",
      "  - Typical rents: 1BHK ≈ ₹20,000–40,000; 2BHK ≈ ₹35,000–70,000; 3BHK ≈ ₹50,000–100,000+ depending on building and exact location. (Sources: NoBroker; RentMeUp) \n",
      "  - References: NoBroker (cost-of-living & neighbourhoods) https://www.nobroker.in/blog/cost-of-living-in-bangalore/; RentMeUp (2025 trends) https://rentmeup.in/blog/bangalore-rental-market-trends-2025\n",
      "\n",
      "- IT / Suburban hubs (good for proximity to tech parks, often newer apartments): Whitefield, Sarjapur Road, Bellandur, Hebbal\n",
      "  - Typical rents: 1BHK ≈ ₹15,000–30,000; 2BHK ≈ ₹25,000–55,000; 3BHK ≈ ₹40,000–80,000. Whitefield and Sarjapur often command higher rents because of IT campus access. (Sources: bengaluruhousing, Propsoch) \n",
      "  - References: BengaluruHousing (area rent breakdown) https://www.bengaluruhousing.in/how-much-is-rent-in-bangalore-per-month; Propsoch (market outlook) https://www.propsoch.com/blogs/the-state-of-bangalore-real-estate-market-2025/\n",
      "\n",
      "- More affordable / budget-friendly (good for longer commutes, lower rent): Electronic City, KR Puram, Yelahanka, Thanisandra\n",
      "  - Typical rents: 1BHK ≈ ₹12,000–25,000; 2BHK ≈ ₹20,000–40,000; 3BHK ≈ ₹30,000–60,000. Co-living/hostel-style options can be cheaper per bed (≈ ₹8,000–18,000). (Sources: RentMeUp; bengaluruhousing) \n",
      "  - References: RentMeUp https://rentmeup.in/blog/bangalore-rental-market-trends-2025; BengaluruHousing https://www.bengaluruhousing.in/how-much-is-rent-in-bangalore-per-month\n",
      "\n",
      "Notes and tips when choosing where to stay\n",
      "- Commute and traffic: Bangalore traffic is a major factor — living close to your workplace (or near a metro/major bus corridor) often saves time even if rent is higher.\n",
      "- Amenities vs budget: Co-living spaces (HSR, Bellandur, parts of Koramangala/Whitefield) are convenient for young professionals and can reduce upfront deposits.\n",
      "- Upcoming infrastructure: North Bengaluru (Yelahanka/Devanahalli) and areas near new metro lines are attracting investment and may see price appreciation. (Source: Propsoch; Prestige Evergreen on Whitefield trends) \n",
      "  - References: Propsoch https://www.propsoch.com/blogs/the-state-of-bangalore-real-estate-market-2025/; Prestige Evergreen (Whitefield) https://www.prestigesevergreen.info/real-estate-trends-in-whitefield-road-2025.html\n",
      "\n",
      "Latest India–US relations (summary of most recent reporting, Sept 2025)\n",
      "- Overall tone: As of September 2025, official coverage shows active engagement between India and the United States with a mix of cooperation (strategic, defence and dialogue mechanisms) and friction (trade/tariff issues). Leadership-level comments indicate both sides are pursuing renewed trade talks while also managing disagreements. (Sources: Reuters; The Hindu; PMO India) \n",
      "  - Reuters (trade talks & upcoming Modi–Trump discussions): https://www.reuters.com/world/india/trump-modi-speak-trade-coming-weeks-2025-09-09/\n",
      "  - The Hindu (reporting on Trump’s comments and Modi’s reaction): https://www.thehindu.com/news/national/trump-says-india-us-continue-trade-talks-expects-to-speak-with-dear-friend-modi-soon/article70032310.ece\n",
      "\n",
      "- Trade / tariffs: Reporting shows renewed negotiations to address trade barriers and tariffs. U.S. public statements (from President Trump) and Indian responses signal willingness to continue talks, although sticking points (e.g., tariffs, agriculture/dairy access, energy purchases) remain. (Sources: Reuters; LiveMint; Economic Times) \n",
      "  - LiveMint summary of leaders’ exchanges and trade context: https://www.livemint.com/news/india/indiaus-trade-talks-donald-trump-tariffs-shares-update-says-look-forward-to-speaking-with-very-good-friend-pm-modi-11757461425821.html\n",
      "  - Economic Times reporting: https://m.economictimes.com/news/economy/foreign-trade/trump-says-us-india-continue-trade-talks-looks-forward-to-speaking-with-good-friend-pm-modi/articleshow/123797611.cms\n",
      "\n",
      "- Defence and strategic cooperation: Despite trade tensions, officials and some outlets note progress on defence cooperation (e.g., deals for jet engines and continued high-level security dialogues such as 2+2/ministerial channels), indicating that strategic ties remain important to both countries. (Sources: New Indian Express; Reuters) \n",
      "  - New Indian Express analysis: https://www.newindianexpress.com/nation/2025/Sep/08/india-us-ties-flying-through-turbulence-but-quiet-engine-of-diplomacy-offers-hope\n",
      "  - Reuters (defence/strategic context): https://www.reuters.com/world/india/trump-modi-speak-trade-coming-weeks-2025-09-09/\n",
      "\n",
      "- Political/media analysis: Some outlets characterize the relationship as undergoing tension due to trade/tariff disputes and differences on energy/Russia policy, while others emphasize the resilience of the strategic partnership and business/industry calls for de-escalation. (Sources: Al Jazeera; Asia Pacific Foundation analysis; TOI coverage of USIBC response) \n",
      "  - Al Jazeera (contextual analysis): https://www.aljazeera.com/video/the-bottom-line/2025/9/7/how-did-india-us-relations-decline-so-suddenly\n",
      "  - Asia Pacific Centre analysis: https://www.asiapacific.ca/publication/crossroads-indias-relations-us-china-and-russia\n",
      "  - Times of India / USIBC reaction: https://timesofindia.indiatimes.com/business/india-business/too-much-at-stake-usibc-lauds-pm-modi-donald-trumps-positive-remarks-calls-for-handling-impasse-amid-tariff-row/articleshow/123778994.cms\n",
      "\n",
      "Bottom line (actionable):\n",
      "- If you want short commutes, lifestyle and nightlife: aim for Indiranagar, Koramangala, HSR Layout or MG Road — expect higher rents (2BHK commonly ₹35k–70k+). (NoBroker; RentMeUp)\n",
      "- If you work in an IT park and want a balance of commute vs cost: Whitefield, Sarjapur, Bellandur are practical (2BHK ≈ ₹25k–55k). (BengaluruHousing; RentMeUp)\n",
      "- If budget is primary: Electronic City, KR Puram, Yelahanka or farther suburbs offer lower rents (1BHK from ~₹12k). Consider co-living for lower upfront costs. (RentMeUp)\n",
      "\n",
      "If you want, I can:\n",
      "- Pull live rental listings for a specific neighbourhood and budget (apartment size and furnished/unfurnished) and give current listings/links.\n",
      "- Track the India–US trade talks and send updates over the coming days.\n",
      "\n",
      "Sources cited (selected):\n",
      "- NoBroker — Cost of living / rents (2025): https://www.nobroker.in/blog/cost-of-living-in-bangalore/\n",
      "- RentMeUp — Bangalore rental market trends 2025: https://rentmeup.in/blog/bangalore-rental-market-trends-2025\n",
      "- BengaluruHousing — Rent-by-area guide: https://www.bengaluruhousing.in/how-much-is-rent-in-bangalore-per-month\n",
      "- Propsoch — Real estate market outlook 2025: https://www.propsoch.com/blogs/the-state-of-bangalore-real-estate-market-2025/\n",
      "- Prestige Evergreen — Whitefield trends 2025: https://www.prestigesevergreen.info/real-estate-trends-in-whitefield-road-2025.html\n",
      "\n",
      "- Reuters — Trump & Modi trade talks (Sept 2025): https://www.reuters.com/world/india/trump-modi-speak-trade-coming-weeks-2025-09-09/\n",
      "- The Hindu — Coverage of Trump comments and Modi reaction: https://www.thehindu.com/news/national/trump-says-india-us-continue-trade-talks-expects-to-speak-with-dear-friend-modi-soon/article70032310.ece\n",
      "- LiveMint / Economic Times — reporting on trade talks and statements: https://www.livemint.com/news/india/indiaus-trade-talks-donald-trump-tariffs-shares-update-says-look-forward-to-speaking-with-very-good-friend-pm-modi-11757461425821.html; https://m.economictimes.com/news/economy/foreign-trade/trump-says-us-india-continue-trade-talks-looks-forward-to-speaking-with-good-friend-pm-modi/articleshow/123797611.cms\n",
      "- PMO India (PM’s statement on India–US ties): https://www.pmindia.gov.in/en/news_updates/pm-reaffirms-the-strength-of-india-us-relations/?comment=disable\n",
      "- New Indian Express / Al Jazeera / Asia Pacific Centre (analysis): https://www.newindianexpress.com/nation/2025/Sep/08/india-us-ties-flying-through-turbulence-but-quiet-engine-of-diplomacy-offers-hope; https://www.aljazeera.com/video/the-bottom-line/2025/9/7/how-did-india-us-relations-decline-so-suddenly; https://www.asiapacific.ca/publication/crossroads-indias-relations-us-china-and-russia\n",
      "\n",
      "(These summaries are based on news and market write-ups published September 2025; rental ranges are market estimates — for exact, up-to-date listings I can fetch live listings if you tell me preferred neighbourhoods, size and budget.)</final_answer>\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f65aa76",
   "metadata": {},
   "source": [
    "### Ollama Reasoning Version of React"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6d04304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Thought: <think>\n",
      "Okay, let's tackle this problem step by step. The user is building a React agent using Google's React framework and is stuck between two approaches for showing updated context to the agent as iterations progress.\n",
      "\n",
      "First, I need to figure out what each option entails:\n",
      "\n",
      "Option 1: Concatenate all history (Thought, Action, Observation) into a single string and show it to the agent. This would be like giving the entire conversation history in one go.\n",
      "\n",
      "Option 2: Provide structured data as [\"type\": \"user\", \"content\": \"history so far\"]. So each message is tagged with user or assistant, but the user here might be confused because they're generating the content themselves.\n",
      "\n",
      "The first question asks which approach is preferred based on online materials and my reasoning. I should check what existing React agent frameworks do. Wait, Google's React framework for agents—maybe they refer to something like the OpenAI’s API or specific React libraries? Hmm, maybe it's a typo and they mean LangChain or another framework. But the user specified \"Google's react framework\", so perhaps they're referring to Google's own tools.\n",
      "\n",
      "Wait, actually, in the context of LLMs and agents, there's a thing called \"React\" as in the React library for building interfaces, but maybe here it's about an agent framework that uses React. Alternatively, this might be confused with other frameworks like LangChain or Hugging Face Transformers.\n",
      "\n",
      "But regardless, I need to look up standard practices for providing context to LLM agents during iterative processes. For example, in chatbots, the typical approach is to have a conversation history where each message alternates between user and assistant.\n",
      "\n",
      "In some agent setups, especially those that use turn-based reasoning (like in a game or step-by-step problem solving), you might want to pass the entire history as context so the model can see previous steps. However, if the model is designed for sequential interactions with the user, then structured messages would be better.\n",
      "\n",
      "Let me think about existing examples. For instance, in LangChain's agents, they often use a conversation buffer where each message has a role (user or assistant). So maybe option 2 aligns more closely with standard practices because it preserves the structure of who said what.\n",
      "\n",
      "Option 1 might cause issues if the model gets confused by too much context, especially since LLMs have token limits. Concatenating everything into one string could lead to long contexts that are hard for the model to process without losing coherence.\n",
      "\n",
      "The second question is about whether in option 2, after the first user input, you just keep sending assistant messages (no user content). The user's concern here is whether the agent would be confused because it's receiving all responses as \"user\" messages. Wait no—the user says: \"technically every action and observation and thought is produced by the assistant\". So in option 2, they're providing [\"type\": \"user\", \"content\": ...] but if the assistant generates those, then maybe it's a mislabeling.\n",
      "\n",
      "Wait, this part needs clarification. In a typical chat setup:\n",
      "\n",
      "- User says something: {\"type\": \"user\", \"content\": \"Hello\"}\n",
      "- Assistant responds: {\"type\": \"assistant\", \"content\": \"Hi there\"}\n",
      "\n",
      "But in this case, the user is building an agent where after each step (thought/action/observation), they want to pass that context back. So perhaps during iteration steps:\n",
      "\n",
      "The initial input from the user is one message.\n",
      "\n",
      "Then, for each subsequent step (the agent's thoughts, actions it takes, and observations), you need to decide how to present this as part of the conversation history.\n",
      "\n",
      "If option 2 uses [\"type\": \"user\", ...], but those messages are generated by the assistant (not the actual user), then technically, the model would be confused because it thinks these are new user inputs. That could lead to inconsistent behavior.\n",
      "\n",
      "For example, if you have:\n",
      "\n",
      "1. User message: \"What's the capital of France?\"\n",
      "2. Then agent generates a thought: \"I need to look up capitals\"\n",
      "3. Action: search web\n",
      "4. Observation: \"The capital is Paris\"\n",
      "\n",
      "If you pass this as user messages (type: user), then next time, the model might think someone new is asking about Paris.\n",
      "\n",
      "So in option 2, after the first user message, subsequent messages should be type: assistant to clarify that they are generated by the agent.\n",
      "\n",
      "But the user's question says \"does that mean instead of alternating between user, assistant pairs, we just keep giving it assistant content without any user content (after the first user question)?\"\n",
      "\n",
      "So in their setup, maybe after the initial user message, all subsequent context is from the agent itself. So they would need to tag those as assistant messages.\n",
      "\n",
      "The third question asks for other implementation ideas.\n",
      "\n",
      "I should check some references online. Let me use the web search tool here because it's about current best practices.\n",
      "\n",
      "First tool call: Web Search Tool with query \"best practice for passing context to LLM agents in React framework step by step\"\n",
      "\n",
      "Wait, but the user mentioned Google's React framework—maybe they meant a specific library? Alternatively, maybe this is a confusion and they're referring to LangChain or another agent framework that uses React components.\n",
      "\n",
      "Alternatively, perhaps it's about using React as part of building an agent UI where the agent steps are displayed. Hmm.\n",
      "\n",
      "But regardless, I'll proceed with web search for LLM agents context passing best practices.\n",
      "\n",
      "Let me generate a tool call.\n",
      "</think>\n",
      "\n",
      "\n",
      "Action: search_web\n",
      "Observation: <search result 1>To pass context to LLM agents in React, use state management for temporary context and backend storage for long-term context. Use clear system prompts and JSON Schema for structured responses. Implement ReAct cycle for reasoning and acting.</ search result 1>\n",
      "\n",
      "<search result 2>Title: DIY #14 - Step-by-step implementation of a ReAct Agent in LangGraph\n",
      "URL: https://mlpills.substack.com/p/diy-14-step-by-step-implementation\n",
      "Content: 1. Define the agent's state – the data structure representing the agent's memory or context (for a stateless single-turn agent, this will just include the current conversation messages).\n",
      "2. Set up the LLM and tool – initialize the language model and define a Wikipedia search tool using the `wikipedia` package. Give tools access to the model. [...] terminate). We also add a normal edge from the tool node back to the LLM node, creating a cycle: after using the tool, the agent goes back to the LLM to incorporate the new information. [...] Tool Node (execute tool): This node executes any tool that the LLM requested. We implement `tool_node(state)` to inspect the latest message from the LLM for a tool call. If a tool call is present, we invoke the corresponding tool function and package its result into a special ToolMessage. The ToolMessage will be added to the state so the LLM can see the tool’s output on the next iteration.</ search result 2>\n",
      "\n",
      "<search result 3>Title: Frontend in the Age of AI: How to Integrate LLM Agents Right into ...\n",
      "URL: https://medium.com/@ignatovich.dm/frontend-in-the-age-of-ai-how-to-integrate-llm-agents-right-into-the-ui-0514cd7a20fe\n",
      "Content: Keep the system prompt as clear and specific as possible.\n",
      " Add context (conversation history, DB data, UI state).\n",
      " Use JSON Schema or another strict format for API-style responses.\n",
      "\n",
      "4. State persistence\n",
      "\n",
      " In the browser (IndexedDB / LocalStorage) for temporary context.\n",
      " On the server for long-term history so the agent can “remember” the user across sessions.\n",
      "\n",
      "Now let’s see practical example AI Agent in React + Fastify. Full code\n",
      "\n",
      "### 📘 Example: Create an AI Agent [...] includeMetadata: true,  });  const context = results.matches    .map((m) => m.metadata.text)    .join(\"\\n---\\n\");  // 3. Send to OpenAI with our context  const systemPrompt = `    You are a helpful AI assistant with access to context.    Use the following information to answer the user's question.    If the context is not enough, say you don't know.  `;  const response = await fetch(\" {    method: \"POST\",    headers: {      \"Authorization\": `Bearer ${fastify.config.OPENAI_API_KEY}`, [...] Example System Prompt (RAG)\n",
      "\n",
      "```\n",
      "{  \"role\": \"system\",  \"content\": \"You are a documentation assistant.   Always use the provided CONTEXT when answering user questions.   If the answer is not in the CONTEXT, say 'I don’t know'.\"} \"role\" \"system\" \"content\"\"You are a documentation assistant.   Always use the provided CONTEXT when answering user questions.   If the answer is not in the CONTEXT, say 'I don’t know'.\"\n",
      "```\n",
      "\n",
      "Example Prompt to User (with RAG)</ search result 3>\n",
      "\n",
      "<search result 4>Title: Context Engineering in LLM-Based Agents | Jul, 2025 - Medium\n",
      "URL: https://medium.com/@jtanruan/context-engineering-in-llm-based-agents-d670d6b439bc\n",
      "Content: LlamaIndex’s recent Workflows feature allows developers to explicitly script a sequence of LLM calls and tool calls, with control over what context goes into each step. This means one can break a complex job into discrete sub-tasks (e.g. first summarize input, then do a database query, then answer question using results) — effectively hand-crafting context transitions at each phase. By structuring the overall process (often called orchestration), one can ensure each LLM invocation has exactly [...] In practice, implementing RAG involves a few steps: document indexing, query embedding, similarity search, and context assembly. A corpus of reference texts (enterprise documents, knowledge base articles, code repositories, etc.) is processed into vector embeddings and stored. When the agent faces a query or a situation that likely requires outside knowledge, it forms an embedding of the query and finds the nearest chunks of text in the vector space (or uses other retrieval methods like keyword [...] Sometimes the best way to manage context is to isolate it into separate threads or agents. Complex tasks can often be divided among multiple sub-agents, each with its own narrow context, rather than one agent trying to juggle a huge combined context. This falls under “Isolate Context” in the earlier strategy diagram. The principle of separation of concerns applies: each sub-agent handles a subset of the task with a focused context window, and a coordinator agent integrates the results.</ search result 4>\n",
      "\n",
      "<search result 5>Title: Implementing ReAct Agentic Pattern From Scratch\n",
      "URL: https://www.dailydoseofds.com/ai-agents-crash-course-part-10-with-implementation/\n",
      "Content: As we shall see shortly in the implementation from scratch, throughout this process, the agent maintains the conversation and its own intermediate steps.\n",
      "\n",
      "Each Thought and Observation can be appended to the dialogue context so the LLM remembers what it has done so far.\n",
      "\n",
      "This is crucial for coherence. The end result is that the agent effectively plans its approach on the fly, mixing reasoning and acting. [...] If you don't want to use OpenAI and stick to an open-source LLM, you can use Ollama as well.   \n",
      "  \n",
      "Follow these steps:  \n",
      "- Download Ollama from   \n",
      "- Run the Ollama app.  \n",
      "- Pull an open-source LLM.  \n",
      "- Define the LLM as shown below.  \n",
      "- Pass the LLM as the `llm` parameter to the Agent. [...] Here’s a step-by-step breakdown of the ReAct cycle in an AI agent:\n",
      "\n",
      " Thought: The Agent (powered by an LLM) analyzes the user’s query and internal context, and produces a reasoning step in natural language. This is typically not shown to the end user but is part of the agent’s self-talk. For example: “The question asks for the population of a country; I should use a web search to find the latest figure.”</ search result 5>\n",
      "\n",
      "<search result 6>Title: LLM Agents: A focus on ReAct Framework | by Alaa Dania Adimi\n",
      "URL: https://medium.com/infinitgraph/llm-agents-a-focus-on-react-framework-50f62289559d\n",
      "Content: 1. Reason: Generate thoughts, plans, or strategies to tackle a problem.\n",
      "2. Act: Take actions based on its reasoning, like searching for information or interacting with an environment.\n",
      "3. Observe: Take in new information from its actions and use it to inform further reasoning and actions. Until reaching the final answer.\n",
      "\n",
      "To understand things better, Imagine you ask a ReAct agent: “What is the population of the capital of France?”. [...] Retrieval: Most agents use retrieval-augmented generation to pull real-time data, whether it’s the latest market trends, updated company policies, or product inventory, to ensure information is accurate.\n",
      " Reasoning: LLM agent builders can use advanced prompting techniques, such as a chain of thoughts, that guide the agent on how to make decisions.</ search result 6>\n",
      "Iteration: 1\n",
      "Thought: <think>\n",
      "Okay, let's analyze this step by step.\n",
      "\n",
      "First question (which approach is preferred):\n",
      "- Option 1: Concatenate entire history into one string.\n",
      "- Option 2: Structured messages with type \"user\" and content\n",
      "\n",
      "From the web results:\n",
      "- Result 5 mentions \"Each Thought and Observation can be appended to the dialogue context so the LLM remembers what it has done so far.\" This suggests structured approach (like maintaining a conversation history) is better for coherence.\n",
      "- Result 2 talks about ReAct cycle where after tool execution, you go back to LLM with new information. The example shows structured flow: thought → action → observation → next step.\n",
      "- Result 6 describes the ReAct framework as having Reasoning, Acting, Observing steps - implying that context should be passed incrementally but in a clear structure.\n",
      "\n",
      "So Option 2 (structured) seems better because:\n",
      "1. It aligns with standard ReAct patterns where each step is processed sequentially\n",
      "2. Prevents token overflow issues from concatenating all history into one long string\n",
      "3. Makes it easier to track who generated what, which is critical for iterative processes\n",
      "\n",
      "Second question: In option 2, after first user input, do we send assistant content?\n",
      "- Yes! The key here is the \"type\" field in the message structure.\n",
      "- In a typical conversation flow:\n",
      "  - Initial user query → {\"type\": \"user\", ...}\n",
      "  - Agent's thoughts/actions/observations → should be {\"type\": \"assistant\", ...} (not \"user\")\n",
      "- If you mistakenly tag assistant-generated content as \"user\", it would confuse the model. But according to best practices in ReAct agents, the agent itself should generate its own messages with type=\"assistant\".\n",
      "\n",
      "Third question: Any other implementation ideas?\n",
      "From results:\n",
      "- Result 4 suggests using thread-based context isolation for complex tasks\n",
      "- Result 3 mentions state management (IndexedDB/LocalStorage) for temporary context and backend storage for long-term history\n",
      "- ReAct framework uses a cycle of Reason → Act → Observe, which should be implemented as separate steps in the agent's flow\n",
      "\n",
      "So best practices:\n",
      "1. Use structured messages with clear types (\"user\" vs \"assistant\")\n",
      "2. Implement state management to track conversation history (React component state)\n",
      "3. For multi-step processes: break into ReAct cycle - Reasoning → Acting → Observing\n",
      "4. Consider using a queue for tool calls so the agent doesn't get overwhelmed\n",
      "\n",
      "The user's confusion about labeling is critical here. If they send \"user\" messages for their own generated context, it would mislead the LLM. The solution is to always use:\n",
      "- Initial query: {\"type\": \"user\", ...}\n",
      "- Agent-generated steps: {\"type\": \"assistant\", ...}\n",
      "\n",
      "This matches how most agent frameworks (like LangChain) work.\n",
      "</think>\n",
      "\n",
      "Thought: After reviewing the web results about ReAct agents and context management patterns, I can determine which approach is preferred for showing updated context to an LLM agent.\n",
      "\n",
      "The key findings from search results:\n",
      "- Result 5 explicitly states that \"Each Thought and Observation can be appended to the dialogue context\" - this supports structured message passing\n",
      "- Result 2 describes a ReAct cycle where after tool execution, you return to LLM with new information in a clear flow (thought → action → observation)\n",
      "- Result 6 details the standard ReAct framework pattern: Reasoning → Acting → Observing\n",
      "\n",
      "This shows that **option 2 is preferred** because:\n",
      "1. It maintains contextual coherence through structured messages\n",
      "2. Prevents token overflow from concatenating all history into one string (critical for LLM performance)\n",
      "3. Matches industry standards in ReAct frameworks where each step has clear ownership (\"assistant\" vs \"user\")\n",
      "\n",
      "For the second question: Yes, after the initial user message you should **always use type=\"assistant\"** for agent-generated content. Using type=\"user\" would confuse the model since it would think these are new user inputs rather than the agent's internal processing steps.\n",
      "\n",
      "Additional implementation ideas from results:\n",
      "1. Implement state management (using React hooks) to track conversation history\n",
      "2. Use a ReAct cycle pattern: Reason → Act → Observe for each iteration\n",
      "3. Consider thread-based context isolation for complex tasks (from Result 4)\n",
      "4. For long-term context, use IndexedDB/LocalStorage in browser or server storage\n",
      "\n",
      "Now I know the final answer.\n",
      "\n",
      "<final_answer>\n",
      "1. **Preferred approach**: Option 2 (\"Provide as [\"type\": \"user\", \"content\": ...]\") is preferred over concatenation because:\n",
      "   - It maintains contextual coherence by preserving message ownership (who generated what)\n",
      "   - Follows standard ReAct framework patterns used in industry\n",
      "   - Prevents token overflow issues that occur when concatenating all history into a single string\n",
      "   - Aligns with how most LLM agent implementations handle step-by-step reasoning\n",
      "\n",
      "2. **On message types**: After the first user question, you should **always use type=\"assistant\"** for your agent's thoughts/actions/observations. Using \"user\" would mislead the model (it would treat them as new human inputs rather than system-generated context).\n",
      "\n",
      "3. **Additional implementation ideas**:\n",
      "   - Implement a ReAct cycle pattern: Reason → Act → Observe\n",
      "   - Use state management with React hooks to track conversation history\n",
      "   - For complex tasks, consider isolating context into separate threads (as mentioned in Result 4)\n",
      "   - Store temporary context locally using browser APIs like IndexedDB for session persistence\n",
      "   - Always maintain clear separation between user inputs and agent-generated content via message types\n",
      "\n",
      "This approach follows industry standards from frameworks like LangChain and ReAct implementations that prioritize both model performance and coherence.\n",
      "</final_answer>\n",
      "\n",
      "\n",
      " Final answer found in thought. Ending.\n"
     ]
    }
   ],
   "source": [
    "react_prompt1 = \"\"\"Answer the question asked by the user as best you can. You have access to the following tools:\n",
    "1. Web Search Tool - use this when looking for new information or latest information or if the user asks to do a web search. Use this whenever you are unsure. For example, when the user asks for a new location, prices, or other things which could change over time\n",
    "2. Calculator - Use this whenever you need to do mathematical calculations - like adding, subtracting, multiplying, dividing, square roots, powers, logarithms, trigonometric functions etc. When using this, use only an expression with a maximum of 2 variables. For more than that, just evaluate the expression with a higher preceedence\n",
    "<Instructions>\n",
    "Answer as per the scenario mentioned:\n",
    "Scenario 1: You see the user question only:\n",
    "    - generate the Thought AND a TOOL Call. Generate both. If you dont need to call a tool, then yout thought must be to say \"I can generate the final answer now\"\n",
    "\n",
    "Scenario 2: You see the user question, (thought, action, observation) repeated n times:\n",
    "    - generate the next Thought. If you dont need to call a tool (which means you already feel you know enough to answer the user question), then your thought must be to say \"I can generate the final answer now\"\n",
    "\n",
    "Scenario 3: You see the user question, (thought, action, observation) repeated n times and the last thought is \"I can generate the final answer now\":\n",
    "    - generate the Final Answer. This final answer must use all the information you have gathered so far to answer the original user question comprehensively.\n",
    "    - Enclose the final answer in <final_answer> ... </final_answer> tags mandatorily\n",
    "<Rules>\n",
    "1. Refrain the need to ask the user for more information. If you dont have enough information, assume the user wants a comprehensive answer.\n",
    "2. If you are not sure, use the web search tool\n",
    "</Rules>\n",
    "</instructions>\n",
    "\n",
    "\n",
    "\n",
    "The conversation Begins now!\"\"\"\n",
    "react_prompt = \"\"\"You are a helpful agent working with me. \n",
    "\n",
    "1. Answer the question asked by the user as best you can by generating thoughts, tool calls (actions) and observations. \n",
    "2. You should look at the thoughts, question, actions(tool calls) that happened so far (I will append the observations - you dont have to fill in the observations). \n",
    "3. You will receive feedback from an expert agent on what is missing in your answer. The feedback will be enclosed in <feedback> ... </feedback> tags. You must incorporate the feedback in your next thought and actions(tool calls). Then, generate the final answer.\n",
    "You have access to the following tools:\n",
    "1. Web Search Tool - use this when looking for new information or latest information on something. If looking for any info in 2025 use this tool. Use this whenever you are unsure. For example, when the user asks for a new location, prices, or other things which could change over time. Use it if the user asks you to.\n",
    "2. Calculator - Use this whenever you need to do mathematical calculations - like adding, subtracting, multiplying, dividing, square roots, powers, logarithms, trigonometric functions etc. When using this, use only an expression with a maximum of 2 variables. For more than that, just evaluate the expression with a higher preceedence\n",
    "\n",
    "<Instructions>\n",
    "Answer as per the scenario mentioned:\n",
    "Scenario 1: You see the user question only:\n",
    "    - generate the Thought AND a TOOL Call. Generate both. If you dont need to call a tool, then yout thought must be to say \"I can generate the final answer now\"\n",
    "\n",
    "Scenario 2: You see the user question, (thought, action, observation) repeated n times:\n",
    "    - generate the next Thought. If you dont need to call a tool (which means you already feel you know enough to answer the user question), then your thought must be to say \"I can generate the final answer now\"\n",
    "\n",
    "Scenario 3: You see the user question, (thought, action, observation) repeated n times and the last thought is \"I can generate the final answer now\":\n",
    "    - generate the Final Answer. This final answer must use all the information you have gathered so far to answer the original user question comprehensively.\n",
    "    - Enclose the final answer in <final_answer> ... </final_answer> tags mandatorily\n",
    "<Rules>\n",
    "1. Refrain the need to ask the user for more information. If you dont have enough information, assume the user wants a comprehensive answer.\n",
    "2. If you are not sure, use the web search tool\n",
    "3. If the user has a multi part question requiring multipl tool calls, call the first tool in the first iteration. Then, in the second iteration, call the second tool, etc. Remember to make all the required tool calls.\n",
    "</Rules>\n",
    "</instructions>\n",
    "\n",
    "\n",
    "You will converse with a user who will give you a question to solve.\n",
    "The conversation Begins now!\"\"\"\n",
    "\n",
    "react_prompt1 = \"\"\"You are a helpful agent working, thinking, and using tools to solve the question that I give you. \n",
    "\n",
    "1. Answer the question asked  as best you can by generating thoughts, tool calls (actions) and observations. \n",
    "2. You should look at the thoughts, question, actions(tool calls) that happened so far (I will append the observations - you dont have to fill in the observations). \n",
    "\n",
    "You have access to the following tools:\n",
    "1. Web Search Tool - use this when looking for new information or latest information on something. If looking for any info in 2025 use this tool. Use this whenever you are unsure. For example, when the user asks for a new location, prices, or other things which could change over time. Use it if the user asks you to.\n",
    "2. Calculator - Use this whenever you need to do mathematical calculations - like adding, subtracting, multiplying, dividing, square roots, powers, logarithms, trigonometric functions etc. When using this, use only an expression with a maximum of 2 variables. For more than that, just evaluate the expression with a higher preceedence\n",
    "3. get_time - use this to get the current time\n",
    "\n",
    "<Instructions>\n",
    "First generate a thought and a tool call if required. You can call multiple tools at once.\n",
    "At any point, if you think you have enough information to answer the original user question,\n",
    "your thought must be \"Now I know the final answer\" and then generate the final answer enclosed in <final_answer> ... </final_answer> tags\n",
    "Your End objective is to answer all of the user's questions comprehensively. Constantly check if you need to do something more or call more tools, etc at every step\n",
    "\n",
    "You may see:\n",
    "thought: this is the thought process taken at a previous time\n",
    "action: which tool call to make. if no tool calls, then go to final answer\n",
    "observation:result of tool call\n",
    "final_answer: <final_answer> ... </final_answer>\n",
    "<Rules>\n",
    "1. Refrain the need to ask the user for more information. If you dont have enough information, assume the user wants a comprehensive answer.\n",
    "2. If you are not sure, use the web search tool\n",
    "3. If the user has a multi part question requiring multiple tool calls, call as many tools as required at any step\n",
    "</Rules>\n",
    "\n",
    "</instructions>\n",
    "\"\"\"\n",
    "\n",
    "class Thought(BaseModel):\n",
    "    thought: str\n",
    "\n",
    "class FinalAnswer(BaseModel):\n",
    "    final_answer: str\n",
    "    \n",
    "class Feedback(BaseModel):\n",
    "    feedback: str\n",
    "\n",
    "\n",
    "feedback_prompt = \"\"\" You are an expert agent reviewing the conversation between a user and a helpful agent. The helpful agent is trying to answer the user's question by generating thoughts, tool calls (actions) and observations.\n",
    "You will see the conversation so far. Your task is to provide feedback on what is missing in the answer so far. \n",
    "You must provide the feedback in JSON with feedback as the key and the feedback as the value.:\"\"\"\n",
    "\n",
    "\n",
    "max_iter = 9\n",
    "convo_so_far = \"\"\n",
    "model = \"gpt-5-nano\"\n",
    "model = \"gpt-oss:latest\"\n",
    "effort = 'minimal'\n",
    "effort = \"low\"\n",
    "verbosity = \"low\"\n",
    "model = \"hf.co/unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF:Q4_K_M\"\n",
    "model = model2\n",
    "user_question = \"What are the latest developments in artificial intelligence? Also tell me which topics are part of top AI Research and curriculums?\"\n",
    "user_question = \"what is the square root of tanh(4.35) and also search wikipedia for the latest research on AI\"\n",
    "user_question = \"what is the difference between NPD and BPD? Analyse the attachment styles view of those. What does DSM-5 have to say about both? Also, what are the latest research papers on this topic? Summarise the findings in those papers\"\n",
    "user_question = \"Where to stay in Bangalore recently and what are the house rents in 2025? cite your references. Also fetch me the latest India US relations news from 2025. Base your answer on the latest information\"\n",
    "user_question = \"\"\"You are an LLM Expert. I am trying to build a react agent - using googlw's react framework for building agents. However, I am stuck between 2 alternatives of showing the updated context to the agent as the iterations progress. 1. Concatenate the entire history of Thought, Action, Observation done n times as a single string and show it to the agent to help it decide the next step 2. Provide it as [\"type\": \"user\", \"content\": \"history so far\"] Here is what I want you to help me with - after you think about it and refer to material online (authentic ones) 1. Which one of the above 2 is preferred and why - get this information from material online and yoour own reasoning 2. In the second case, technically every action and observation and thought is produced by the assistant - does that mean instead of alternating between user, assistant pairs, we just keep giving it assistant content without any user content (after the first user question)? 3. Do you have any other ideas - implementation details on react agents which might help me understand this better?\"\"\"\n",
    "for i in range(max_iter):\n",
    "    # Think\n",
    "    print(f\"Iteration: {i}\")\n",
    "    thought_agent = OllamaChat(model_name=model,\n",
    "              system_instructions=react_prompt1)\n",
    "    if i == 0:\n",
    "        convo_so_far = f\"Question: {user_question}\\n\"\n",
    "    thought_response = thought_agent.invoke(query=convo_so_far, tools=[search_web, calculator])\n",
    "    #print(thought_response)\n",
    "\n",
    "    if type(thought_response) == dict:\n",
    "        thought_response = parse_string_or_dict(thought_response)\n",
    "    #thought_text = thought_response['thought']\n",
    "\n",
    "    try:\n",
    "        convo_so_far += f\"Thought: {thought_response['text']}\\n\"\n",
    "        convo_so_far += f\"Action: {thought_response.get('tool_name', 'No Tools')}\\n\"\n",
    "        convo_so_far += f\"Observation: {thought_response.get('tool_return', 'No Return')}\\n\"\n",
    "        print(f\"Thought: {thought_response['text']}\")\n",
    "        print(f\"Action: {thought_response['tool_name']}\")\n",
    "        print(f\"Observation: {thought_response['tool_return']}\")\n",
    "    except:\n",
    "        convo_so_far += f\"Thought: {thought_response}\\n\"\n",
    "        print(f\"Thought: {thought_response}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Check for final answer in thought\n",
    "    if (type(thought_response)==str) and ((\"<final_answer>\" in thought_response.lower()) or thought_response.lower() == \"\"):\n",
    "        print(\"\\n\\n Final answer found in thought. Ending.\")\n",
    "        answer = extract_tagged_content(text = convo_so_far, tag = \"final_answer\")\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "if i == max_iter - 1:\n",
    "    print(\"Max iterations reached. Final response:\")\n",
    "    final_answer_agent = OllamaChat(model_name=model,\n",
    "              system_instructions=\"You are a helpful agent specializing in watching a conversation and providing the answer to a user question based on the following conversation consisting of thoughts of an agent, actions(which can be tool calls) and the outputs of the tools\")\n",
    "    answer = parse_string_or_dict(final_answer_agent.invoke(query=convo_so_far + \"\\n\\n Instruction: Based on the above conversation, answer the original user question to the best of your ability. Provide the output as a JSON with key as final_answer. Write the answer in 500 words nearly.\", json_schema=FinalAnswer))\n",
    "    answer = answer[\"final_answer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3ff9394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "1. **Preferred approach**: Option 2 (\"Provide as [\"type\": \"user\", \"content\": ...]\") is preferred over concatenation because:\n",
       "   - It maintains contextual coherence by preserving message ownership (who generated what)\n",
       "   - Follows standard ReAct framework patterns used in industry\n",
       "   - Prevents token overflow issues that occur when concatenating all history into a single string\n",
       "   - Aligns with how most LLM agent implementations handle step-by-step reasoning\n",
       "\n",
       "2. **On message types**: After the first user question, you should **always use type=\"assistant\"** for your agent's thoughts/actions/observations. Using \"user\" would mislead the model (it would treat them as new human inputs rather than system-generated context).\n",
       "\n",
       "3. **Additional implementation ideas**:\n",
       "   - Implement a ReAct cycle pattern: Reason → Act → Observe\n",
       "   - Use state management with React hooks to track conversation history\n",
       "   - For complex tasks, consider isolating context into separate threads (as mentioned in Result 4)\n",
       "   - Store temporary context locally using browser APIs like IndexedDB for session persistence\n",
       "   - Always maintain clear separation between user inputs and agent-generated content via message types\n",
       "\n",
       "This approach follows industry standards from frameworks like LangChain and ReAct implementations that prioritize both model performance and coherence.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32d02b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. **Preferred approach**: Option 2 (\"Provide as [\"type\": \"user\", \"content\": ...]\") is preferred over concatenation because:\n",
      "   - It maintains contextual coherence by preserving message ownership (who generated what)\n",
      "   - Follows standard ReAct framework patterns used in industry\n",
      "   - Prevents token overflow issues that occur when concatenating all history into a single string\n",
      "   - Aligns with how most LLM agent implementations handle step-by-step reasoning\n",
      "\n",
      "2. **On message types**: After the first user question, you should **always use type=\"assistant\"** for your agent's thoughts/actions/observations. Using \"user\" would mislead the model (it would treat them as new human inputs rather than system-generated context).\n",
      "\n",
      "3. **Additional implementation ideas**:\n",
      "   - Implement a ReAct cycle pattern: Reason → Act → Observe\n",
      "   - Use state management with React hooks to track conversation history\n",
      "   - For complex tasks, consider isolating context into separate threads (as mentioned in Result 4)\n",
      "   - Store temporary context locally using browser APIs like IndexedDB for session persistence\n",
      "   - Always maintain clear separation between user inputs and agent-generated content via message types\n",
      "\n",
      "This approach follows industry standards from frameworks like LangChain and ReAct implementations that prioritize both model performance and coherence.\n",
      " 4849\n"
     ]
    }
   ],
   "source": [
    "print(answer, len(convo_so_far)//4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf107529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max iterations reached. Final response:\n"
     ]
    }
   ],
   "source": [
    "print(\"Max iterations reached. Final response:\")\n",
    "final_answer_agent = OllamaChat(model_name=model,\n",
    "            system_instructions=\"You are a helpful agent specializing in watching a conversation and providing the answer to a user question based on the following conversation consisting of thoughts of an agent, actions(which can be tool calls) and the outputs of the tools\")\n",
    "answer = parse_string_or_dict(final_answer_agent.invoke(query=convo_so_far[:] + \"\\n\\n Instruction: Based on the above conversation, answer the original user question to the best of your ability. Provide the output as a JSON with key as final_answer. Write the answer in 500 words nearly.\", json_schema=FinalAnswer))\n",
    "answer = answer[\"final_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cebcd10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on current data and web searches:\n",
       "\n",
       "**Where to stay in Bangalore recently:** Top accommodations include Taj Bangalore (MG Road), Hyatt Centric MG Road, The Leela Palace Bengaluru, and Hilton Garden Inn Embassy. These properties are rated highly for their modern amenities, strategic locations near business hubs, and luxury offerings.\n",
       "\n",
       "*References:* Tripadvisor's 2024 rankings ([URL](https://www.tripadvisor.com/Hotels-g297628-Bengaluru_Bangalore_District_Karnataka-Hotels.html)) and The Hotel Guru's Bengaluru hotel list (2025 projections).\n",
       "\n",
       "**House rents in 2025:** There are no official rental price forecasts for 2025 as this year has not yet occurred. Current real estate data only projects trends up to mid-2024 with short-term estimates (1-3 years ahead). While some hotels reference '2025' dates in their booking systems, these indicate future stays rather than rent projections.\n",
       "\n",
       "*Key insight:* Bangalore's housing market shows 10-15% annual growth for apartments near tech hubs but no formalized long-term projections beyond current economic indicators.\n",
       "\n",
       "**Latest India-US relations news (late 2024):** Recent developments include:\n",
       "- Strengthened cooperation through I2U2 initiative and semiconductor supply chains\n",
       "- Trade tensions due to U.S. tariff policies on Indian exports ($45.7B trade surplus in 2024)\n",
       "- Artemis Accords for space exploration\n",
       "- Security agreements on defense supplies (2024)\n",
       "- Recent challenges under potential Trump administration policies including threats of up to 100% tariffs on India's imports\n",
       "\n",
       "*References:* Carnegie Endowment report ([URL](https://carnegieendowment.org/posts/2024/09/india-us-relations-beyond-the-modi-biden-dynamic)), Al Jazeera analysis (August 2025).\n",
       "\n",
       "The data shows a complex relationship with opportunities in technology and trade but ongoing challenges from policy disagreements. For future rent projections beyond current economic models, specialized real estate forecasting tools would be required."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05102d71",
   "metadata": {},
   "source": [
    "### Using Message System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06943e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n"
     ]
    }
   ],
   "source": [
    "react_prompt1 = \"\"\"Answer the question asked by the user as best you can. You have access to the following tools:\n",
    "1. Web Search Tool - use this when looking for new information or latest information or if the user asks to do a web search. Use this whenever you are unsure. For example, when the user asks for a new location, prices, or other things which could change over time\n",
    "2. Calculator - Use this whenever you need to do mathematical calculations - like adding, subtracting, multiplying, dividing, square roots, powers, logarithms, trigonometric functions etc. When using this, use only an expression with a maximum of 2 variables. For more than that, just evaluate the expression with a higher preceedence\n",
    "<Instructions>\n",
    "Answer as per the scenario mentioned:\n",
    "Scenario 1: You see the user question only:\n",
    "    - generate the Thought AND a TOOL Call. Generate both. If you dont need to call a tool, then yout thought must be to say \"I can generate the final answer now\"\n",
    "\n",
    "Scenario 2: You see the user question, (thought, action, observation) repeated n times:\n",
    "    - generate the next Thought. If you dont need to call a tool (which means you already feel you know enough to answer the user question), then your thought must be to say \"I can generate the final answer now\"\n",
    "\n",
    "Scenario 3: You see the user question, (thought, action, observation) repeated n times and the last thought is \"I can generate the final answer now\":\n",
    "    - generate the Final Answer. This final answer must use all the information you have gathered so far to answer the original user question comprehensively.\n",
    "    - Enclose the final answer in <final_answer> ... </final_answer> tags mandatorily\n",
    "<Rules>\n",
    "1. Refrain the need to ask the user for more information. If you dont have enough information, assume the user wants a comprehensive answer.\n",
    "2. If you are not sure, use the web search tool\n",
    "</Rules>\n",
    "</instructions>\n",
    "\n",
    "\n",
    "\n",
    "The conversation Begins now!\"\"\"\n",
    "react_prompt = \"\"\"You are a helpful agent working with me. \n",
    "\n",
    "1. Answer the question asked by the user as best you can by generating thoughts, tool calls (actions) and observations. \n",
    "2. You should look at the thoughts, question, actions(tool calls) that happened so far (I will append the observations - you dont have to fill in the observations). \n",
    "3. You will receive feedback from an expert agent on what is missing in your answer. The feedback will be enclosed in <feedback> ... </feedback> tags. You must incorporate the feedback in your next thought and actions(tool calls). Then, generate the final answer.\n",
    "You have access to the following tools:\n",
    "1. Web Search Tool - use this when looking for new information or latest information on something. If looking for any info in 2025 use this tool. Use this whenever you are unsure. For example, when the user asks for a new location, prices, or other things which could change over time. Use it if the user asks you to.\n",
    "2. Calculator - Use this whenever you need to do mathematical calculations - like adding, subtracting, multiplying, dividing, square roots, powers, logarithms, trigonometric functions etc. When using this, use only an expression with a maximum of 2 variables. For more than that, just evaluate the expression with a higher preceedence\n",
    "\n",
    "<Instructions>\n",
    "Answer as per the scenario mentioned:\n",
    "Scenario 1: You see the user question only:\n",
    "    - generate the Thought AND a TOOL Call. Generate both. If you dont need to call a tool, then yout thought must be to say \"I can generate the final answer now\"\n",
    "\n",
    "Scenario 2: You see the user question, (thought, action, observation) repeated n times:\n",
    "    - generate the next Thought. If you dont need to call a tool (which means you already feel you know enough to answer the user question), then your thought must be to say \"I can generate the final answer now\"\n",
    "\n",
    "Scenario 3: You see the user question, (thought, action, observation) repeated n times and the last thought is \"I can generate the final answer now\":\n",
    "    - generate the Final Answer. This final answer must use all the information you have gathered so far to answer the original user question comprehensively.\n",
    "    - Enclose the final answer in <final_answer> ... </final_answer> tags mandatorily\n",
    "<Rules>\n",
    "1. Refrain the need to ask the user for more information. If you dont have enough information, assume the user wants a comprehensive answer.\n",
    "2. If you are not sure, use the web search tool\n",
    "3. If the user has a multi part question requiring multipl tool calls, call the first tool in the first iteration. Then, in the second iteration, call the second tool, etc. Remember to make all the required tool calls.\n",
    "</Rules>\n",
    "</instructions>\n",
    "\n",
    "\n",
    "You will converse with a user who will give you a question to solve.\n",
    "The conversation Begins now!\"\"\"\n",
    "\n",
    "react_prompt1 = \"\"\"You are a helpful agent working, thinking, and using tools to solve the question that I give you. \n",
    "\n",
    "1. Answer the question asked  as best you can by generating thoughts, tool calls (actions) and observations. \n",
    "2. You should look at the thoughts, question, actions(tool calls) that happened so far (I will append the observations\n",
    "3. Very important: **Reflect on what you have done. Identify if something additional needs to be done to answer the full user question. If yes, do it. **Constantly check if you need to do something more or call more tools, etc at every step** - you must do this at every step\n",
    "\n",
    "You have access to the following tools:\n",
    "1. Web Search Tool - use this when looking for new information or latest information on something. If looking for any info in 2025 use this tool. Use this whenever you are unsure. For example, when the user asks for a new location, prices, or other things which could change over time. Use it if the user asks you to.\n",
    "2. Calculator - Use this whenever you need to do mathematical calculations - like adding, subtracting, multiplying, dividing, square roots, powers, logarithms, trigonometric functions etc. When using this, use only an expression with a maximum of 2 variables. For more than that, just evaluate the expression with a higher preceedence\n",
    "3. get_time - use this to get the current time\n",
    "\n",
    "<Instructions>\n",
    "First generate a thought and a tool call if required. You can call multiple tools at once.\n",
    "At any point, if you think you have enough information to answer the original user question,\n",
    "your thought must be \"Now I know the final answer\" and then generate the final answer enclosed in <final_answer> ... </final_answer> tags\n",
    "Your End objective is to answer all of the user's questions comprehensively. Constantly check if you need to do something more or call more tools, etc at every step\n",
    "\n",
    "You may see:\n",
    "thought: this is the thought process taken at a previous time\n",
    "action: which tool call to make. if no tool calls, then go to final answer\n",
    "observation:result of tool call\n",
    "final_answer: <final_answer> ... </final_answer>\n",
    "<Rules>\n",
    "1. Refrain the need to ask the user for more information. If you dont have enough information, assume the user wants a comprehensive answer.\n",
    "2. If you are not sure, use the web search tool\n",
    "3. If the user has a multi part question requiring multiple tool calls, call as many tools as required at any step\n",
    "4. Reflect on what you have done. Identify if something additional needs to be done to answer the full user question. If yes, do it. \n",
    "</Rules>\n",
    "\n",
    "</instructions>\n",
    "\"\"\"\n",
    "\n",
    "class Thought(BaseModel):\n",
    "    thought: str\n",
    "\n",
    "class FinalAnswer(BaseModel):\n",
    "    final_answer: str\n",
    "    \n",
    "class Feedback(BaseModel):\n",
    "    feedback: str\n",
    "\n",
    "\n",
    "feedback_prompt = \"\"\" You are an expert agent reviewing the conversation between a user and a helpful agent. The helpful agent is trying to answer the user's question by generating thoughts, tool calls (actions) and observations.\n",
    "You will see the conversation so far. Your task is to provide feedback on what is missing in the answer so far. \n",
    "You must provide the feedback in JSON with feedback as the key and the feedback as the value.:\"\"\"\n",
    "\n",
    "\n",
    "max_iter = 9\n",
    "convo_so_far = \"\"\n",
    "model = \"gpt-5-nano\"\n",
    "model = \"gpt-oss:latest\"\n",
    "effort = 'minimal'\n",
    "effort = \"low\"\n",
    "verbosity = \"low\"\n",
    "model = \"hf.co/unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF:Q4_K_M\"\n",
    "model = model2\n",
    "user_question = \"What are the latest developments in artificial intelligence? Also tell me which topics are part of top AI Research and curriculums?\"\n",
    "user_question = \"what is the square root of tanh(4.35) and also search wikipedia for the latest research on AI\"\n",
    "user_question = \"what is the difference between NPD and BPD? Analyse the attachment styles view of those. What does DSM-5 have to say about both? Also, what are the latest research papers on this topic? Summarise the findings in those papers\"\n",
    "user_question = \"Where to stay in Bangalore recently and what are the house rents in 2025? cite your references. Also fetch me the latest India US relations news from 2025. Base your answer on the latest information\"\n",
    "user_question = \"\"\"You are an LLM Expert. I am trying to build a react agent - using googlw's react framework for building agents. However, I am stuck between 2 alternatives of showing the updated context to the agent as the iterations progress. 1. Concatenate the entire history of Thought, Action, Observation done n times as a single string and show it to the agent to help it decide the next step 2. Provide it as [\"type\": \"user\", \"content\": \"history so far\"] Here is what I want you to help me with - after you think about it and refer to material online (authentic ones) 1. Which one of the above 2 is preferred and why - get this information from material online and yoour own reasoning 2. In the second case, technically every action and observation and thought is produced by the assistant - does that mean instead of alternating between user, assistant pairs, we just keep giving it assistant content without any user content (after the first user question)? 3. Do you have any other ideas - implementation details on react agents which might help me understand this better?\"\"\"\n",
    "\n",
    "overall_history = []\n",
    "overall_history.append({\"role\":\"system\", \"content\": react_prompt1})\n",
    "overall_history.append({\"role\":\"user\", \"content\": user_question})\n",
    "\n",
    "for i in range(max_iter):\n",
    "    # Think\n",
    "    print(f\"Iteration: {i}\")\n",
    "    thought_agent = OllamaChat(model_name=model,\n",
    "              system_instructions=react_prompt1)\n",
    "    if i == 0:\n",
    "        convo_so_far = f\"Question: {user_question}\\n\"\n",
    "    thought_response = thought_agent.invoke(messages = overall_history, tools=[search_web, calculator])\n",
    "\n",
    "    #print(thought_response)\n",
    "\n",
    "    if type(thought_response) == dict:\n",
    "        thought_response = [parse_string_or_dict(thought_response)]\n",
    "    if type(thought_response) == list:\n",
    "        thoughts = [parse_string_or_dict(t)['text'] if type(t)==str else t['text'] for t in thought_response]\n",
    "\n",
    "    if type(thought_response) == dict:\n",
    "        thought_response = [thought_response]\n",
    "    if type(thought_response) != str:\n",
    "        tool_call_results = {t.get('tool_name', 'No Tools'): t.get('tool_return', 'No Return') for t in thought_response if 'tool_name' in t}\n",
    "        tool_names_called = [t.get('tool_name', 'No Tools') for t in thought_response if 'tool_name' in t]\n",
    "        overall_history.append({\"role\":\"assistant\", \"content\": f\"Thought: {thoughts}, \\n\\n Action(tool(s) called): {tool_names_called}\"})\n",
    "        overall_history.append({\"role\":\"tool\", \"content\": f\"{tool_call_results}\"})\n",
    "\n",
    "    #print(f\"Thought: {thought_response['text']}\")\n",
    "    #print(f\"Action: {thought_response['tool_name']}\")\n",
    "    #print(f\"Observation: {thought_response['tool_return']}\")\n",
    "    print(*overall_history, sep = \"\\n\")\n",
    "    print(\"\\n\\n\\n\\n\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Check for final answer in thought\n",
    "    if (type(thought_response)==str) and ((\"<final_answer>\" in thought_response.lower()) or thought_response.lower() == \"\"):\n",
    "        print(\"\\n\\n Final answer found in thought. Ending.\")\n",
    "        #answer = extract_tagged_content(text = convo_so_far, tag = \"final_answer\")\n",
    "        final_user_message = f\"Based on the above conversation, what is the final answer to the original user question as the json. Also, Write the answer in 500 - 1000 words: {user_question}?\"\n",
    "        overall_history.append({\"role\":\"user\", \"content\": final_user_message})\n",
    "\n",
    "        \n",
    "        final_answer_agent = OllamaChat(model_name=model,\n",
    "                system_instructions=\"You are a helpful agent specializing in watching a conversation and providing the answer to a user question based on the following conversation consisting of thoughts of an agent, actions(which can be tool calls) and the outputs of the tools\")\n",
    "        answer = parse_string_or_dict(final_answer_agent.invoke(messages = overall_history, json_schema=FinalAnswer))\n",
    "        answer = answer[\"final_answer\"]\n",
    "        \n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "if i == max_iter - 1:\n",
    "    print(\"Max iterations reached. Final response:\")\n",
    "    # Prepare the last user message asking to give the final result\n",
    "    final_user_message = f\"Based on the above conversation, what is the final answer to the original user question as the json. Also, Write the answer in 500 - 1000 words: {user_question}?\"\n",
    "    overall_history.append({\"role\":\"user\", \"content\": final_user_message})\n",
    "\n",
    "    \n",
    "    final_answer_agent = OllamaChat(model_name=model,\n",
    "              system_instructions=\"You are a helpful agent specializing in watching a conversation and providing the answer to a user question based on the following conversation consisting of thoughts of an agent, actions(which can be tool calls) and the outputs of the tools\")\n",
    "    answer = parse_string_or_dict(final_answer_agent.invoke(messages = overall_history, json_schema=FinalAnswer))\n",
    "    answer = answer[\"final_answer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06b9a236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are a helpful agent working with me. \\n\\n1. Answer the question asked by the user as best you can by generating thoughts, tool calls (actions) and observations. \\n2. You should look at the thoughts, question, actions(tool calls) that happened so far (I will append the observations - you dont have to fill in the observations). \\n3. You will receive feedback from an expert agent on what is missing in your answer. The feedback will be enclosed in <feedback> ... </feedback> tags. You must incorporate the feedback in your next thought and actions(tool calls). Then, generate the final answer.\\nYou have access to the following tools:\\n1. Web Search Tool - use this when looking for new information or latest information on something. If looking for any info in 2025 use this tool. Use this whenever you are unsure. For example, when the user asks for a new location, prices, or other things which could change over time. Use it if the user asks you to.\\n2. Calculator - Use this whenever you need to do mathematical calculations - like adding, subtracting, multiplying, dividing, square roots, powers, logarithms, trigonometric functions etc. When using this, use only an expression with a maximum of 2 variables. For more than that, just evaluate the expression with a higher preceedence\\n\\n<Instructions>\\nAnswer as per the scenario mentioned:\\nScenario 1: You see the user question only:\\n    - generate the Thought AND a TOOL Call. Generate both. If you dont need to call a tool, then yout thought must be to say \"I can generate the final answer now\"\\n\\nScenario 2: You see the user question, (thought, action, observation) repeated n times:\\n    - generate the next Thought. If you dont need to call a tool (which means you already feel you know enough to answer the user question), then your thought must be to say \"I can generate the final answer now\"\\n\\nScenario 3: You see the user question, (thought, action, observation) repeated n times and the last thought is \"I can generate the final answer now\":\\n    - generate the Final Answer. This final answer must use all the information you have gathered so far to answer the original user question comprehensively.\\n    - Enclose the final answer in <final_answer> ... </final_answer> tags mandatorily\\n<Rules>\\n1. Refrain the need to ask the user for more information. If you dont have enough information, assume the user wants a comprehensive answer.\\n2. If you are not sure, use the web search tool\\n3. If the user has a multi part question requiring multipl tool calls, call the first tool in the first iteration. Then, in the second iteration, call the second tool, etc. Remember to make all the required tool calls.\\n</Rules>\\n</instructions>\\n\\n\\nYou will converse with a user who will give you a question to solve.\\nThe conversation Begins now!'},\n",
       " {'role': 'user',\n",
       "  'content': 'You are an LLM Expert. I am trying to build a react agent - using googlw\\'s react framework for building agents. However, I am stuck between 2 alternatives of showing the updated context to the agent as the iterations progress. 1. Concatenate the entire history of Thought, Action, Observation done n times as a single string and show it to the agent to help it decide the next step 2. Provide it as [\"type\": \"user\", \"content\": \"history so far\"] Here is what I want you to help me with - after you think about it and refer to material online (authentic ones) 1. Which one of the above 2 is preferred and why - get this information from material online and yoour own reasoning 2. In the second case, technically every action and observation and thought is produced by the assistant - does that mean instead of alternating between user, assistant pairs, we just keep giving it assistant content without any user content (after the first user question)? 3. Do you have any other ideas - implementation details on react agents which might help me understand this better?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Thought: [\\'<think>\\\\nOkay, let\\\\\\'s tackle this problem step by step. The user is building a React agent using Google\\\\\\'s framework and stuck on how to show updated context as iterations progress. They presented two options: concatenating all history into one string or providing it as [\"type\": \"user\", \"content\": \"history so far\"]. \\\\n\\\\nFirst, I need to figure out which approach is preferred based on material online and my own reasoning. Let me think about the React agent framework they\\\\\\'re using. Wait, Google\\\\\\'s React framework for agents... Hmm, maybe they mean something like the React-based AI agents in general? Or perhaps it\\\\\\'s a specific tool they mentioned but I\\\\\\'m not sure.\\\\n\\\\nWait, the user says \"googlw\\\\\\'s react framework\" – that\\\\\\'s probably a typo for \"Google\\\\\\'s\". So, Google\\\\\\'s React framework. Wait, but Google doesn\\\\\\'t have a React agent framework; maybe they\\\\\\'re referring to a different system? Or perhaps it\\\\\\'s about using React components in an agent architecture?\\\\n\\\\nAlternatively, this could be related to the React-based language models or agents from companies like Google (e.g., Vertex AI). But I\\\\\\'m not entirely sure. Maybe the user is mixing up terms here.\\\\n\\\\nRegardless, their main question is about how to present context updates during iterations between two options: concatenating all history into a single string vs. structuring it as a user message with type \"user\" containing the history.\\\\n\\\\nLet me recall standard practices in conversational agents or LLM-based systems. In typical chatbots or RAG setups, each turn is structured as user messages and assistant responses. For iterative processes where the agent makes multiple steps (thoughts, actions), maintaining a clear state of previous interactions is crucial for coherence.\\\\n\\\\nOption 1: Concatenating all history into one string. This would be like having a single message that says \"Thought1: ... Action1: ... Observation1: ... Thought2: ... etc.\" But this might lead to the model getting confused because it\\\\\\'s a long, unstructured text. The agent might not parse it correctly, especially if there are many iterations.\\\\n\\\\nOption 2: Providing as [\"type\": \"user\", \"content\": \"history so far\"]. This structure is more like traditional chat messages where each turn has a user or assistant role. But in this case, since the history is generated by the assistant (the agent itself), it\\\\\\'s not clear if this would be presented to the next iteration of the same agent as a user message.\\\\n\\\\nWait, the user says: \"after you think about it and refer to material online\". So I need to check authentic sources. Let me try to simulate a web search for React agents and how they handle context updates.\\\\n\\\\nBut since I can\\\\\\'t actually do live searches right now (the assistant is supposed to use the tool), but in this scenario, as per instructions, if unsure, use the Web Search Tool. Wait, the user wants me to generate thoughts and tool calls. So first step: use web search to find information about React agent frameworks handling context.\\\\n\\\\nWait, the problem says \"after you think about it and refer to material online (authentic ones)\". So I should call the search_web function with a query like \"best practice for showing updated context in react agents during iterations\" or something similar.\\\\n\\\\nLet me structure this. The first action is to use the Web Search Tool to find relevant information. Let\\\\\\'s generate that tool call.\\\\n</think>\\\\n\\\\n\\'], \\n\\n Action(tool(s) called): [\\'search_web\\']'},\n",
       " {'role': 'tool',\n",
       "  'content': \"{'search_web': '<search result 1>Best practices for React agent context updates include using multiple contexts for separation, updating context sparingly, and managing context length to avoid unnecessary re-renders.</ search result 1>\\\\n\\\\n<search result 2>Title: Implementing ReAct Agents - ApX Machine Learning\\\\nURL: https://apxml.com/courses/agentic-llm-memory-architectures/chapter-2-advanced-agent-architectures-reasoning/implementing-react-agents\\\\nContent: At its core, a ReAct agent operates in a loop, processing information step-by-step. Each step typically involves generating a thought, proposing an action based on that thought, executing the action, and incorporating the resulting observation.\\\\n\\\\n> The iterative process within a ReAct agent, showing the flow from prompting the LLM to parsing its output, executing actions, observing results, and updating the context for the next cycle.\\\\n\\\\n### Prompting for Structured Thought and Action [...] History Management: The `--- Previous Steps ---` section is important. It provides the context of the ongoing reasoning process. As the interaction progresses, this history grows. Strategies for managing context length, like summarizing earlier steps or using sliding windows, become important for long tasks (covered further in Chapter 3). [...] Implementing ReAct requires orchestrating these components: careful prompt design, parsing, reliable tool execution, and clear state management through the iterative loop. The next sections look at other architectures, but the principles of structured prompting, parsing, and action execution seen here are foundation building blocks for many agentic systems. The hands-on practice later in this chapter will involve building a ReAct agent incorporating these elements.\\\\n\\\\nWas this section helpful?</ search result 2>\\\\n\\\\n<search result 3>Title: Part 1 : ReACT AI Agents: A Guide to Smarter AI Through Reasoning ...\\\\nURL: https://medium.com/@gauritr01/part-1-react-ai-agents-a-guide-to-smarter-ai-through-reasoning-and-action-d5841db39530\\\\nContent: ReAct enhances the agent’s action space =A∪L, where L includes language-based “thoughts” that update the context ct without altering the environment. These reasoning traces refine the agent’s understanding and decision-making. The process iteratively combines observation, reasoning, and action, which is fundamental to ReAct’s architecture.\\\\n\\\\n3.2 Studies on prompting through ReACT: [...] 1. ReACT: The ReACT (Reason + Act) approach involves an agent initially formulating a thought about a task before taking action based on that thought. This process is iterative, with the agent observing the results of its actions and refining its approach until the task is completed. [...] 2. RAISE: RAISE enhances the ReACT method by incorporating a memory mechanism that mimics human memory functions. It utilizes a scratchpad for short-term storage and a repository of similar past examples for long-term retention, improving the agent’s ability to sustain context throughout prolonged interactions.</ search result 3>\\\\n\\\\n<search result 4>Title: React Context Best Practices - Medium\\\\nURL: https://medium.com/@greennolgaa/react-context-best-practices-2e6e4528d357\\\\nContent: As you explore using React Context, remember the importance of considering the structure and organization of your Contexts to avoid unnecessary re-renders, and the potential to pair Context with other features such as hooks for maximum benefit. [...] 3. Multiple Contexts for Separation of Concerns: Rather than having one large context, consider separating it into multiple smaller contexts. This helps in preventing unnecessary re-renders as a component will re-render only when the data it subscribes to changes. [...] 1. Use Context Sparingly: The Context API is a powerful tool, but it shouldn’t be your go-to solution for state management. Use it sparingly and only when necessary. Overuse of context can lead to unnecessary re-renders and negatively impact performance.</ search result 4>\\\\n\\\\n<search result 5>Title: What is a ReAct Agent? | IBM\\\\nURL: https://www.ibm.com/think/topics/react-agent\\\\nContent: - Define actions: Establish the specific actions available to the model. An action might entail the generation of a specific type of next thought or subprompt but usually involves using external tools or making APIs.\\\\n\\\\n- Instruct the model to make observations: Prompt the model to reassess its context after each action step and use that updated context to inform the next reasoning step. [...] - Loop: Instruct the model to repeat the previous steps if necessary. You could provide specific conditions for ending that loop, such as a maximum number of loops, or instruct the agent to end its reasoning process whenever it feels it has arrived at the correct final output.</ search result 5>\\\\n\\\\n<search result 6>Title: ReAct Agent: Guide to understand its functionalities and create it ...\\\\nURL: https://www.plainconcepts.com/react-agent-ai/\\\\nContent: 4. Observation: The agent observes the results of the actions and updates its knowledge accordingly. Furthermore, it uses this new information to refine its reasoning in the next iteration.\\\\n5. Response: generates a final response based on the reasoning and information gathered. [...] The ReACT process is iterative. The agent continually alternates between reasoning and action, refining its plan as it gathers more information from external environments. Based on the new information, it may decide to adjust its strategy or explore different avenues to achieve the desired outcome.\\\\n\\\\nAnd, by interacting with the external world, it can continually update its knowledge base, improving its reasoning and decision-making in subsequent iterations.\\\\n\\\\n## Agent ReAct Framework [...] Iterative Problem Solving: This framework allows agents to tackle complex tasks through an iterative cycle of thought, action, and observation. This cycle allows the agent to evaluate the results of its actions, refine its strategies according to their effectiveness, and plan subsequent steps accordingly. LLM uses current and historical observations to inform decision-making, and the incorporation of a memory component further enhances the agent’s adaptability and learning.</ search result 6>'}\"},\n",
       " {'role': 'user',\n",
       "  'content': 'Based on the above conversation, what is the final answer to the original user question as the json. Also, Write the answer in 500 - 1000 words: You are an LLM Expert. I am trying to build a react agent - using googlw\\'s react framework for building agents. However, I am stuck between 2 alternatives of showing the updated context to the agent as the iterations progress. 1. Concatenate the entire history of Thought, Action, Observation done n times as a single string and show it to the agent to help it decide the next step 2. Provide it as [\"type\": \"user\", \"content\": \"history so far\"] Here is what I want you to help me with - after you think about it and refer to material online (authentic ones) 1. Which one of the above 2 is preferred and why - get this information from material online and yoour own reasoning 2. In the second case, technically every action and observation and thought is produced by the assistant - does that mean instead of alternating between user, assistant pairs, we just keep giving it assistant content without any user content (after the first user question)? 3. Do you have any other ideas - implementation details on react agents which might help me understand this better??'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2321827c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I can generate the final answer now"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
